{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Learning to Manipulate under Limited Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Requirement Installation (Linux, Python >=3.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip3 install pref_voting # need at least version 0.4.42\n",
        "# !pip3 install numba  # needed for pref_voting\n",
        "# !pip3 install nashpy # needed for pref_voting\n",
        "# !pip3 install seaborn\n",
        "# !pip3 install multiprocess\n",
        "# !pip3 install tqdm\n",
        "# !pip3 install torch # https://pytorch.org/get-started/locally/\n",
        "# !pip3 install google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5nLR19AXMcp"
      },
      "outputs": [],
      "source": [
        "import pref_voting.generate_spatial_profiles\n",
        "import pref_voting.generate_utility_profiles\n",
        "import pref_voting.utility_profiles\n",
        "import pref_voting.utility_functions\n",
        "import pref_voting.voting_methods\n",
        "import pref_voting.profiles\n",
        "import pref_voting.generate_profiles\n",
        "from pref_voting.generate_profiles import *\n",
        "from pref_voting.iterative_methods import *\n",
        "from pref_voting.analysis import means_with_estimated_standard_error\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import itertools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import multiprocessing\n",
        "import dill\n",
        "import multiprocess\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from google.cloud import storage\n",
        "import pickle\n",
        "import zipfile\n",
        "import os\n",
        "import fnmatch\n",
        "\n",
        "# Set-up for multiprocessing\n",
        "dill.Pickler.dumps, dill.Pickler.loads = dill.dumps, dill.loads\n",
        "multiprocessing.reduction.ForkingPickler = dill.Pickler\n",
        "multiprocessing.reduction.dump = dill.dump\n",
        "try:\n",
        "    cpus = multiprocessing.cpu_count() - 1\n",
        "except NotImplementedError:\n",
        "    cpus = 2   # arbitrary default\n",
        "\n",
        "# Set-up for timing\n",
        "current_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=-7)))\n",
        "CURRENT_TIME_STR = current_time.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
        "\n",
        "run_dir = None\n",
        "DEVICE = 'cuda'  # change this to 'mps' to run on Apple Silicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pref_voting\n",
        "\n",
        "print(pref_voting.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Setup\n",
        "    * 1.1. Global variables\n",
        "    * 1.2. Create directories\n",
        "    * 1.3. Helper functions\n",
        "    * 1.4. Agent class\n",
        "    * 1.5. Manipulator information\n",
        "    * 1.6. Labeling setup\n",
        "        * 1.6.1. Satisficing\n",
        "        * 1.6.2. Optimizing\n",
        "    * 1.7. Validation setup\n",
        "    * 1.8. Training setup\n",
        "    * 1.9. Evaluation setup\n",
        "2. Generate utility profiles\n",
        "3. Generate labels\n",
        "4. Train models\n",
        "5. Evaluate models\n",
        "6. Create CSV files for visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DOWNLOAD_DATA = False # if true, then any missing utility profiles,  labels, and/or models will be downloaded from google cloud, unless the following variables are set to True. \n",
        "GENERATE_UTILITY_PROFILES = False\n",
        "GENERATE_LABELS = False\n",
        "TRAIN_MODELS = False\n",
        "\n",
        "EVALUATE_MODELS = False\n",
        "CREATE_CSVS = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# How many generations to run\n",
        "generation_list = [1]\n",
        "\n",
        "# How many candidates will be in each election\n",
        "num_cands_list = [3, 4, 5, 6]  \n",
        "\n",
        "# How many voters will be in each election\n",
        "num_voters_list = [5, 6, 10, 11, 20, 21]   \n",
        "\n",
        "# Which models to use for sampling utility functions\n",
        "prob_models_list = [\n",
        "    'uniform', \n",
        "    #'spatial_2dim',\n",
        "    #'mallows',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# What voting methods to study\n",
        "voting_methods_list = [\n",
        "    pref_voting.voting_methods.plurality,\n",
        "    pref_voting.voting_methods.borda,\n",
        "    pref_voting.voting_methods.instant_runoff,\n",
        "    pref_voting.voting_methods.instant_runoff_put,\n",
        "    pref_voting.voting_methods.minimax,\n",
        "    pref_voting.voting_methods.split_cycle,\n",
        "    pref_voting.voting_methods.strict_nanson,\n",
        "    pref_voting.voting_methods.stable_voting,\n",
        "    pref_voting.voting_methods.blacks,\n",
        "]\n",
        "\n",
        "# How much weight to give the manipulating voter. \n",
        "# manip_weight > 1 represents coalitional manipulation\n",
        "manip_weight_list = [1]\n",
        "\n",
        "# How rankings should be labeled\n",
        "labeling_list = [\n",
        "    #'satisfice',\n",
        "    'optimize'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# What information will the manipulator know\n",
        "agent_infos_list = [\n",
        "    # ['full'], \n",
        "    # ['anon_prof'],\n",
        "    ['plurality_scores'],\n",
        "    ['plurality_ranking'],\n",
        "    # ['borda_scores'], \n",
        "    ['margin'],\n",
        "    ['qual_margin'],\n",
        "    ['majority'],\n",
        "    ['sincere_winners'],\n",
        "    # ['plurality_scores, 'majority'], # List of multiple means that information will be concatenated together.\n",
        "]\n",
        "\n",
        "# Training hyperparameters\n",
        "learning_rates = [6e-3]\n",
        "max_num_iterations = 2000\n",
        "training_batch_size = 512 \n",
        "validation_batch_size = 4096 \n",
        "validate_every = 20\n",
        "patience = 10\n",
        "threshold = .001\n",
        "\n",
        "# Evaluation parameters\n",
        "max_est_std_error = 0.0005\n",
        "evaluation_batch_size = 4096\n",
        "max_num_evaluation_rounds = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2. Create directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create directories for saving results\n",
        "\n",
        "os.makedirs(f'./training_utility_profiles', exist_ok=True)\n",
        "os.makedirs(f'./validation_utility_profiles', exist_ok=True)\n",
        "os.makedirs(f'./evaluation_utility_profiles', exist_ok=True)\n",
        "os.makedirs(f'./labels', exist_ok=True)\n",
        "os.makedirs(f'./models', exist_ok=True)\n",
        "os.makedirs(f'./losses', exist_ok=True)\n",
        "os.makedirs(f'./graphs', exist_ok=True)\n",
        "\n",
        "for probmodel in prob_models_list:\n",
        "    for labeling in labeling_list:\n",
        "        for agent_infos in agent_infos_list:\n",
        "            for manip_weight in manip_weight_list:\n",
        "                os.makedirs(f'./models/models_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}', exist_ok=True)\n",
        "                os.makedirs(f'./losses/losses_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}', exist_ok=True)\n",
        "os.makedirs(f'./evaluation', exist_ok=True)\n",
        "for probmodel in prob_models_list:\n",
        "    for labeling in labeling_list:\n",
        "        for agent_infos in agent_infos_list:\n",
        "            for manip_weight in manip_weight_list:\n",
        "                os.makedirs(f'./evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_file(file_path):\n",
        "    bucket_name = 'ltmdata'  \n",
        "    dirs = file_path.split('/')\n",
        "    filename = dirs[-1]\n",
        "    dir = '/'.join(dirs[:-1])\n",
        "    # if dir != '':\n",
        "    #     dir += '/'\n",
        "\n",
        "    storage_client = storage.Client.create_anonymous_client()\n",
        "\n",
        "    blobs = storage_client.list_blobs(bucket_name, \n",
        "                                      prefix=dir, \n",
        "                                      delimiter=None)\n",
        "    found_file = False\n",
        "    for blob in blobs:\n",
        "        # only download the file that matches the file_path\n",
        "        if blob.name.endswith('zip') and fnmatch.fnmatch(blob.name, file_path + '.zip'):\n",
        "            found_file = True\n",
        "            with open(blob.name, 'wb') as file:\n",
        "                blob.download_to_file(file)\n",
        "            \n",
        "            with zipfile.ZipFile(blob.name, 'r') as zip_ref:\n",
        "                for file_info in zip_ref.infolist():\n",
        "                    file_name = os.path.basename(file_info.filename)\n",
        "                    # Skip directories\n",
        "                    if not file_name:\n",
        "                        continue\n",
        "                    source = zip_ref.open(file_info)\n",
        "                    target_file_path = os.path.join(dir, file_name)\n",
        "                    with open(target_file_path, \"wb\") as target:\n",
        "                        with source:\n",
        "                            target.write(source.read())\n",
        "\n",
        "            # delete the zip file from the local directory\n",
        "            os.remove(blob.name)\n",
        "    if found_file:    \n",
        "        print(\"downloaded file\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"file not found\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_file(file_path):\n",
        "    bucket_name = 'ltmdata'  \n",
        "    dirs = file_path.split('/')\n",
        "    filename = dirs[-1]\n",
        "    dir = '/'.join(dirs[:-1])\n",
        "    # if dir != '':\n",
        "    #     dir += '/'\n",
        "\n",
        "    storage_client = storage.Client.create_anonymous_client()\n",
        "\n",
        "    blobs = storage_client.list_blobs(bucket_name, \n",
        "                                      prefix=dir, \n",
        "                                      delimiter=None)\n",
        "    found_file = False\n",
        "    for blob in blobs:\n",
        "        # only download the file that matches the file_path\n",
        "        if blob.name.endswith('zip') and fnmatch.fnmatch(blob.name, file_path + '.zip'):\n",
        "            found_file = True\n",
        "            #print(\"file found\")\n",
        "    return found_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if DOWNLOAD_DATA:\n",
        "    print('Downloading data...\\n\\n')\n",
        "    \n",
        "    if not GENERATE_UTILITY_PROFILES:\n",
        "        prof_types = [\n",
        "            'evaluation', \n",
        "            'training', \n",
        "            'validation']\n",
        "        for prof_type in prof_types:\n",
        "            print(f'Checking {prof_type} utility profiles...\\n')\n",
        "            all_util_profiles_exist = list()\n",
        "            for gen in generation_list:\n",
        "                for num_cands in num_cands_list:\n",
        "                    for num_voters in num_voters_list:\n",
        "                        for probmodel in prob_models_list:\n",
        "                            filename = f'{prof_type}_utility_profiles/{prof_type}_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl'\n",
        "                            #print(filename)\n",
        "                            if os.path.isfile(filename):\n",
        "                                #print(f'already exists') \n",
        "                                found_file = True\n",
        "                            else: \n",
        "                                found_file = download_file(filename)\n",
        "                                #found_file = check_file(filename)\n",
        "                                if not found_file:\n",
        "                                    print(f'file does not exist: ', filename)\n",
        "                            all_util_profiles_exist.append(found_file)\n",
        "                            \n",
        "            if all(all_util_profiles_exist):\n",
        "                print(f'\\nAll {prof_type} utility profiles exist.\\n\\n********\\n')\n",
        "            else: \n",
        "                print(f'\\nNot all {prof_type} utility profiles exist.  You will need to generate the missing ones.\\n\\n********\\n')\n",
        "\n",
        "    if not GENERATE_LABELS:   \n",
        "        print(f'Checking all labels...\\n')\n",
        "        all_labels_exist = list()\n",
        "        for gen in generation_list:\n",
        "            for num_cands in num_cands_list:\n",
        "                for num_voters in num_voters_list:\n",
        "                    for probmodel in prob_models_list:\n",
        "                        for manip_weight in manip_weight_list:\n",
        "                            for vm in voting_methods_list:\n",
        "                                vm_name = vm.name\n",
        "                                for labeling in labeling_list:\n",
        "                                    filename = f'labels/labels_{gen}_{num_cands}_{num_voters}_{probmodel}_{manip_weight}_{vm_name}_{labeling}.pkl'\n",
        "                                    #print(filename)\n",
        "                                    if os.path.isfile(filename):\n",
        "                                        #print(f'already exists') \n",
        "                                        found_file = True\n",
        "                                    else: \n",
        "                                        found_file = download_file(filename)\n",
        "                                        if not found_file:\n",
        "                                            print(f'file does not exist: ', filename)\n",
        "                                    all_labels_exist.append(found_file)\n",
        "                            \n",
        "        if all(all_labels_exist):\n",
        "            print(f'\\nAll labels exist.\\n\\n********\\n')\n",
        "        else: \n",
        "            print(f'\\nNot all labels exist.  You will need to generate the missing ones.\\n\\n********\\n') \n",
        "\n",
        "\n",
        "    if not TRAIN_MODELS:   \n",
        "        print(f'Checking all models...\\n')\n",
        "        all_models_exist = list()\n",
        "\n",
        "        for agent_infos in agent_infos_list:\n",
        "            for probmodel in prob_models_list:\n",
        "                for labeling in labeling_list:\n",
        "                    for manip_weight in manip_weight_list:\n",
        "                        model_dir = f'models/models_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}'\n",
        "                        os.makedirs(f'{model_dir}', exist_ok=True)\n",
        "                        for vm in voting_methods_list:\n",
        "                            vm_name = vm.name\n",
        "                            for gen in generation_list: \n",
        "                                for num_cands in num_cands_list:\n",
        "                                    for num_voters in num_voters_list:\n",
        "                                        filename = f'{model_dir}/{vm_name}_{gen}_{num_cands}_{num_voters}_*.pickle'\n",
        "                                        \n",
        "                                        #print(filename)\n",
        "\n",
        "                                        # get all files matching this pattern\n",
        "                                        files=glob.glob(filename)\n",
        "\n",
        "                                        if len(files) == 1:\n",
        "                                            #print(f'already exists') \n",
        "                                            found_file = True\n",
        "                                        elif len(files) > 1:\n",
        "                                            print(f'found multiple model files')\n",
        "                                            found_file = False\n",
        "                                        else: # the file does not exist\n",
        "                                            found_file = download_file(filename)\n",
        "                                            if not found_file:\n",
        "                                                print(f'file does not exist: ', filename)\n",
        "                                        all_models_exist.append(found_file)\n",
        "        if all(all_models_exist):\n",
        "            print(f'\\nAll models exist.\\n\\n********\\n')\n",
        "        else: \n",
        "            print(f'\\nNot all models exist.  You will need to generate the missing ones.\\n\\n********\\n\\n')   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2uy2Mb1XORx"
      },
      "outputs": [],
      "source": [
        "def generate_permutations(n):\n",
        "    # Create a list from 0 to n-1\n",
        "    num_list = list(range(n))\n",
        "\n",
        "    # generate all permutations\n",
        "    perms = list(itertools.permutations(num_list))\n",
        "\n",
        "    return perms\n",
        "\n",
        "permutations_of = dict()\n",
        "permutations_of = {\n",
        "    i: generate_permutations(i)\n",
        "    for i in range(3, 7)\n",
        "}\n",
        "\n",
        "permutations_index_dict = {\n",
        "    i : {\n",
        "            tuple(permutations_of[i][j]) : j \n",
        "            for j in range(len(permutations_of[i]))\n",
        "        }\n",
        "    for i in range(3, 7)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFlcbGxhX8L5",
        "outputId": "1a504021-a2ad-4963-9251-ba9cdebe5e7f"
      },
      "outputs": [],
      "source": [
        "print(\"All rankings for 3 candidates:\\n\", permutations_of[3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_linear_prof(uprof):\n",
        "    # Convert utility profile to linear profile. \n",
        "    return pref_voting.profiles.Profile([sorted(uprof.domain, key=lambda x: u(x), reverse=True) for u in uprof.utilities])\n",
        "\n",
        "def clone_voter(prof, manip_weight = 1):\n",
        "    # Duplicate voter in profile (useful for coalitional manipulation)\n",
        "    rankings, rcounts = prof.rankings_counts\n",
        "    rcounts = list(rcounts)\n",
        "    rankings = list([tuple(r) for r in rankings])\n",
        "    new_rcounts = [rcounts[0] + (manip_weight - 1)] + rcounts[1:]\n",
        "    return Profile(rankings, rcounts=new_rcounts)\n",
        "\n",
        "def apply_manipulation(prof_with_clones, new_ranking, manip_weight):\n",
        "    # Apply manipulation in profile with clones given a manipulation weight\n",
        "    return pref_voting.profiles.Profile([new_ranking] * manip_weight + prof_with_clones.rankings[manip_weight:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prof = pref_voting.generate_profiles.generate_profile(3, 4)\n",
        "\n",
        "manip_weight = 1\n",
        "print('Sincere profile:')\n",
        "prof.display()\n",
        "print('')\n",
        "print('Apply manipulation (0, 1, 2):')\n",
        "mprof = apply_manipulation(prof, (0, 1, 2), manip_weight)\n",
        "mprof.display()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4. Agent Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKyMd6Z_YBTr"
      },
      "outputs": [],
      "source": [
        "class Agent(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, classification=False, layers=[128, 64, 32]):\n",
        "        # Basic configurable MLP model.\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = math.factorial(output_dim)  # action space is [0, (output_dim)! - 1]\n",
        "\n",
        "        module_list = []\n",
        "\n",
        "        layers = [input_dim] + layers + [self.output_dim]\n",
        "\n",
        "        for i in range(1, len(layers)):\n",
        "            module_list.append(\n",
        "                nn.Linear(layers[i - 1], layers[i]),\n",
        "            )\n",
        "            if i != len(layers) - 1:\n",
        "                module_list.append(\n",
        "                    nn.LeakyReLU(),\n",
        "                )\n",
        "\n",
        "        if classification:\n",
        "            module_list.append(\n",
        "                nn.Sigmoid(),\n",
        "            )\n",
        "        else:\n",
        "            module_list.append(\n",
        "                nn.Softmax(dim=-1),\n",
        "            )\n",
        "\n",
        "        self.model = nn.Sequential(*module_list)\n",
        "\n",
        "        print(self.model)\n",
        "\n",
        "\n",
        "    def forward(self, manipulator_utilities, additional_contexts): \n",
        "        # manipulator_utilities: [bs, n_c] (utility of voter 0)\n",
        "        # additional_contexts: [bs, n_c]\n",
        "        # returns (action_probs, action)\n",
        "\n",
        "        context = torch.cat([manipulator_utilities, additional_contexts], dim=-1)\n",
        "\n",
        "        action_probs = self.model(context)\n",
        "\n",
        "        dist=torch.distributions.categorical.Categorical(probs=action_probs)\n",
        "        actions = dist.sample()\n",
        "\n",
        "        # action_probs: [BS, num_possible_actions]\n",
        "        # actions: [BS,]\n",
        "        return action_probs, actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5. Manipulator Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_score_context(profs, scoring_rule = 'plurality', device=DEVICE):\n",
        "    # generates score contexts from utility profile for each candidate\n",
        "    #\n",
        "    # scoring_rule: ('plurality', 'borda')\n",
        "    # returns: [bs, num_cands]\n",
        "\n",
        "    if scoring_rule == 'plurality':\n",
        "        scores = [ prof.plurality_scores() for prof in profs] # list of dicts\n",
        "    elif scoring_rule == 'borda':\n",
        "        scores = [ prof.borda_scores() for prof in profs] # list of dicts\n",
        "\n",
        "    final_scores = []\n",
        "    for score in scores:\n",
        "\n",
        "        scores_list = []\n",
        "        for cand_index in sorted(score.keys()):\n",
        "            scores_list.append(score[cand_index])\n",
        "\n",
        "        final_scores.append(scores_list)\n",
        "\n",
        "    return torch.tensor(\n",
        "        final_scores,\n",
        "        device=device,\n",
        "    ) # [bs, num_cands]\n",
        "\n",
        "def generate_majority_contexts(profs, num_cands, device=DEVICE):\n",
        "    # generates majority matrix contexts\n",
        "    # \n",
        "    # profs: list of profiles\n",
        "    # outputs: [bs, num_cands * num_cands]\n",
        "\n",
        "    bs = len(profs)\n",
        "    \n",
        "    contexts = torch.zeros((bs, num_cands, num_cands), device=device)\n",
        "    \n",
        "    for pidx, prof in enumerate(profs): \n",
        "        for c1 in prof.candidates: \n",
        "            for c2 in prof.candidates: \n",
        "                if prof.majority_prefers(c1, c2):\n",
        "                    contexts[pidx, c1, c2] = 1.0\n",
        "                elif prof.majority_prefers(c2, c1): \n",
        "                    contexts[pidx, c1, c2] = -1.0\n",
        "                else:\n",
        "                    contexts[pidx, c1, c2] = 0.0\n",
        "\n",
        "    contexts = torch.flatten(contexts, start_dim=1) # [bs, num_cands * num_cands]\n",
        "    return contexts\n",
        "\n",
        "\n",
        "def generate_sincere_winners_contexts(profs, num_cands, vm, device=DEVICE):\n",
        "    # generates sincere winners contexts\n",
        "    # \n",
        "    # profs: list of profiles\n",
        "    # vm: voting method function\n",
        "    # outputs: [bs, num_cands] (one-hot encoding)\n",
        "\n",
        "    bs = len(profs)\n",
        "\n",
        "    contexts = torch.zeros((bs, num_cands), device=device)\n",
        "\n",
        "    for pidx, prof in enumerate(profs):\n",
        "        ws = vm(prof)\n",
        "        for c in ws:\n",
        "            contexts[pidx, c] = 1.0\n",
        "\n",
        "    return contexts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scores_to_qual_scores(scores):\n",
        "\n",
        "    # sort the scores\n",
        "    sorted_scores = sorted(list(set(scores)))\n",
        "\n",
        "    return [sorted_scores.index(s) + 1 for s in scores]\n",
        "\n",
        "def generate_score_ranking_context(profs, scoring_rule = 'plurality', device=DEVICE):\n",
        "    # generates score rankings from utility profiles for each candidate\n",
        "    #\n",
        "    # profs: list of profiles\n",
        "    # scoring_rule: ('plurality', 'borda')\n",
        "    # returns: [bs, num_cands]\n",
        "\n",
        "    if scoring_rule == 'plurality':\n",
        "        scores = [ prof.plurality_scores() for prof in profs] # list of dicts\n",
        "    elif scoring_rule == 'borda':\n",
        "        scores = [ prof.borda_scores() for prof in profs] # list of dicts\n",
        "\n",
        "    final_rankings = []\n",
        "    for score in scores:\n",
        "\n",
        "        scores_list = []\n",
        "        for cand_index in sorted(score.keys()):\n",
        "            scores_list.append(score[cand_index])\n",
        "\n",
        "        final_rankings.append(scores_to_qual_scores(scores_list))\n",
        "\n",
        "    return torch.tensor(\n",
        "        final_rankings,\n",
        "        device=device,\n",
        "    ) # [bs, num_cands]\n",
        "\n",
        "def generate_full_profile_contexts(profs, num_cands, num_voters, device=DEVICE):\n",
        "    # generates full profile contexts\n",
        "    # \n",
        "    # profs: list of profiles\n",
        "    # outputs: [bs, num_voters * num_actions]\n",
        "\n",
        "    bs = len(profs)\n",
        "    \n",
        "    action_space_size = math.factorial(num_cands)\n",
        "\n",
        "    contexts = torch.zeros((bs, num_voters, action_space_size), device=device)\n",
        "\n",
        "    # generate linear rankings as list comprehension\n",
        "\n",
        "    for pidx, prof in enumerate(profs): \n",
        "        for ridx, r in enumerate(prof.rankings): \n",
        "            contexts[pidx, ridx, permutations_index_dict[num_cands][tuple(r)]] = 1.0\n",
        "\n",
        "    contexts = torch.flatten(contexts, start_dim=1) # [bs, num_voters * num_actions]\n",
        "    return contexts\n",
        "\n",
        "def generate_anon_prof_contexts(profs, num_cands,  device=DEVICE):\n",
        "    # generates anonymous profile contexts\n",
        "    # \n",
        "    # profs: list of profiles\n",
        "    # outputs: [bs, num_actions]\n",
        "\n",
        "    bs = len(profs)\n",
        "    \n",
        "    action_space_size = math.factorial(num_cands)\n",
        "\n",
        "    contexts = torch.zeros((bs, action_space_size), device=device)\n",
        "    \n",
        "    for pidx, prof in enumerate(profs):\n",
        "        rankings, counts = prof.rankings_counts \n",
        "        for ridx, r in enumerate(rankings): \n",
        "            contexts[pidx, permutations_index_dict[num_cands][tuple(r)]] = counts[ridx]\n",
        "\n",
        "    return contexts\n",
        "\n",
        "def generate_margin_contexts(profs, num_cands, device=DEVICE):\n",
        "    # generates margin matrix contexts\n",
        "    # \n",
        "    # profs: list of profiles\n",
        "    # outputs: [bs, num_cands * num_cands]\n",
        "\n",
        "    bs = len(profs)\n",
        "    \n",
        "    contexts = torch.zeros((bs, num_cands, num_cands), device=device)\n",
        "    \n",
        "    for pidx, prof in enumerate(profs): \n",
        "        for c1 in prof.candidates: \n",
        "            for c2 in prof.candidates: \n",
        "                contexts[pidx, c1, c2] = prof.margin(c1, c2)\n",
        "\n",
        "    contexts = torch.flatten(contexts, start_dim=1) # [bs, num_cands * num_cands]\n",
        "    return contexts\n",
        "\n",
        "def generate_qual_margin_contexts(profs, num_cands, device=DEVICE):\n",
        "    # generates qualitative margin contexts\n",
        "    # \n",
        "    # profs: list of profiles\n",
        "    # outputs: [bs, num_cands * num_cands]\n",
        "\n",
        "    bs = len(profs)\n",
        "    \n",
        "    contexts = torch.zeros((bs, num_cands, num_cands), device=device)\n",
        "    \n",
        "    for pidx, prof in enumerate(profs): \n",
        "        pos_margins = [prof.margin(c1, c2) for c1 in prof.candidates for c2 in prof.candidates if prof.margin(c1, c2) > 0]\n",
        "        qual_pos_margins = scores_to_qual_scores(pos_margins)\n",
        "        pos_margin_to_qual_margin = dict(zip(pos_margins, qual_pos_margins))\n",
        "        for c1 in prof.candidates: \n",
        "            for c2 in prof.candidates: \n",
        "                if prof.margin(c1, c2) > 0:\n",
        "                    contexts[pidx, c1, c2] = pos_margin_to_qual_margin[prof.margin(c1, c2)]\n",
        "                elif prof.margin(c1, c2) < 0:\n",
        "                    contexts[pidx, c1, c2] = -1.0 * pos_margin_to_qual_margin[-prof.margin(c1, c2)]\n",
        "                else:\n",
        "                    contexts[pidx, c1, c2] = 0.0\n",
        "\n",
        "    contexts = torch.flatten(contexts, start_dim=1) # [bs, num_cands * num_cands]\n",
        "    return contexts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.6 Labeling Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.6.1 Satisficing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def profitable_manipulations(vm, uprof, manip_weight):\n",
        "    # Calculates all profitable manipulations for voter 0 in a utility profile\n",
        "\n",
        "    u = uprof.utilities[0]\n",
        "    prof = to_linear_prof(uprof)\n",
        "    prof_with_clones = clone_voter(prof, manip_weight=manip_weight)\n",
        "    strat_rankings = list()\n",
        "    for new_r in itertools.permutations(prof.candidates):\n",
        "        if new_r != prof.rankings[0]:\n",
        "            new_prof = apply_manipulation(prof_with_clones, new_r, manip_weight)\n",
        "            ws = vm(prof)\n",
        "            new_ws = vm(new_prof)\n",
        "            if ws != new_ws:\n",
        "                exp_u_ws  = np.average([u(w) for w in ws])\n",
        "                new_exp_u_ws = np.average([u(w) for w in new_ws])\n",
        "                if new_exp_u_ws > exp_u_ws:\n",
        "                    strat_rankings.append(new_r)\n",
        "    return strat_rankings\n",
        "\n",
        "\n",
        "def mask_manipulations(election, vm, manip_weight):\n",
        "    # Generates the binary mask of which manipulations are\n",
        "    # profitable.\n",
        "    # \n",
        "    # election is a tuple of (utility_fn, profile)\n",
        "    \n",
        "    utility_fn, prof = election\n",
        "\n",
        "    prof_with_clones = clone_voter(prof, manip_weight=manip_weight)\n",
        "\n",
        "    output_mask = torch.zeros((math.factorial(len(prof.candidates)),))\n",
        "\n",
        "    ws = vm(prof)\n",
        "    exp_u_ws  = np.average([utility_fn(w) for w in ws])\n",
        "\n",
        "    for i, new_r in enumerate(itertools.permutations(prof.candidates)):\n",
        "        new_prof = apply_manipulation(prof_with_clones, new_r, manip_weight)\n",
        "\n",
        "        new_ws = vm(new_prof)\n",
        "        if new_ws == ws:\n",
        "            output_mask[i] = 0.0\n",
        "        else:\n",
        "            new_exp_u_ws = np.average([utility_fn(w) for w in new_ws])\n",
        "            if new_exp_u_ws > exp_u_ws:\n",
        "                output_mask[i] = 1.0\n",
        "            else:\n",
        "                output_mask[i] = -1.0\n",
        "    return output_mask\n",
        "\n",
        "\n",
        "def generate_labels_reduced_actions(\n",
        "    action_dists, \n",
        "    utility_fns, \n",
        "    profs, \n",
        "    num_cands, \n",
        "    vm, \n",
        "    manip_weight, \n",
        "    reduction_contexts=None\n",
        "):\n",
        "    # Reduce the distribution of actions (action_dists) to the probability\n",
        "    # of choosing a profitable manipulation.\n",
        "\n",
        "    # Returns: output_dists (bs, 2), output_labels (bs, 2)\n",
        "\n",
        "    batch_size = len(profs)\n",
        "\n",
        "    pool = multiprocess.Pool(processes=cpus)\n",
        "\n",
        "    if reduction_contexts is None:\n",
        "        manipulation_responses = pool.map(\n",
        "            lambda x: mask_manipulations(x, vm, manip_weight),\n",
        "            zip(utility_fns, profs)\n",
        "        )\n",
        "\n",
        "        masks = torch.stack(\n",
        "            manipulation_responses\n",
        "        ).to(action_dists.device) # [bs, num_actions]\n",
        "    else:\n",
        "        masks = reduction_contexts\n",
        "\n",
        "    positive_mask = masks > 0 # [bs, num_actions]\n",
        "\n",
        "    # check for places in the batch where there is no positive choice\n",
        "    no_positive = ~positive_mask.any(dim=-1) # [bs, ]\n",
        "\n",
        "    positive_mask[no_positive] = masks[no_positive] >= 0\n",
        "\n",
        "    negative_mask = ~positive_mask\n",
        "\n",
        "\n",
        "    output_dists = torch.zeros((batch_size, 2)).to(action_dists.device) # [pos, neg]\n",
        "\n",
        "    output_dists[:, 0] = torch.sum(action_dists * positive_mask, dim=-1)\n",
        "    output_dists[:, 1] = torch.sum(action_dists * negative_mask, dim=-1)\n",
        "\n",
        "    output_labels = torch.zeros_like(output_dists)\n",
        "    output_labels[:, 0] = 1.0\n",
        "\n",
        "    return output_dists, output_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  1.6.2. Optimizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_best_response_single(utility_fn, prof, vm, manip_weight):\n",
        "    # Find best response(s) for voter 0 in an election\n",
        "    # \n",
        "    # Return: list of best responses\n",
        "\n",
        "    # process = psutil.Process(os.getpid())\n",
        "    #print(f\"Worker {os.getpid()}: Memory usage: {process.memory_info().rss / (1024 ** 2)} MB\")\n",
        "\n",
        "    prof_with_clones = clone_voter(prof, manip_weight=manip_weight)\n",
        "\n",
        "    ws = vm(prof)\n",
        "    eu_ws = np.average([utility_fn(w) for w in ws])\n",
        "    best_rankings = list()\n",
        "    current_best_eu = eu_ws\n",
        "    for new_ranking in itertools.permutations(prof.candidates):\n",
        "        new_prof = apply_manipulation(prof_with_clones, new_ranking, manip_weight)\n",
        "        new_ws = vm(new_prof)\n",
        "        new_eu_ws = np.average([utility_fn(w) for w in new_ws])\n",
        "        if new_eu_ws > current_best_eu:\n",
        "            current_best_eu = new_eu_ws\n",
        "            best_rankings = [new_ranking]\n",
        "        elif new_eu_ws == current_best_eu:\n",
        "            best_rankings.append(new_ranking)\n",
        "    return best_rankings\n",
        "\n",
        "\n",
        "def threaded_generate_classification_labels(utility_fns, profs, num_cands, vm, manip_weight, chunk_size=1000):\n",
        "    def find_best_response_wrapper(args):\n",
        "        utility_fn, prof = args\n",
        "        return find_best_response_single(utility_fn, prof, vm, manip_weight)\n",
        "    \n",
        "    batch_size = len(profs)\n",
        "    output = torch.zeros((batch_size, len(permutations_of[num_cands])))\n",
        "\n",
        "    pool = multiprocess.Pool(processes=cpus)\n",
        "    for i in range(0, batch_size, chunk_size):\n",
        "        chunk_utility_fns = utility_fns[i:i + chunk_size]\n",
        "        chunk_profs = profs[i:i + chunk_size]\n",
        "        best_responses = pool.map(find_best_response_wrapper, zip(chunk_utility_fns, chunk_profs))\n",
        "\n",
        "        for j in range(len(best_responses)):\n",
        "            best_response = best_responses[j]\n",
        "            for k, perm in enumerate(permutations_of[num_cands]):\n",
        "                if perm in best_response:\n",
        "                    output[i + j, k] = 1.0\n",
        "\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return output\n",
        "\n",
        "def generate_labels_reduced_actions_opt(\n",
        "    action_dists, \n",
        "    utility_fns, \n",
        "    profs, \n",
        "    num_cands, \n",
        "    vm, \n",
        "    manip_weight, \n",
        "    reduction_contexts=None):\n",
        "    # Reduce the distribution of actions (action_dists) to the probability\n",
        "    # of choosing an optimal manipulation. \n",
        "\n",
        "    # Uses threaded_generate_classification_labels to generate the labels.\n",
        "\n",
        "    # Returns: output_dists (bs, 2), output_labels (bs, 2)\n",
        "\n",
        "    batch_size = len(profs)\n",
        "\n",
        "    if reduction_contexts is None:\n",
        "        optimal_labels = threaded_generate_classification_labels(\n",
        "            utility_fns,\n",
        "            profs,\n",
        "            num_cands=num_cands, \n",
        "            vm=vm, \n",
        "            manip_weight=manip_weight, \n",
        "            chunk_size=5_000\n",
        "        ).to(action_dists.device)\n",
        "    else:\n",
        "        optimal_labels = reduction_contexts\n",
        "\n",
        "    output_dists = torch.zeros((batch_size, 2)).to(action_dists.device) # [pos, neg]\n",
        "\n",
        "    positive_mask = optimal_labels == 1\n",
        "    negative_mask = ~positive_mask\n",
        "\n",
        "    output_dists[:, 0] = torch.sum(action_dists * positive_mask, dim = -1)\n",
        "    output_dists[:, 1] = torch.sum(action_dists * negative_mask, dim=-1)\n",
        "\n",
        "    output_labels = torch.zeros_like(output_dists)\n",
        "    output_labels[:, 0] = 1.0\n",
        "\n",
        "    return output_dists, output_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.7. Validation setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reward_function(\n",
        "    actions, \n",
        "    utility_fns, \n",
        "    profs, \n",
        "    vm, \n",
        "    num_cands, \n",
        "    manip_weight, \n",
        "    metric_op='normalized_subtract'):\n",
        "    # find the profitability of manipulation (as in Equation 4 from the paper)\n",
        "    # for voter 0 in each profile.\n",
        "\n",
        "    # actions = [BS,]\n",
        "    # uprofs = list of uprofs\n",
        "    # vm = voting method fn\n",
        "\n",
        "    profs_with_clones = [clone_voter(prof, manip_weight=manip_weight) for prof in profs]\n",
        "\n",
        "    ws_batch = [vm(prof) for prof in profs_with_clones]\n",
        "    cands_batch = [prof.candidates for prof in profs_with_clones]\n",
        "\n",
        "    exp_util_ws_batch = torch.tensor([\n",
        "        np.average([utility_fn(w) for w in ws])\n",
        "        for utility_fn, ws in zip(utility_fns, ws_batch)\n",
        "    ]).float() # [BS,]\n",
        "    exp_util_ws_batch = exp_util_ws_batch.to(DEVICE)\n",
        "\n",
        "    max_util_batch = torch.tensor([\n",
        "        np.max([utility_fn(c) for c in cands])\n",
        "        for utility_fn, cands in zip(utility_fns, cands_batch)\n",
        "    ]).float() # [BS,]\n",
        "    max_util_batch = max_util_batch.to(DEVICE)\n",
        "\n",
        "    min_util_batch = torch.tensor([\n",
        "        np.min([utility_fn(c) for c in cands])\n",
        "        for utility_fn, cands in zip(utility_fns, cands_batch)\n",
        "    ]).float() # [BS,]\n",
        "    min_util_batch = min_util_batch.to(DEVICE)\n",
        "\n",
        "    new_profs = [\n",
        "        apply_manipulation(prof, permutations_of[num_cands][action], manip_weight)\n",
        "        for prof, action in zip(profs_with_clones, actions)\n",
        "    ]\n",
        "\n",
        "    new_ws_batch = [vm(new_prof) for new_prof in new_profs]\n",
        "    \n",
        "    new_exp_util_ws_batch = torch.tensor([\n",
        "        np.average([utility_fn(w) for w in new_ws])\n",
        "        for utility_fn, new_ws in zip(utility_fns, new_ws_batch)\n",
        "    ]).float() # [BS,]\n",
        "\n",
        "    new_exp_util_ws_batch = new_exp_util_ws_batch.to(DEVICE)\n",
        "\n",
        "    if metric_op == 'subtract':\n",
        "        reward = new_exp_util_ws_batch - exp_util_ws_batch\n",
        "    elif metric_op == 'normalized_subtract':\n",
        "        reward = (new_exp_util_ws_batch - exp_util_ws_batch) / (max_util_batch - min_util_batch)\n",
        "    # create mask for which indices are different\n",
        "    # add the cost to those indices\n",
        "\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validation_function(\n",
        "    agent, \n",
        "    batch_size, \n",
        "    vm, \n",
        "    num_cands, \n",
        "    num_voters, \n",
        "    manip_weight, \n",
        "    elections, \n",
        "    decision_rule='argmax', \n",
        "    metric_op=\"normalized_subtract\", \n",
        "    agent_infos=('plurality_scores',)):\n",
        "    # computes the profitability of submitted rankings chosen by \n",
        "    # the agent according to the decision_rule.\n",
        "\n",
        "    manipulator_utility_fns, profiles = elections\n",
        "\n",
        "    manipulator_utilities = torch.tensor(\n",
        "        [\n",
        "            [m_util_fn(i) for i in range(num_cands)]\n",
        "            for m_util_fn in manipulator_utility_fns\n",
        "        ],\n",
        "    ).float().to(DEVICE)\n",
        "\n",
        "    additional_contexts = None # guarantee that this is of shape [bs, x]\n",
        "\n",
        "    additional_contexts = [] # guarantee that each entry is of shape [bs, x]\n",
        "\n",
        "    for agent_info in agent_infos:\n",
        "        additional_context = None\n",
        "\n",
        "        if agent_info == 'full':\n",
        "            additional_context = generate_full_profile_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                num_voters=num_voters,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'anon_prof':\n",
        "            additional_context = generate_anon_prof_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE,\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'plurality_scores':\n",
        "\n",
        "            additional_context = generate_score_context(\n",
        "                profiles,\n",
        "                scoring_rule='plurality',\n",
        "                device=DEVICE,\n",
        "            ).float()\n",
        "            \n",
        "        elif agent_info == 'plurality_ranking':\n",
        "\n",
        "            additional_context = generate_score_ranking_context(\n",
        "                profiles,\n",
        "                scoring_rule='plurality',\n",
        "                device=DEVICE,\n",
        "            ).float()\n",
        "            \n",
        "        elif agent_info == 'borda_scores':\n",
        "            additional_context = generate_score_context(\n",
        "                profiles,\n",
        "                scoring_rule='borda',\n",
        "                device=DEVICE,\n",
        "            ).float()\n",
        "\n",
        "        elif agent_info == 'margin':\n",
        "            additional_context = generate_margin_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE\n",
        "            )\n",
        "        elif agent_info == 'qual_margin':\n",
        "            additional_context = generate_qual_margin_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'majority':\n",
        "            additional_context = generate_majority_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "\n",
        "        elif agent_info == 'sincere_winners':\n",
        "            additional_context = generate_sincere_winners_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                vm=vm,\n",
        "                device=DEVICE\n",
        "            )\n",
        "            \n",
        "        additional_contexts.append(additional_context)\n",
        "\n",
        "    additional_contexts = torch.cat(additional_contexts, dim=-1)\n",
        "\n",
        "    action_probs_batch, actions_batch = agent(manipulator_utilities, additional_contexts)\n",
        "\n",
        "    if decision_rule == 'expectation':\n",
        "\n",
        "        eval_result = torch.zeros((batch_size,)).to(DEVICE)\n",
        "\n",
        "        for i in range(action_probs_batch.shape[-1]):\n",
        "            actions_batch = torch.ones_like(actions_batch) * i\n",
        "\n",
        "            reward_val = reward_function(\n",
        "                actions=actions_batch,\n",
        "                utility_fns=manipulator_utility_fns,\n",
        "                profs=profiles,\n",
        "                vm=vm,\n",
        "                num_cands=num_cands,\n",
        "                manip_weight=manip_weight,\n",
        "                metric_op=metric_op,\n",
        "            )\n",
        "\n",
        "            eval_result += reward_val * action_probs_batch[:, i]\n",
        "    elif decision_rule == 'argmax':\n",
        "        # using the argmax\n",
        "        actions_batch = torch.argmax(action_probs_batch, dim=-1)\n",
        "        eval_result = reward_function(\n",
        "                actions=actions_batch,\n",
        "                utility_fns=manipulator_utility_fns,\n",
        "                profs=profiles,\n",
        "                vm=vm,\n",
        "                num_cands=num_cands,\n",
        "                manip_weight=manip_weight,\n",
        "                metric_op=metric_op,\n",
        "            )\n",
        "    elif decision_rule == 'distribution':\n",
        "        eval_result = reward_function(\n",
        "            actions=actions_batch,\n",
        "            utility_fns=manipulator_utility_fns,\n",
        "            profs=profiles,\n",
        "            vm=vm,\n",
        "            num_cands=num_cands,\n",
        "            manip_weight=manip_weight,\n",
        "            metric_op=metric_op,\n",
        "        )\n",
        "    else:\n",
        "        raise Exception(\"pick one\")\n",
        "    \n",
        "    return eval_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.8. Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDC6GiL69Dmh"
      },
      "outputs": [],
      "source": [
        "def train_strategy_classification(\n",
        "        num_voters, \n",
        "        num_candidates, \n",
        "        training_elections,\n",
        "        validation_elections,\n",
        "        model_size=[128, 64, 32], \n",
        "        vm=pref_voting.voting_methods.plurality,\n",
        "        max_num_iterations = 2000,\n",
        "        validate_every = 20, \n",
        "        reduction_contexts = None, \n",
        "        training_batch_size = 512, \n",
        "        validation_batch_size = 4096, \n",
        "        patience = 20, \n",
        "        threshold = 0.001, \n",
        "        learning_rate = 6e-3, \n",
        "        labeling='satisfice', \n",
        "        agent_infos=[('plurality_scores',)], \n",
        "        manip_weight = 1):  \n",
        "    # the main training function, including periodic validation checks. \n",
        "\n",
        "    input_dim = 0\n",
        "\n",
        "    input_dim += num_candidates\n",
        "\n",
        "    agent_infos = sorted(agent_infos)\n",
        "\n",
        "    # additional input dimension for each type of context\n",
        "    dim_additions = {\n",
        "        'full': num_voters * math.factorial(num_candidates),\n",
        "        'anon_prof':  math.factorial(num_candidates),\n",
        "        'plurality_scores': num_candidates,\n",
        "        'plurality_ranking': num_candidates,\n",
        "        'borda_scores': num_candidates,\n",
        "        'margin': num_candidates * num_candidates,\n",
        "        'qual_margin': num_candidates * num_candidates,\n",
        "        'majority': num_candidates * num_candidates,\n",
        "        'sincere_winners': num_candidates,\n",
        "    }\n",
        "\n",
        "    for agent_info in agent_infos:\n",
        "        input_dim += dim_additions[agent_info]\n",
        "\n",
        "    # Create agent\n",
        "    agent = Agent(\n",
        "        input_dim=input_dim,\n",
        "        output_dim=num_candidates, classification=False,\n",
        "        layers=model_size,\n",
        "    )\n",
        "\n",
        "    agent = agent.to(DEVICE)\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # set up optimizer\n",
        "    optimizer = torch.optim.Adam(\n",
        "        agent.parameters(),\n",
        "        learning_rate, \n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    train_evals = []\n",
        "    #assert len(elections[0]) >= max_num_iterations * batch_size and len(elections[0]) == len(elections[1])\n",
        "\n",
        "    best_eval = float('-inf')\n",
        "    patience_counter = 0\n",
        "    val_iter = 0\n",
        "    for iter in tqdm(range(max_num_iterations)):\n",
        "\n",
        "        #print(\"[INFO] iter: {}...\".format(iter + 1))\n",
        "\n",
        "        # build batch up\n",
        "\n",
        "        iter_training_elections = (\n",
        "            training_elections[0][iter * training_batch_size: iter * training_batch_size + training_batch_size],\n",
        "            training_elections[1][iter * training_batch_size: iter * training_batch_size + training_batch_size]\n",
        "        )\n",
        "\n",
        "        iter_manipulator_utility_functions = iter_training_elections[0]\n",
        "        iter_profiles = iter_training_elections[1]\n",
        "\n",
        "        iter_reduction_contexts = reduction_contexts[iter * training_batch_size: iter * training_batch_size + training_batch_size]\n",
        "        \n",
        "        manipulator_utilities = torch.tensor(\n",
        "            [\n",
        "                [m_util_fn(i) for i in range(num_candidates)]\n",
        "                for m_util_fn in iter_manipulator_utility_functions\n",
        "            ],\n",
        "        ).float().to(DEVICE)\n",
        "\n",
        "        additional_contexts = [] # guarantee that each entry is of shape [bs, x]\n",
        "\n",
        "        for agent_info in agent_infos:\n",
        "            additional_context = None\n",
        "\n",
        "            if agent_info == 'full':\n",
        "                additional_context = generate_full_profile_contexts(\n",
        "                    iter_profiles,\n",
        "                    num_cands=num_candidates,\n",
        "                    num_voters=num_voters,\n",
        "                    device=DEVICE\n",
        "                )\n",
        "\n",
        "            elif agent_info == 'anon_prof':\n",
        "                additional_context = generate_anon_prof_contexts(\n",
        "                    iter_profiles,\n",
        "                    num_cands=num_candidates,\n",
        "                    device=DEVICE,\n",
        "                )\n",
        "\n",
        "            elif agent_info == 'plurality_scores':\n",
        "\n",
        "                additional_context = generate_score_context(\n",
        "                    iter_profiles,\n",
        "                    scoring_rule='plurality',\n",
        "                    device=DEVICE,\n",
        "                ).float()\n",
        "                \n",
        "            elif agent_info == 'borda_scores':\n",
        "                additional_context = generate_score_context(\n",
        "                    iter_profiles,\n",
        "                    scoring_rule='borda',\n",
        "                    device=DEVICE,\n",
        "                ).float()\n",
        "\n",
        "            elif agent_info == 'plurality_ranking':\n",
        "\n",
        "                additional_context = generate_score_ranking_context(\n",
        "                    iter_profiles,\n",
        "                    scoring_rule='plurality',\n",
        "                    device=DEVICE,\n",
        "                ).float()\n",
        "\n",
        "            elif agent_info == 'margin':\n",
        "                additional_context = generate_margin_contexts(\n",
        "                    iter_profiles,\n",
        "                    num_cands=num_candidates,\n",
        "                    device=DEVICE\n",
        "                )\n",
        "\n",
        "            elif agent_info == 'majority':\n",
        "                additional_context = generate_majority_contexts(\n",
        "                    iter_profiles,\n",
        "                    num_cands=num_candidates,\n",
        "                    device=DEVICE\n",
        "                )\n",
        "            elif agent_info == 'qual_margin':\n",
        "                additional_context = generate_qual_margin_contexts(\n",
        "                    iter_profiles,\n",
        "                    num_cands=num_candidates,\n",
        "                    device=DEVICE\n",
        "                )\n",
        "\n",
        "            elif agent_info == 'sincere_winners':\n",
        "                additional_context = generate_sincere_winners_contexts(\n",
        "                    iter_profiles,\n",
        "                    num_cands=num_candidates,\n",
        "                    vm=vm,\n",
        "                    device=DEVICE\n",
        "                )\n",
        "            \n",
        "            additional_contexts.append(additional_context)\n",
        "\n",
        "        additional_contexts = torch.cat(additional_contexts, dim=-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # get agent actions\n",
        "        action_probs_batch, actions_batch = agent(manipulator_utilities, additional_contexts)\n",
        "\n",
        "        # print(action_probs_batch)\n",
        "\n",
        "        if labeling == 'satisfice':\n",
        "\n",
        "            reduced_actions, labels = generate_labels_reduced_actions(\n",
        "                action_probs_batch, \n",
        "                iter_manipulator_utility_functions,\n",
        "                iter_profiles,\n",
        "                num_candidates, \n",
        "                vm=vm,\n",
        "                manip_weight=manip_weight,\n",
        "                reduction_contexts=iter_reduction_contexts,\n",
        "            )\n",
        "\n",
        "        elif labeling == 'optimize':\n",
        "\n",
        "            reduced_actions, labels = generate_labels_reduced_actions_opt(\n",
        "                action_probs_batch, \n",
        "                iter_manipulator_utility_functions,\n",
        "                iter_profiles,\n",
        "                num_candidates, \n",
        "                vm=vm,\n",
        "                manip_weight=manip_weight,\n",
        "                reduction_contexts=iter_reduction_contexts,\n",
        "            )\n",
        "\n",
        "        loss = loss_fn(reduced_actions, labels)\n",
        "        #print(\"loss:\", loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        if (iter + 1) % validate_every == 0:\n",
        "            with torch.no_grad():\n",
        "                agent.eval()\n",
        "                iter_validation_elections = (\n",
        "                validation_elections[0][val_iter * validation_batch_size: val_iter * validation_batch_size + validation_batch_size],\n",
        "                validation_elections[1][val_iter * validation_batch_size: val_iter * validation_batch_size + validation_batch_size]\n",
        "                )\n",
        "                iter_evals = validation_function(agent, validation_batch_size, vm, num_candidates, num_voters, manip_weight, iter_validation_elections, decision_rule='argmax',metric_op=\"normalized_subtract\", agent_infos=agent_infos)\n",
        "                iter_eval = iter_evals.mean().cpu().detach().numpy()\n",
        "                print(\"iter_eval\", iter_eval)\n",
        "                train_evals.append(iter_eval)\n",
        "\n",
        "                val_iter += 1\n",
        "\n",
        "                if iter_eval - best_eval > threshold: \n",
        "                    best_eval = iter_eval\n",
        "                    patience_counter = 0\n",
        "                    best_agent = copy.deepcopy(agent)\n",
        "                    best_losses = train_losses\n",
        "                    best_evals = train_evals\n",
        "                else: \n",
        "                    patience_counter += 1\n",
        "\n",
        "                if patience_counter >= patience: \n",
        "                    print(\"best eval is \", best_eval)\n",
        "                    break\n",
        "            agent.train()\n",
        "    \n",
        "    return best_agent, best_losses, best_evals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.9. Evaluation setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluation_function(\n",
        "    agent, \n",
        "    vm, \n",
        "    num_cands, \n",
        "    num_voters, \n",
        "    manip_weight, \n",
        "    all_utility_profs, \n",
        "    decision_rule, \n",
        "    metric_op, \n",
        "    agent_infos, \n",
        "    num_samples, \n",
        "    step):\n",
        "    # evaluate the trained agent's submitted rankings\n",
        "\n",
        "    utility_profiles = all_utility_profs[num_samples*step:num_samples*(step+1)]\n",
        "\n",
        "    manipulator_utility_fns = [uprof.utilities[0] for uprof in utility_profiles]\n",
        "    profiles = [to_linear_prof(uprof) for uprof in utility_profiles]\n",
        "\n",
        "    manipulator_utilities = torch.tensor(\n",
        "        [\n",
        "            [m_util_fn(i) for i in range(num_cands)]\n",
        "            for m_util_fn in manipulator_utility_fns\n",
        "        ],\n",
        "    ).float().to(DEVICE)\n",
        "\n",
        "    additional_contexts = None # guarantee that this is of shape [bs, x]\n",
        "\n",
        "    additional_contexts = [] # guarantee that each entry is of shape [bs, x]\n",
        "\n",
        "    for agent_info in agent_infos:\n",
        "        additional_context = None\n",
        "\n",
        "        if agent_info == 'full':\n",
        "            additional_context = generate_full_profile_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                num_voters=num_voters,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'anon_prof':\n",
        "            additional_context = generate_anon_prof_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE,\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'plurality_scores':\n",
        "\n",
        "            additional_context = generate_score_context(\n",
        "                profiles,\n",
        "                scoring_rule='plurality',\n",
        "                device=DEVICE,\n",
        "            ).float()\n",
        "            \n",
        "        elif agent_info == 'plurality_ranking':\n",
        "\n",
        "            additional_context = generate_score_ranking_context(\n",
        "                profiles,\n",
        "                scoring_rule='plurality',\n",
        "                device=DEVICE,\n",
        "            ).float()\n",
        "            \n",
        "        elif agent_info == 'borda_scores':\n",
        "            additional_context = generate_score_context(\n",
        "                profiles,\n",
        "                scoring_rule='borda',\n",
        "                device=DEVICE,\n",
        "            ).float()\n",
        "\n",
        "        elif agent_info == 'margin':\n",
        "            additional_context = generate_margin_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE\n",
        "            )\n",
        "        elif agent_info == 'qual_margin':\n",
        "            additional_context = generate_qual_margin_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'majority':\n",
        "            additional_context = generate_majority_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'sincere_winners':\n",
        "            additional_context = generate_sincere_winners_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                vm=vm,\n",
        "                device=DEVICE\n",
        "            )\n",
        "            \n",
        "        additional_contexts.append(additional_context)\n",
        "\n",
        "    additional_contexts = torch.cat(additional_contexts, dim=-1)\n",
        "\n",
        "    action_probs_batch, actions_batch = agent(manipulator_utilities, additional_contexts)\n",
        "\n",
        "    if decision_rule == 'argmax':\n",
        "        actions_batch = torch.argmax(action_probs_batch, dim=-1)\n",
        "        eval_result = reward_function(\n",
        "                actions=actions_batch,\n",
        "                utility_fns=manipulator_utility_fns,\n",
        "                profs=profiles,\n",
        "                vm=vm,\n",
        "                num_cands=num_cands,\n",
        "                manip_weight=manip_weight,\n",
        "                metric_op=metric_op,\n",
        "            )\n",
        "\n",
        "    elif decision_rule == 'distribution':\n",
        "        eval_result = reward_function(\n",
        "            actions=actions_batch,\n",
        "            utility_fns=manipulator_utility_fns,\n",
        "            profs=profiles,\n",
        "            vm=vm,\n",
        "            num_cands=num_cands,\n",
        "            manip_weight=manip_weight,\n",
        "            metric_op=metric_op,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"pick one\")\n",
        "    \n",
        "    return np.array([eval_result.cpu().detach().numpy()])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Generate utility profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_utility_profile(\n",
        "    num_cands, \n",
        "    num_voters, \n",
        "    probmodel = 'uniform', \n",
        "    num_profiles = 1\n",
        "):\n",
        "    # Generate utility profiles\n",
        "    # \n",
        "    # probmodel = ('uniform', 'spatial_2dim')\n",
        "    # Return: list of utility profiles\n",
        "\n",
        "    if probmodel == 'uniform': \n",
        "        return pref_voting.generate_utility_profiles.generate_utility_profile_uniform(num_cands, num_voters, num_profiles = num_profiles)\n",
        "    \n",
        "    elif probmodel == 'spatial_2dim':\n",
        "        ndims = 2\n",
        "        sprofs = pref_voting.generate_spatial_profiles.generate_spatial_profile(num_cands, num_voters, ndims, num_profiles = num_profiles)\n",
        "        return [sprof.to_utility_profile() for sprof in sprofs]\n",
        "    \n",
        "    elif probmodel == 'mallows': \n",
        "        profs = generate_profile(num_cands, num_voters, num_profiles=num_profiles, probmodel=\"mallows\", normalize_phi=True, phi=0.8)\n",
        "        return [prof.to_utility_profile() for prof in profs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate utility profiles for training, validation, and evaluation \n",
        "# and save them to disk as pickle files\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "training_utility_base_dir = \"training_utility_profiles\"\n",
        "validation_utility_base_dir = \"validation_utility_profiles\"\n",
        "evaluation_utility_base_dir = \"evaluation_utility_profiles\"\n",
        "\n",
        "if GENERATE_UTILITY_PROFILES:\n",
        "\n",
        "    os.makedirs(training_utility_base_dir, exist_ok=True)\n",
        "    os.makedirs(validation_utility_base_dir, exist_ok=True)\n",
        "    os.makedirs(evaluation_utility_base_dir, exist_ok=True)\n",
        "\n",
        "    for probmodel in prob_models_list:\n",
        "        for num_cands in num_cands_list:\n",
        "            for num_voters in num_voters_list:\n",
        "\n",
        "                generations_to_run = list()\n",
        "                for gen in generation_list:\n",
        "                    # creating training utility profiles.\n",
        "                    filename = os.path.join(\n",
        "                        training_utility_base_dir,\n",
        "                        f\"training_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\")\n",
        "                    if not os.path.exists(filename):\n",
        "                        generations_to_run.append(gen)\n",
        "\n",
        "                if len(generations_to_run) > 0:\n",
        "                    total_utility_profiles = generate_utility_profile(\n",
        "                        num_cands, \n",
        "                        num_voters, \n",
        "                        probmodel = probmodel, \n",
        "                        num_profiles = training_batch_size * max_num_iterations * len(generations_to_run))\n",
        "                    \n",
        "                    for gen_idx, gen in enumerate(generations_to_run):\n",
        "\n",
        "                        start_index = gen_idx * training_batch_size * max_num_iterations\n",
        "                        end_index = (gen_idx + 1) * training_batch_size * max_num_iterations\n",
        "\n",
        "                        elections = total_utility_profiles[start_index: end_index] \n",
        "                        pickle.dump(\n",
        "                            elections, \n",
        "                            open(\n",
        "                                os.path.join(\n",
        "                                    training_utility_base_dir,\n",
        "                                    f\"training_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                                ), \"wb\",\n",
        "                            ),   \n",
        "                        )\n",
        "\n",
        "                generations_to_run = list()\n",
        "                for gen in generation_list:\n",
        "                    # creating training utility profiles.\n",
        "                    filename = os.path.join(\n",
        "                        validation_utility_base_dir,\n",
        "                                f\"validation_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\")\n",
        "                    if not os.path.exists(filename):\n",
        "                        generations_to_run.append(gen)\n",
        "\n",
        "                if len(generations_to_run) > 0:\n",
        "\n",
        "                    # creating validation utility profiles\n",
        "                    total_utility_profiles = generate_utility_profile(\n",
        "                        num_cands, \n",
        "                        num_voters, \n",
        "                        probmodel = probmodel, \n",
        "                        num_profiles = validation_batch_size * math.ceil(max_num_iterations / validate_every) * len(generations_to_run))\n",
        "\n",
        "                    for gen_idx, gen in enumerate(generations_to_run):\n",
        "\n",
        "                        start_index = gen_idx * validation_batch_size * math.ceil(max_num_iterations / validate_every)\n",
        "                        end_index = (gen_idx + 1) * validation_batch_size * math.ceil(max_num_iterations / validate_every)\n",
        "\n",
        "                        elections = total_utility_profiles[start_index: end_index] \n",
        "                        pickle.dump(\n",
        "                            elections, \n",
        "                            open(\n",
        "                                os.path.join(\n",
        "                                    validation_utility_base_dir,\n",
        "                                    f\"validation_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                                ), \"wb\",\n",
        "                            ),   \n",
        "                        )\n",
        "\n",
        "                generations_to_run = list()\n",
        "                for gen in generation_list:\n",
        "                    # creating training utility profiles.\n",
        "                    filename = os.path.join(\n",
        "                        evaluation_utility_base_dir,\n",
        "                                f\"evaluation_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\")\n",
        "                    if not os.path.exists(filename):\n",
        "                        generations_to_run.append(gen)\n",
        "\n",
        "                if len(generations_to_run) > 0:\n",
        "\n",
        "                    # creating evaluation utility profiles\n",
        "                    total_utility_profiles = generate_utility_profile(\n",
        "                        num_cands, \n",
        "                        num_voters, \n",
        "                        probmodel = probmodel, \n",
        "                        num_profiles = evaluation_batch_size * max_num_evaluation_rounds *  len(generations_to_run))\n",
        "\n",
        "                    for gen_idx, gen in enumerate(generations_to_run):\n",
        "\n",
        "                        start_index = gen_idx * evaluation_batch_size * max_num_evaluation_rounds\n",
        "                        end_index = (gen_idx + 1) * evaluation_batch_size * max_num_evaluation_rounds\n",
        "\n",
        "                        elections = total_utility_profiles[start_index: end_index] \n",
        "                        pickle.dump(\n",
        "                            elections, \n",
        "                            open(\n",
        "                                os.path.join(\n",
        "                                    evaluation_utility_base_dir,\n",
        "                                    f\"evaluation_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                                ), \"wb\",\n",
        "                            ),   \n",
        "                        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Generate labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate the labels for the stored training profiles \n",
        "# and save to disk as pickle files. \n",
        "\n",
        "if GENERATE_LABELS:        \n",
        "    for gen in generation_list:\n",
        "        for probmodel in prob_models_list:\n",
        "            for num_cands in num_cands_list:\n",
        "                for num_voters in num_voters_list:\n",
        "\n",
        "                    utility_profiles = pickle.load(\n",
        "                        open(\n",
        "                            os.path.join(\n",
        "                                training_utility_base_dir,\n",
        "                                f\"training_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                            ), \"rb\",\n",
        "                        ),\n",
        "                        )\n",
        "\n",
        "                    elections = (\n",
        "                        [uprof.utilities[0] for uprof in utility_profiles],\n",
        "                        [to_linear_prof(uprof) for uprof in utility_profiles],\n",
        "                    )\n",
        "                                \n",
        "                    for manip_weight in manip_weight_list:\n",
        "                        for vm in voting_methods_list:\n",
        "                            for labeling in labeling_list:\n",
        "                                \n",
        "                                filename = f\"labels/labels_{gen}_{num_cands}_{num_voters}_{probmodel}_{manip_weight}_{vm.name}_{labeling}.pkl\"\n",
        "                                \n",
        "                                if os.path.exists(filename):\n",
        "                                    print(f\"{filename} exists\")\n",
        "                                    continue\n",
        "\n",
        "                                labels_dict = {} \n",
        "\n",
        "                                print(\"Generation:\", gen)\n",
        "                                print(\"Num cands:\", num_cands)\n",
        "                                print(\"Num voters:\", num_voters)\n",
        "                                print(\"Prob model:\", probmodel)\n",
        "                                print(\"Manip weight:\", manip_weight)\n",
        "                                print(\"Voting method:\", vm.name)\n",
        "                                print(\"Labeling:\", labeling)\n",
        "                                print(\"\")\n",
        "\n",
        "                                if labeling == 'satisfice':\n",
        "                                    pool = multiprocess.Pool(processes=cpus)\n",
        "                                    manipulation_responses = pool.map(\n",
        "                                        lambda x: mask_manipulations(x, vm, manip_weight),\n",
        "                                        zip(*elections),\n",
        "                                    )\n",
        "\n",
        "                                    masks = torch.stack(\n",
        "                                        manipulation_responses\n",
        "                                    ).to(\"cpu\") # [bs, num_actions]\n",
        "\n",
        "                                    reduction_contexts = masks\n",
        "\n",
        "                                elif labeling == 'optimize':\n",
        "                                    reduction_contexts = threaded_generate_classification_labels(\n",
        "                                        utility_fns=elections[0],\n",
        "                                        profs=elections[1], \n",
        "                                        num_cands=num_cands,\n",
        "                                        vm=vm, \n",
        "                                        manip_weight=manip_weight,\n",
        "                                        chunk_size=5_000,\n",
        "                                        ).to(\"cpu\")\n",
        "                                \n",
        "                                pickle.dump(reduction_contexts, open(f\"labels/labels_{gen}_{num_cands}_{num_voters}_{probmodel}_{manip_weight}_{vm.name}_{labeling}.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Train models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_run_directory():\n",
        "    # helper function to create a run directory\n",
        "    \n",
        "    current_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=-7)))\n",
        "    current_time_str = current_time.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
        "    run_dir = os.path.join(os.getcwd(), \"run_of_{}\".format(current_time_str))\n",
        "    os.mkdir(run_dir)\n",
        "    os.mkdir(os.path.join(run_dir, \"models\"))\n",
        "    os.mkdir(os.path.join(run_dir, \"torch_models\"))\n",
        "    os.mkdir(os.path.join(run_dir, \"losses\"))\n",
        "    os.mkdir(os.path.join(run_dir, \"validation\"))\n",
        "    os.mkdir(os.path.join(run_dir, \"evaluation\"))\n",
        "    os.mkdir(os.path.join(run_dir, \"graphs\"))\n",
        "    return run_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42pF-hHWv3rL",
        "outputId": "9ec4232c-9d1d-4dc2-88c8-770f5bdb8664"
      },
      "outputs": [],
      "source": [
        "# used the stored training and validation profiles and \n",
        "# the stored labels to train models with configurations \n",
        "# from model_size_lists \n",
        "\n",
        "if TRAIN_MODELS: \n",
        "    run_dir = create_run_directory()\n",
        "    print(run_dir)\n",
        "\n",
        "    model_size_lists = [\n",
        "        [4],\n",
        "        [8],\n",
        "        [16],\n",
        "        [32],\n",
        "        [64],\n",
        "        [128],\n",
        "        [256],\n",
        "        [512],\n",
        "\n",
        "        [4,4],\n",
        "        [8,8],\n",
        "        [16,8],\n",
        "        [16,16],\n",
        "        [32,32],\n",
        "        [64,32],\n",
        "        [64,64],\n",
        "        [128,128],\n",
        "        [256,128],\n",
        "        [256,256],\n",
        "\n",
        "        [8,8,8],\n",
        "        [32,16,8],\n",
        "        [32,32,32],\n",
        "        [64,64,64],\n",
        "        [128,64,32],\n",
        "        [128,128,128],\n",
        "        [256,256,256],\n",
        "        [512,256,128],\n",
        "    ]\n",
        "\n",
        "    for gen in generation_list:\n",
        "        for probmodel in prob_models_list:\n",
        "            for num_cands in num_cands_list:\n",
        "                for num_voters in num_voters_list: \n",
        "                    \n",
        "                    training_utility_profiles = pickle.load(\n",
        "                        open(\n",
        "                            os.path.join(\n",
        "                                training_utility_base_dir,\n",
        "                                f\"training_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                            ), \"rb\",\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "                    training_elections = (\n",
        "                        [uprof.utilities[0] for uprof in training_utility_profiles],\n",
        "                        [to_linear_prof(uprof) for uprof in training_utility_profiles],\n",
        "                    )\n",
        "\n",
        "                    validation_utility_profiles = pickle.load(\n",
        "                        open(\n",
        "                            os.path.join(\n",
        "                                validation_utility_base_dir,\n",
        "                                f\"validation_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                            ), \"rb\",\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "                    validation_elections = (\n",
        "                        [uprof.utilities[0] for uprof in validation_utility_profiles],\n",
        "                        [to_linear_prof(uprof) for uprof in validation_utility_profiles],\n",
        "                    )\n",
        "\n",
        "                    print(\"len(validation_elections[0])\", len(validation_elections[0]))\n",
        "                    print(\"len(validation_elections[1])\", len(validation_elections[1]))\n",
        "                    for manip_weight in manip_weight_list:\n",
        "                        for agent_infos in agent_infos_list:\n",
        "                            for vm in voting_methods_list:\n",
        "                                results_dict = {}\n",
        "\n",
        "                                for labeling in labeling_list:\n",
        "\n",
        "                                    filenames = glob.glob(f'models/models_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/{vm.name}_{gen}_{num_cands}_{num_voters}_{probmodel}_{tuple(agent_infos)}_{manip_weight}_*.pickle')\n",
        "                                    \n",
        "                                    if len(list(filenames)) > 0:\n",
        "                                        print(f\"{list(filenames)} exists\")\n",
        "                                        continue\n",
        "                                    \n",
        "                                    reduction_contexts = pickle.load(\n",
        "                                        open(\n",
        "                                            f\"labels/labels_{gen}_{num_cands}_{num_voters}_{probmodel}_{manip_weight}_{vm.name}_{labeling}.pkl\", \n",
        "                                            \"rb\",\n",
        "                                        ),\n",
        "                                    )\n",
        "                                    \n",
        "                                    reduction_contexts = reduction_contexts.to(DEVICE)\n",
        "                                    \n",
        "                                    for model_size in model_size_lists:\n",
        "                                        for learning_rate in learning_rates:\n",
        "                                            \n",
        "                                            print(\"Generation:\", gen)\n",
        "                                            print(\"Num cands:\", num_cands)\n",
        "                                            print(\"Num voters:\", num_voters)\n",
        "                                            print(\"Prob model:\", probmodel)\n",
        "                                            print(\"Manip weight:\", manip_weight)\n",
        "                                            print(\"Voting method:\", vm.name)\n",
        "                                            print(\"Labeling:\", labeling)\n",
        "                                            print(\"Agent info:\", agent_infos)\n",
        "                                            print(\"Learning rate:\", learning_rate)\n",
        "                                            print(\"\")\n",
        "\n",
        "\n",
        "                                            torch.manual_seed(144 + gen)\n",
        "                                            np.random.seed(144 + gen)\n",
        "\n",
        "                                            agent, losses, evals = train_strategy_classification(\n",
        "                                                num_voters=num_voters,\n",
        "                                                num_candidates=num_cands,\n",
        "                                                training_elections=training_elections,\n",
        "                                                validation_elections=validation_elections,\n",
        "                                                model_size=model_size,\n",
        "                                                vm=vm,\n",
        "                                                max_num_iterations = max_num_iterations,\n",
        "                                                validate_every=validate_every,\n",
        "                                                reduction_contexts=reduction_contexts,\n",
        "                                                training_batch_size=training_batch_size,\n",
        "                                                validation_batch_size=validation_batch_size,\n",
        "                                                learning_rate = learning_rate,\n",
        "                                                labeling=labeling,\n",
        "                                                agent_infos=agent_infos,\n",
        "                                                manip_weight=manip_weight,\n",
        "                                                patience = patience,\n",
        "                                            )\n",
        "\n",
        "                                            torch.save(agent,\n",
        "                                                os.path.join(\n",
        "                                                    f'{run_dir}/torch_models', \n",
        "                                                    f'{vm.name}_{gen}_{num_cands}_{num_voters}_{probmodel}_{learning_rate}_{tuple(model_size)}_{max_num_iterations}_{labeling}_{tuple(agent_infos)}_{manip_weight}.pth', \n",
        "                                                )  \n",
        "                                            )\n",
        "\n",
        "                                            results_dict[\n",
        "                                                (vm.name, gen, tuple(model_size), num_cands, num_voters, probmodel, learning_rate, training_batch_size, max_num_iterations, labeling, tuple(agent_infos), manip_weight)\n",
        "                                            ] = (agent.to(\"cpu\"), losses)\n",
        "                                            print('len(evals)', len(evals))\n",
        "                                            plt.plot(losses, label=\"loss\")\n",
        "                                            plt.plot([((i+1) * validate_every) - 1 for i in range(len(evals))], evals, label=\"evals\")\n",
        "                                            plt.legend()\n",
        "                                            plt.title(f\"{vm.name}, {gen}, {tuple(model_size)}, {num_cands}, {num_voters}, {probmodel}, {learning_rate}, {training_batch_size}, {max_num_iterations}, {labeling}, {tuple(agent_infos)}, {manip_weight}\")\n",
        "\n",
        "                                            plt.savefig(f'{run_dir}/losses/{vm.name}_{gen}_{num_cands}_{num_voters}_{probmodel}_{learning_rate}_{tuple(model_size)}_{max_num_iterations}_{labeling}_{tuple(agent_infos)}_{manip_weight}_{CURRENT_TIME_STR}.png')\n",
        "\n",
        "                                            plt.savefig(f'losses/losses_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/{vm.name}_{gen}_{num_cands}_{num_voters}_{probmodel}_{learning_rate}_{tuple(model_size)}_{max_num_iterations}_{labeling}_{tuple(agent_infos)}_{manip_weight}_{CURRENT_TIME_STR}.png')\n",
        "\n",
        "                                            plt.close()\n",
        "\n",
        "                                            clear_output(wait=True)\n",
        "\n",
        "                                    with open(f'{run_dir}/models/{vm.name}_{gen}_{num_cands}_{num_voters}_{probmodel}_{tuple(agent_infos)}_{manip_weight}_{CURRENT_TIME_STR}.pickle', 'wb') as handle:\n",
        "                                        pickle.dump(results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)                        \n",
        "                                    with open(f'models/models_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/{vm.name}_{gen}_{num_cands}_{num_voters}_{probmodel}_{tuple(agent_infos)}_{manip_weight}_{CURRENT_TIME_STR}.pickle', 'wb') as handle:\n",
        "                                        pickle.dump(results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Evaluate models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBkb2DLmkbGT"
      },
      "outputs": [],
      "source": [
        "def evaluate_models(\n",
        "        models_dict, \n",
        "        num_cands, \n",
        "        num_voters, \n",
        "        probmodel, \n",
        "        all_utility_profs, \n",
        "        min_num_trials, \n",
        "        max_num_trials, \n",
        "        decision_rule, \n",
        "        metric_op):\n",
        "    # evaluate all models in models_dict using all_utility_profs\n",
        "    # the number of profiles used for evaluation is determined by \n",
        "    # repeated application of the percentile bootstrap method \n",
        "    # for determining confidence intervals (see https://pref-voting.readthedocs.io/en/latest/analysis_overview.html#pref_voting.analysis.bootstrap_cia)\n",
        "\n",
        "    vm_dict = {\n",
        "        vm.name : vm for vm in voting_methods_list\n",
        "    }\n",
        "\n",
        "    metrics_dict = {}\n",
        "\n",
        "    for key in tqdm(models_dict.keys()):\n",
        "        \n",
        "        if num_cands == key[3] and num_voters == key[4] and probmodel == key[5]:\n",
        "            vm_name, _, _, num_cands, num_voters,probmodel, learning_rate, _, _, _, agent_infos, manip_weight = key\n",
        "\n",
        "            agent = models_dict[key][0]\n",
        "            agent = agent.eval()\n",
        "\n",
        "            vm = vm_dict[vm_name]\n",
        "\n",
        "            generate_samples = partial(\n",
        "                evaluation_function, \n",
        "                agent=agent, \n",
        "                vm=vm, \n",
        "                num_cands=num_cands, \n",
        "                num_voters=num_voters,\n",
        "                manip_weight=manip_weight, \n",
        "                all_utility_profs=all_utility_profs, \n",
        "                decision_rule=decision_rule, \n",
        "                metric_op=metric_op,\n",
        "                agent_infos=agent_infos\n",
        "            )\n",
        "            means,est_std_errors,variances, num_trials = means_with_estimated_standard_error(\n",
        "                generate_samples,\n",
        "                max_est_std_error,             \n",
        "                initial_trials=4096, \n",
        "                step_trials=4096, \n",
        "                min_num_trials=min_num_trials,\n",
        "                max_num_trials=max_num_trials,\n",
        "                verbose=False,)\n",
        "\n",
        "            metrics_dict[key] = {\n",
        "                \"means\":means, \n",
        "                \"est_std_errors\": est_std_errors, \n",
        "                \"variances\": variances, \n",
        "                \"num_trials\": num_trials\n",
        "                }\n",
        "\n",
        "    return metrics_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the stored evaluation profiles and trained models for evaluation\n",
        "\n",
        "if EVALUATE_MODELS:\n",
        "    eval_batch_size = 4096\n",
        "    decision_rule = 'argmax'\n",
        "    metric_op = 'normalized_subtract'\n",
        "    min_num_trials = 4095\n",
        "\n",
        "    for labeling in labeling_list:\n",
        "        for manip_weight in manip_weight_list:\n",
        "            for agent_infos in agent_infos_list:\n",
        "                for probmodel in prob_models_list:\n",
        "                    for gen in generation_list: \n",
        "                        for num_cands in num_cands_list: \n",
        "                            for num_voters in num_voters_list: \n",
        "\n",
        "                                filenames = glob.glob(f'evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/{gen}_{num_cands}_{num_voters}_{probmodel}_{tuple(agent_infos)}_{manip_weight}_{decision_rule}_{metric_op}_{evaluation_batch_size}_{max_est_std_error}_*.pickle')\n",
        "                                    \n",
        "                                if len(list(filenames)) > 0:\n",
        "                                    print(f\"{list(filenames)} exists\")\n",
        "                                    continue\n",
        "\n",
        "                                # read in the utility profiles\n",
        "                                all_utility_profiles = pickle.load(\n",
        "                                    open(\n",
        "                                        os.path.join(\n",
        "                                            evaluation_utility_base_dir,\n",
        "                                            f\"evaluation_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                                            ), \"rb\",\n",
        "                                            ),\n",
        "                                            )\n",
        "\n",
        "                                # read in the models\n",
        "                                models_dict = {}\n",
        "                                for file in glob.glob(f\"models/models_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/*_{gen}_{num_cands}_{num_voters}_{probmodel}_{tuple(agent_infos)}_{manip_weight}_*.pickle\"):\n",
        "                                    with open(file, 'rb') as handle:\n",
        "                                        m_dict = pickle.load(handle)\n",
        "                                        new_m_dict = {}\n",
        "                                        for key in m_dict.keys():\n",
        "                                            if key[1] == gen and key[3] == num_cands and key[4] == num_voters and key[9] == labeling: \n",
        "                                                new_m_dict[key] = (m_dict[key][0].to(DEVICE), m_dict[key][1])\n",
        "                                        models_dict.update(new_m_dict)\n",
        "                                print(\"Evaluating\")\n",
        "                                print(f\"labeling: {labeling}\")\n",
        "                                print(f\"manip_weight: {manip_weight}\")\n",
        "                                print(f\"agent_infos: {agent_infos}\")\n",
        "                                print(f\"probmodel: {probmodel}\")\n",
        "                                print(f\"gen: {gen}\")\n",
        "                                print(f\"num_cands: {num_cands}\")\n",
        "                                print(f\"num_voters: {num_voters}\")\n",
        "                                print(f\"decision_rule: {decision_rule}\")\n",
        "                                print(f\"metric_op: {metric_op}\")\n",
        "                                print(f\"evaluation_batch_size: {evaluation_batch_size}\")\n",
        "                                print(f\"max_est_std_error: {max_est_std_error}\")\n",
        "                                print(\"Number of models: \", len(models_dict.keys()))\n",
        "                                metrics_dict = evaluate_models(\n",
        "                                    models_dict, \n",
        "                                    num_cands, \n",
        "                                    num_voters, \n",
        "                                    probmodel,\n",
        "                                    all_utility_profiles,\n",
        "                                    min_num_trials,\n",
        "                                    evaluation_batch_size * max_num_evaluation_rounds - 1,\n",
        "                                    decision_rule, \n",
        "                                    metric_op)\n",
        "\n",
        "\n",
        "                                if run_dir is not None:\n",
        "                                    with open(f'{run_dir}/evaluation/{gen}_{num_cands}_{num_voters}_{probmodel}_{tuple(agent_infos)}_{manip_weight}_{decision_rule}_{metric_op}_{evaluation_batch_size}_{max_est_std_error}_{CURRENT_TIME_STR}.pickle', 'wb') as handle:\n",
        "                                        pickle.dump(metrics_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "                                with open(f'evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/{gen}_{num_cands}_{num_voters}_{probmodel}_{tuple(agent_infos)}_{manip_weight}_{decision_rule}_{metric_op}_{evaluation_batch_size}_{max_est_std_error}_{CURRENT_TIME_STR}.pickle', 'wb') as handle:\n",
        "                                    pickle.dump(metrics_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Create CSV files for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataframe(evaluation_dict): \n",
        "    # create a dataframe from the dictionary of evaluations of the models\n",
        "    \n",
        "    data_for_df = {\n",
        "        \"vm\": list(),\n",
        "        \"model_size\": list(),\n",
        "        \"num_cands\": list(),\n",
        "        \"num_voters\": list(),\n",
        "        \"probmodel\": list(),\n",
        "\n",
        "        \"mean_profitability\": list(),\n",
        "        \"est_std_error\": list(),\n",
        "        \"variance\": list(),\n",
        "        \"num_trials\": list(),\n",
        "\n",
        "        \"learning_rate\": list(),\n",
        "        \"labeling\": list(),\n",
        "        \"batch_size\": list(),\n",
        "        \"num_iterations\": list(),\n",
        "        \"generation\": list(),\n",
        "        \"agent_infos\": list(),\n",
        "        \"manip_weight\": list(),\n",
        "    }\n",
        "    for key in evaluation_dict.keys():\n",
        "        try:\n",
        "            vm, generation, model_size, num_cands, num_voters, probmodel, learning_rate, batch_size, num_iterations, labeling, agent_infos, manip_weight = key\n",
        "            data = evaluation_dict[key]\n",
        "            data_for_df[\"vm\"].append(vm)\n",
        "            data_for_df[\"model_size\"].append(model_size)\n",
        "            data_for_df[\"num_cands\"].append(num_cands)\n",
        "            data_for_df[\"num_voters\"].append(num_voters)\n",
        "            data_for_df[\"probmodel\"].append(probmodel)\n",
        "\n",
        "            data_for_df[\"mean_profitability\"].append(data['means'][0])\n",
        "            data_for_df[\"est_std_error\"].append(data['est_std_errors'][0])\n",
        "            data_for_df[\"variance\"].append(data['variances'][0])\n",
        "            data_for_df[\"num_trials\"].append(data['num_trials'])\n",
        "\n",
        "            data_for_df[\"learning_rate\"].append(learning_rate)\n",
        "            data_for_df[\"labeling\"].append(labeling) \n",
        "            data_for_df[\"batch_size\"].append(batch_size)\n",
        "            data_for_df[\"num_iterations\"].append(num_iterations)\n",
        "            data_for_df[\"generation\"].append(generation)\n",
        "            data_for_df[\"agent_infos\"].append(agent_infos)\n",
        "            data_for_df[\"manip_weight\"].append(manip_weight)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            print(f\"Skipping key {key}\")\n",
        "    print(f\"Finished creating dataframe for generation {generation} and agent_infos {agent_infos}\")\n",
        "    return pd.DataFrame(data_for_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CREATE_CSVS: \n",
        "    probmodel = 'uniform'\n",
        "\n",
        "    gen = 1\n",
        "    all_num_cands = [3, 4, 5, 6]\n",
        "    all_num_voters = [5, 6, 10, 11, 20, 21]\n",
        "    manip_weight = 1\n",
        "\n",
        "    all_agent_infos = [\n",
        "        'plurality_scores',\n",
        "        'plurality_ranking',\n",
        "        'majority',\n",
        "        'margin',\n",
        "        'sincere_winners',\n",
        "        'qual_margin']\n",
        "\n",
        "    labeling = 'optimize'\n",
        "    decision_rule = 'argmax'\n",
        "    metric_op = 'normalized_subtract'\n",
        "    evaluation_batch_size = 4096\n",
        "    max_est_std_error = 0.0005\n",
        "    eval_dict = {}\n",
        "    for num_cands in all_num_cands:\n",
        "        for num_voters in all_num_voters: \n",
        "            for agent_info in all_agent_infos:\n",
        "                # get filename from regular expression\n",
        "                filenames = glob.glob(f\"evaluation/evaluation_('{agent_info}',)_{probmodel}_{labeling}_{manip_weight}/{gen}_{num_cands}_{num_voters}_{probmodel}_('{agent_info}',)_{manip_weight}_{decision_rule}_{metric_op}_{evaluation_batch_size}_{max_est_std_error}_*.pickle\")\n",
        "                if len(filenames) == 0:\n",
        "                    raise ValueError(f\"no file found for {num_cands} candidates, {num_voters} voters, and {agent_info}\\n\")\n",
        "                elif len(filenames) > 1:\n",
        "                    print(filenames)\n",
        "                    raise ValueError(\"more than one file found!\")\n",
        "                    \n",
        "                else: \n",
        "                    filename = filenames[0]\n",
        "                    print(f\"loading {agent_info} evaluation data:\\n {filename.split('/')[-1]}\")\n",
        "                    _eval_dict  = pickle.load(open(filename, 'rb'))\n",
        "                    eval_dict.update(_eval_dict)\n",
        "\n",
        "    print(len(eval_dict.keys()))\n",
        "    df = create_dataframe(eval_dict)\n",
        "    df['agent_infos'] = df['agent_infos'].astype(str)\n",
        "    df['agent_infos'] = df['agent_infos'].replace({\n",
        "        \"('plurality_scores',)\": \"plurality_scores\", \n",
        "        \"('majority',)\": \"majority\",\n",
        "        \"('plurality_ranking',)\": \"plurality_ranking\",\n",
        "        \"('margin',)\": \"margin\",\n",
        "        \"('sincere_winners',)\": \"sincere_winners\",\n",
        "        \"('qual_margin',)\": \"qual_margin\",\n",
        "        })\n",
        "\n",
        "    model_sizes_order = [\n",
        "        \"(4,)\", \n",
        "        \"(8,)\", \n",
        "        \"(16,)\", \n",
        "        \"(32,)\", \n",
        "        \"(64,)\", \n",
        "        \"(128,)\", \n",
        "        \"(256,)\", \n",
        "        \"(512,)\",\n",
        "        \"(4, 4)\", \n",
        "        \"(8, 8)\", \n",
        "        \"(16, 8)\", \n",
        "        \"(16, 16)\", \n",
        "        \"(32, 32)\", \n",
        "        \"(64, 32)\", \n",
        "        \"(64, 64)\", \n",
        "        \"(128, 128)\", \n",
        "        \"(256, 128)\", \n",
        "        \"(256, 256)\",\n",
        "        \"(8, 8, 8)\", \n",
        "        \"(32, 16, 8)\", \n",
        "        \"(32, 32, 32)\", \n",
        "        \"(64, 64, 64)\", \n",
        "        \"(128, 64, 32)\", \n",
        "        \"(128, 128, 128)\", \n",
        "        \"(256, 256, 256)\", \n",
        "        \"(512, 256, 128)\"\n",
        "    ]\n",
        "\n",
        "    df['model_size_str'] = df['model_size'].astype(str)\n",
        "    df = df[df['model_size_str'].isin(model_sizes_order)]\n",
        "    # make sure the directory exists\n",
        "    os.makedirs(\"./evaluation_csv\", exist_ok=True) \n",
        "    os.makedirs(f\"./evaluation_csv/{probmodel}\", exist_ok=True)\n",
        "    \n",
        "    df.to_csv(\"./evaluation_csv/{probmodel}/all_agent_infos_diff_cands.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
