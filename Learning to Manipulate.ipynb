{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Learning to Manipulate under Limited Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Requirement Installation (Linux, Python >=3.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip3 install pref_voting # need at least version 0.4.42\n",
        "# !pip3 install numba  # needed for pref_voting\n",
        "# !pip3 install nashpy # needed for pref_voting\n",
        "# !pip3 install seaborn\n",
        "# !pip3 install multiprocess\n",
        "# !pip3 install tqdm\n",
        "# !pip3 install torch # https://pytorch.org/get-started/locally/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "p5nLR19AXMcp"
      },
      "outputs": [],
      "source": [
        "import pref_voting.generate_spatial_profiles\n",
        "import pref_voting.generate_utility_profiles\n",
        "import pref_voting.utility_profiles\n",
        "import pref_voting.utility_functions\n",
        "import pref_voting.voting_methods\n",
        "import pref_voting.profiles\n",
        "import pref_voting.generate_profiles\n",
        "from pref_voting.generate_profiles import *\n",
        "from pref_voting.iterative_methods import *\n",
        "from pref_voting.analysis import means_with_estimated_standard_error\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import itertools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import multiprocessing\n",
        "import dill\n",
        "import multiprocess\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from google.cloud import storage\n",
        "import pickle\n",
        "import zipfile\n",
        "import os\n",
        "import fnmatch\n",
        "\n",
        "# Set-up for multiprocessing\n",
        "dill.Pickler.dumps, dill.Pickler.loads = dill.dumps, dill.loads\n",
        "multiprocessing.reduction.ForkingPickler = dill.Pickler\n",
        "multiprocessing.reduction.dump = dill.dump\n",
        "try:\n",
        "    cpus = multiprocessing.cpu_count() - 1\n",
        "except NotImplementedError:\n",
        "    cpus = 2   # arbitrary default\n",
        "\n",
        "# Set-up for timing\n",
        "current_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=-7)))\n",
        "CURRENT_TIME_STR = current_time.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
        "\n",
        "run_dir = None\n",
        "DEVICE = 'cuda'  # change this to 'mps' to run on Apple Silicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5.32\n"
          ]
        }
      ],
      "source": [
        "import pref_voting\n",
        "\n",
        "print(pref_voting.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Setup\n",
        "    * 1.1. Global variables\n",
        "    * 1.2. Create directories\n",
        "    * 1.3. Helper functions\n",
        "    * 1.4. Agent class\n",
        "    * 1.5. Manipulator information\n",
        "    * 1.6. Labeling setup\n",
        "        * 1.6.1. Satisficing\n",
        "        * 1.6.2. Optimizing\n",
        "    * 1.7. Validation setup\n",
        "    * 1.8. Training setup\n",
        "    * 1.9. Evaluation setup\n",
        "2. Generate utility profiles\n",
        "3. Generate labels\n",
        "4. Train models\n",
        "5. Evaluate models\n",
        "6. Visualize results\n",
        "    * 6.1. Performance by model size (Figure 3)\n",
        "    * 6.2. Graphs to compare manipulator information types for a fixed model size (Figure 1)\n",
        "    * 6.3. Graphs to compare across different numbers of candidates (Figure 2)\n",
        "    * 6.4. Compare different generations of models\n",
        "    * 6.5. Statistics about the number of models evaluated (Footnote 8)\n",
        "    * 6.6. The total number of model evaluated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [],
      "source": [
        "DOWNLOAD_DATA = True # if true, then any missing utility profiles,  labels, and/or models will be downloaded from google cloud, unless the following variables are set to True. \n",
        "GENERATE_UTILITY_PROFILES = False\n",
        "GENERATE_LABELS = False\n",
        "TRAIN_MODELS = False\n",
        "\n",
        "EVALUATE_MODELS = False\n",
        "VISUALIZE_RESULTS = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "# How many generations to run\n",
        "generation_list = [1]\n",
        "\n",
        "# How many candidates will be in each election\n",
        "num_cands_list = [3, 4, 5, 6]  \n",
        "\n",
        "# How many voters will be in each election\n",
        "num_voters_list = [5, 6, 10, 11, 20, 21]   \n",
        "\n",
        "# Which models to use for sampling utility functions\n",
        "prob_models_list = [\n",
        "    'uniform', \n",
        "    #'spatial_2dim'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [],
      "source": [
        "# What voting methods to study\n",
        "voting_methods_list = [\n",
        "    pref_voting.voting_methods.plurality,\n",
        "    pref_voting.voting_methods.borda,\n",
        "    pref_voting.voting_methods.instant_runoff,\n",
        "    pref_voting.voting_methods.instant_runoff_put,\n",
        "    pref_voting.voting_methods.minimax,\n",
        "    pref_voting.voting_methods.split_cycle,\n",
        "    pref_voting.voting_methods.strict_nanson,\n",
        "    pref_voting.voting_methods.stable_voting,\n",
        "]\n",
        "\n",
        "# How much weight to give the manipulating voter. \n",
        "# manip_weight > 1 represents coalitional manipulation\n",
        "manip_weight_list = [1]\n",
        "\n",
        "# How rankings should be labeled\n",
        "labeling_list = [\n",
        "    #'satisfice',\n",
        "    'optimize'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "# What information will the manipulator know\n",
        "agent_infos_list = [\n",
        "    # ['full'], \n",
        "    # ['anon_prof'],\n",
        "    ['plurality_scores'],\n",
        "    # ['plurality_ranking'],\n",
        "    # ['borda_scores'], \n",
        "    # ['margin'],\n",
        "    # ['qual_margin'],\n",
        "    ['majority'],\n",
        "    # ['sincere_winners'],\n",
        "    # ['plurality_scores, 'majority'], # List of multiple means that information will be concatenated together.\n",
        "]\n",
        "\n",
        "# Training hyperparameters\n",
        "learning_rates = [6e-3]\n",
        "max_num_iterations = 2000\n",
        "training_batch_size = 512 \n",
        "validation_batch_size = 4096 \n",
        "validate_every = 20\n",
        "patience = 10\n",
        "threshold = .001\n",
        "\n",
        "# Evaluation parameters\n",
        "max_est_std_error = 0.0005\n",
        "evaluation_batch_size = 4096\n",
        "max_num_evaluation_rounds = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2. Create directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create directories for saving results\n",
        "\n",
        "os.makedirs(f'./training_utility_profiles', exist_ok=True)\n",
        "os.makedirs(f'./validation_utility_profiles', exist_ok=True)\n",
        "os.makedirs(f'./evaluation_utility_profiles', exist_ok=True)\n",
        "os.makedirs(f'./labels', exist_ok=True)\n",
        "os.makedirs(f'./models', exist_ok=True)\n",
        "os.makedirs(f'./losses', exist_ok=True)\n",
        "os.makedirs(f'./graphs', exist_ok=True)\n",
        "\n",
        "for probmodel in prob_models_list:\n",
        "    for labeling in labeling_list:\n",
        "        for agent_infos in agent_infos_list:\n",
        "            for manip_weight in manip_weight_list:\n",
        "                os.makedirs(f'./models/models_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}', exist_ok=True)\n",
        "                os.makedirs(f'./losses/losses_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}', exist_ok=True)\n",
        "os.makedirs(f'./evaluation', exist_ok=True)\n",
        "for probmodel in prob_models_list:\n",
        "    for labeling in labeling_list:\n",
        "        for agent_infos in agent_infos_list:\n",
        "            for manip_weight in manip_weight_list:\n",
        "                os.makedirs(f'./evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_file(file_path):\n",
        "    bucket_name = 'ltmdata'  \n",
        "    dirs = file_path.split('/')\n",
        "    filename = dirs[-1]\n",
        "    dir = '/'.join(dirs[:-1])\n",
        "    # if dir != '':\n",
        "    #     dir += '/'\n",
        "\n",
        "    storage_client = storage.Client.create_anonymous_client()\n",
        "\n",
        "    blobs = storage_client.list_blobs(bucket_name, \n",
        "                                      prefix=dir, \n",
        "                                      delimiter=None)\n",
        "    found_file = False\n",
        "    for blob in blobs:\n",
        "        # only download the file that matches the file_path\n",
        "        if blob.name.endswith('zip') and fnmatch.fnmatch(blob.name, file_path + '.zip'):\n",
        "            found_file = True\n",
        "            with open(blob.name, 'wb') as file:\n",
        "                blob.download_to_file(file)\n",
        "            \n",
        "            with zipfile.ZipFile(blob.name, 'r') as zip_ref:\n",
        "                for file_info in zip_ref.infolist():\n",
        "                    file_name = os.path.basename(file_info.filename)\n",
        "                    # Skip directories\n",
        "                    if not file_name:\n",
        "                        continue\n",
        "                    source = zip_ref.open(file_info)\n",
        "                    target_file_path = os.path.join(dir, file_name)\n",
        "                    with open(target_file_path, \"wb\") as target:\n",
        "                        with source:\n",
        "                            target.write(source.read())\n",
        "\n",
        "            # delete the zip file from the local directory\n",
        "            os.remove(blob.name)\n",
        "    if found_file:    \n",
        "        print(\"downloaded file\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"file not found\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_file(file_path):\n",
        "    bucket_name = 'ltmdata'  \n",
        "    dirs = file_path.split('/')\n",
        "    filename = dirs[-1]\n",
        "    dir = '/'.join(dirs[:-1])\n",
        "    # if dir != '':\n",
        "    #     dir += '/'\n",
        "\n",
        "    storage_client = storage.Client.create_anonymous_client()\n",
        "\n",
        "    blobs = storage_client.list_blobs(bucket_name, \n",
        "                                      prefix=dir, \n",
        "                                      delimiter=None)\n",
        "    found_file = False\n",
        "    for blob in blobs:\n",
        "        # only download the file that matches the file_path\n",
        "        if blob.name.endswith('zip') and fnmatch.fnmatch(blob.name, file_path + '.zip'):\n",
        "            found_file = True\n",
        "            print(\"file found\")\n",
        "    return found_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data...\n",
            "\n",
            "\n",
            "Checking evaluation utility profiles...\n",
            "\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_3_5_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_3_6_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_3_10_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_3_11_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_3_20_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_3_21_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_4_5_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_4_6_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_4_10_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_4_11_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_4_20_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_4_21_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_5_5_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_5_6_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_5_10_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_5_11_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_5_20_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_5_21_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_6_5_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_6_6_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_6_10_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_6_11_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_6_20_uniform.pkl\n",
            "file found\n",
            "evaluation_utility_profiles/evaluation_util_profs_1_6_21_uniform.pkl\n",
            "file found\n",
            "\n",
            "All evaluation utility profiles exist.\n",
            "\n",
            "********\n",
            "\n",
            "Checking training utility profiles...\n",
            "\n",
            "training_utility_profiles/training_util_profs_1_3_5_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_3_6_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_3_10_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_3_11_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_3_20_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_3_21_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_4_5_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_4_6_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_4_10_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_4_11_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_4_20_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_4_21_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_5_5_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_5_6_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_5_10_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_5_11_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_5_20_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_5_21_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_6_5_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_6_6_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_6_10_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_6_11_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_6_20_uniform.pkl\n",
            "training_utility_profiles/training_util_profs_1_6_21_uniform.pkl\n",
            "\n",
            "Not all training utility profiles exist.  You will need to generate the missing ones.\n",
            "\n",
            "********\n",
            "\n",
            "Checking validation utility profiles...\n",
            "\n",
            "validation_utility_profiles/validation_util_profs_1_3_5_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_3_6_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_3_10_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_3_11_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_3_20_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_3_21_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_4_5_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_4_6_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_4_10_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_4_11_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_4_20_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_4_21_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_5_5_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_5_6_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_5_10_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_5_11_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_5_20_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_5_21_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_6_5_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_6_6_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_6_10_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_6_11_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_6_20_uniform.pkl\n",
            "file found\n",
            "validation_utility_profiles/validation_util_profs_1_6_21_uniform.pkl\n",
            "file found\n",
            "\n",
            "All validation utility profiles exist.\n",
            "\n",
            "********\n",
            "\n",
            "Checking all labels...\n",
            "\n",
            "labels/labels_1_3_5_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_5_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_5_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_5_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_5_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_5_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_5_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_5_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_6_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_6_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_6_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_6_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_6_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_6_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_6_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_6_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_10_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_10_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_10_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_10_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_10_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_10_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_10_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_10_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_11_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_11_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_11_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_11_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_11_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_11_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_11_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_11_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_20_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_20_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_20_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_20_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_20_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_20_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_20_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_20_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_21_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_21_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_21_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_21_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_21_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_21_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_21_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_3_21_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_5_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_5_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_5_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_5_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_5_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_5_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_5_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_5_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_6_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_6_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_6_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_6_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_6_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_6_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_6_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_6_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_10_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_10_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_10_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_10_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_10_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_10_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_10_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_10_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_11_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_11_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_11_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_11_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_11_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_11_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_11_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_11_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_20_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_20_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_20_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_20_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_20_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_20_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_20_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_20_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_21_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_21_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_21_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_21_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_21_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_21_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_21_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_4_21_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_5_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_5_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_5_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_5_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_5_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_5_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_5_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_5_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_6_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_6_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_6_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_6_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_6_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_6_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_6_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_6_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_10_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_10_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_10_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_10_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_10_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_10_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_10_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_10_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_11_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_11_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_11_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_11_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_11_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_11_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_11_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_11_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_20_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_20_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_20_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_20_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_20_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_20_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_20_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_20_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_21_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_21_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_21_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_21_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_21_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_21_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_21_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_5_21_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_5_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_5_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_5_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_5_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_5_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_5_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_5_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_5_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_6_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_6_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_6_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_6_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_6_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_6_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_6_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_6_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_10_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_10_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_10_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_10_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_10_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_10_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_10_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_10_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_11_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_11_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_11_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_11_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_11_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_11_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_11_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_11_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_20_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_20_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_20_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_20_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_20_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_20_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_20_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_20_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_21_uniform_1_Plurality_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_21_uniform_1_Borda_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_21_uniform_1_Instant Runoff_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_21_uniform_1_Instant Runoff PUT_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_21_uniform_1_Minimax_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_21_uniform_1_Split Cycle_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_21_uniform_1_Strict Nanson_optimize.pkl\n",
            "file found\n",
            "labels/labels_1_6_21_uniform_1_Stable Voting_optimize.pkl\n",
            "file found\n",
            "\n",
            "All labels exist.\n",
            "\n",
            "********\n",
            "\n",
            "Checking all models...\n",
            "\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Plurality_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Borda_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Instant Runoff PUT_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Minimax_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Split Cycle_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Strict Nanson_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('plurality_scores',)_uniform_optimize_1/Stable Voting_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Plurality_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Borda_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Instant Runoff PUT_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_5_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_6_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_6_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_6_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_6_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_6_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Minimax_1_6_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_3_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_3_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_3_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_3_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_3_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_3_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_4_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_4_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_4_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_4_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_4_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_4_21_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_5_5_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_5_6_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_5_10_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_5_11_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_5_20_*.pickle\n",
            "file found\n",
            "models/models_('majority',)_uniform_optimize_1/Split Cycle_1_5_21_*.pickle\n"
          ]
        }
      ],
      "source": [
        "if DOWNLOAD_DATA:\n",
        "    print('Downloading data...\\n\\n')\n",
        "    \n",
        "    if not GENERATE_UTILITY_PROFILES:\n",
        "        prof_types = [\n",
        "            'evaluation', \n",
        "            'training', \n",
        "            'validation']\n",
        "        for prof_type in prof_types:\n",
        "            print(f'Checking {prof_type} utility profiles...\\n')\n",
        "            all_util_profiles_exist = list()\n",
        "            for gen in generation_list:\n",
        "                for num_cands in num_cands_list:\n",
        "                    for num_voters in num_voters_list:\n",
        "                        for probmodel in prob_models_list:\n",
        "                            filename = f'{prof_type}_utility_profiles/{prof_type}_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl'\n",
        "                            print(filename)\n",
        "                            if os.path.isfile(filename):\n",
        "                                print(f'already exists') \n",
        "                                found_file = True\n",
        "                            else: \n",
        "                                #found_file = download_file(filename)\n",
        "                                found_file = check_file(filename)\n",
        "\n",
        "                            all_util_profiles_exist.append(found_file)\n",
        "                            \n",
        "            if all(all_util_profiles_exist):\n",
        "                print(f'\\nAll {prof_type} utility profiles exist.\\n\\n********\\n')\n",
        "            else: \n",
        "                print(f'\\nNot all {prof_type} utility profiles exist.  You will need to generate the missing ones.\\n\\n********\\n')\n",
        "\n",
        "    if not GENERATE_LABELS:   \n",
        "        print(f'Checking all labels...\\n')\n",
        "        all_labels_exist = list()\n",
        "        for gen in generation_list:\n",
        "            for num_cands in num_cands_list:\n",
        "                for num_voters in num_voters_list:\n",
        "                    for probmodel in prob_models_list:\n",
        "                        for manip_weight in manip_weight_list:\n",
        "                            for vm in voting_methods_list:\n",
        "                                vm_name = vm.name\n",
        "                                for labeling in labeling_list:\n",
        "                                    filename = f'labels/labels_{gen}_{num_cands}_{num_voters}_{probmodel}_{manip_weight}_{vm_name}_{labeling}.pkl'\n",
        "                                    print(filename)\n",
        "                                    if os.path.isfile(filename):\n",
        "                                        print(f'already exists') \n",
        "                                        found_file = True\n",
        "                                    else: \n",
        "                                        found_file = check_file(filename)\n",
        "                                        \n",
        "                                    all_labels_exist.append(found_file)\n",
        "                            \n",
        "        if all(all_labels_exist):\n",
        "            print(f'\\nAll labels exist.\\n\\n********\\n')\n",
        "        else: \n",
        "            print(f'\\nNot all labels exist.  You will need to generate the missing ones.\\n\\n********\\n') \n",
        "\n",
        "\n",
        "    if not TRAIN_MODELS:   \n",
        "        print(f'Checking all models...\\n')\n",
        "        all_models_exist = list()\n",
        "\n",
        "        for agent_infos in agent_infos_list:\n",
        "            for probmodel in prob_models_list:\n",
        "                for labeling in labeling_list:\n",
        "                    for manip_weight in manip_weight_list:\n",
        "                        model_dir = f'models/models_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}'\n",
        "                        os.makedirs(f'{model_dir}', exist_ok=True)\n",
        "                        for vm in voting_methods_list:\n",
        "                            vm_name = vm.name\n",
        "                            for gen in generation_list: \n",
        "                                for num_cands in num_cands_list:\n",
        "                                    for num_voters in num_voters_list:\n",
        "                                        filename = f'{model_dir}/{vm_name}_{gen}_{num_cands}_{num_voters}_*.pickle'\n",
        "                                        \n",
        "                                        print(filename)\n",
        "\n",
        "                                        # get all files matching this pattern\n",
        "                                        files=glob.glob(filename)\n",
        "\n",
        "                                        if len(files) == 1:\n",
        "                                            print(f'already exists') \n",
        "                                            found_file = True\n",
        "                                        elif len(files) > 1:\n",
        "                                            print(f'found multiple model files')\n",
        "                                            found_file = True\n",
        "                                        else: # the file does not exist\n",
        "                                            found_file = check_file(filename)\n",
        "                                        \n",
        "                                        all_models_exist.append(found_file)\n",
        "        if all(all_models_exist):\n",
        "            print(f'\\nAll models exist.\\n\\n********\\n')\n",
        "        else: \n",
        "            print(f'\\nNot all models exist.  You will need to generate the missing ones.\\n\\n********\\n\\n')   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2uy2Mb1XORx"
      },
      "outputs": [],
      "source": [
        "def generate_permutations(n):\n",
        "    # Create a list from 0 to n-1\n",
        "    num_list = list(range(n))\n",
        "\n",
        "    # generate all permutations\n",
        "    perms = list(itertools.permutations(num_list))\n",
        "\n",
        "    return perms\n",
        "\n",
        "permutations_of = dict()\n",
        "permutations_of = {\n",
        "    i: generate_permutations(i)\n",
        "    for i in range(3, 7)\n",
        "}\n",
        "\n",
        "permutations_index_dict = {\n",
        "    i : {\n",
        "            tuple(permutations_of[i][j]) : j \n",
        "            for j in range(len(permutations_of[i]))\n",
        "        }\n",
        "    for i in range(3, 7)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFlcbGxhX8L5",
        "outputId": "1a504021-a2ad-4963-9251-ba9cdebe5e7f"
      },
      "outputs": [],
      "source": [
        "print(\"All rankings for 3 candidates:\\n\", permutations_of[3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_linear_prof(uprof):\n",
        "    # Convert utility profile to linear profile. \n",
        "    return pref_voting.profiles.Profile([sorted(uprof.domain, key=lambda x: u(x), reverse=True) for u in uprof.utilities])\n",
        "\n",
        "def clone_voter(prof, manip_weight = 1):\n",
        "    # Duplicate voter in profile (useful for coalitional manipulation)\n",
        "    rankings, rcounts = prof.rankings_counts\n",
        "    rcounts = list(rcounts)\n",
        "    rankings = list([tuple(r) for r in rankings])\n",
        "    new_rcounts = [rcounts[0] + (manip_weight - 1)] + rcounts[1:]\n",
        "    return Profile(rankings, rcounts=new_rcounts)\n",
        "\n",
        "def apply_manipulation(prof_with_clones, new_ranking, manip_weight):\n",
        "    # Apply manipulation in profile with clones given a manipulation weight\n",
        "    return pref_voting.profiles.Profile([new_ranking] * manip_weight + prof_with_clones.rankings[manip_weight:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prof = pref_voting.generate_profiles.generate_profile(3, 4)\n",
        "\n",
        "manip_weight = 1\n",
        "print('Sincere profile:')\n",
        "prof.display()\n",
        "print('')\n",
        "print('Apply manipulation (0, 1, 2):')\n",
        "mprof = apply_manipulation(prof, (0, 1, 2), manip_weight)\n",
        "mprof.display()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4. Agent Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKyMd6Z_YBTr"
      },
      "outputs": [],
      "source": [
        "class Agent(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, classification=False, layers=[128, 64, 32]):\n",
        "        # Basic configurable MLP model.\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = math.factorial(output_dim)  # action space is [0, (output_dim)! - 1]\n",
        "\n",
        "        module_list = []\n",
        "\n",
        "        layers = [input_dim] + layers + [self.output_dim]\n",
        "\n",
        "        for i in range(1, len(layers)):\n",
        "            module_list.append(\n",
        "                nn.Linear(layers[i - 1], layers[i]),\n",
        "            )\n",
        "            if i != len(layers) - 1:\n",
        "                module_list.append(\n",
        "                    nn.LeakyReLU(),\n",
        "                )\n",
        "\n",
        "        if classification:\n",
        "            module_list.append(\n",
        "                nn.Sigmoid(),\n",
        "            )\n",
        "        else:\n",
        "            module_list.append(\n",
        "                nn.Softmax(dim=-1),\n",
        "            )\n",
        "\n",
        "        self.model = nn.Sequential(*module_list)\n",
        "\n",
        "        print(self.model)\n",
        "\n",
        "\n",
        "    def forward(self, manipulator_utilities, additional_contexts): \n",
        "        # manipulator_utilities: [bs, n_c] (utility of voter 0)\n",
        "        # additional_contexts: [bs, n_c]\n",
        "        # returns (action_probs, action)\n",
        "\n",
        "        context = torch.cat([manipulator_utilities, additional_contexts], dim=-1)\n",
        "\n",
        "        action_probs = self.model(context)\n",
        "\n",
        "        dist=torch.distributions.categorical.Categorical(probs=action_probs)\n",
        "        actions = dist.sample()\n",
        "\n",
        "        # action_probs: [BS, num_possible_actions]\n",
        "        # actions: [BS,]\n",
        "        return action_probs, actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5. Manipulator Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_score_context(profs, scoring_rule = 'plurality', device=DEVICE):\n",
        "    # generates score contexts from utility profile for each candidate\n",
        "    #\n",
        "    # scoring_rule: ('plurality', 'borda')\n",
        "    # returns: [bs, num_cands]\n",
        "\n",
        "    if scoring_rule == 'plurality':\n",
        "        scores = [ prof.plurality_scores() for prof in profs] # list of dicts\n",
        "    elif scoring_rule == 'borda':\n",
        "        scores = [ prof.borda_scores() for prof in profs] # list of dicts\n",
        "\n",
        "    final_scores = []\n",
        "    for score in scores:\n",
        "\n",
        "        scores_list = []\n",
        "        for cand_index in sorted(score.keys()):\n",
        "            scores_list.append(score[cand_index])\n",
        "\n",
        "        final_scores.append(scores_list)\n",
        "\n",
        "    return torch.tensor(\n",
        "        final_scores,\n",
        "        device=device,\n",
        "    ) # [bs, num_cands]\n",
        "\n",
        "def generate_majority_contexts(profs, num_cands, device=DEVICE):\n",
        "    # generates majority matrix contexts\n",
        "    # \n",
        "    # profs: list of profiles\n",
        "    # outputs: [bs, num_cands * num_cands]\n",
        "\n",
        "    bs = len(profs)\n",
        "    \n",
        "    contexts = torch.zeros((bs, num_cands, num_cands), device=device)\n",
        "    \n",
        "    for pidx, prof in enumerate(profs): \n",
        "        for c1 in prof.candidates: \n",
        "            for c2 in prof.candidates: \n",
        "                if prof.majority_prefers(c1, c2):\n",
        "                    contexts[pidx, c1, c2] = 1.0\n",
        "                elif prof.majority_prefers(c2, c1): \n",
        "                    contexts[pidx, c1, c2] = -1.0\n",
        "                else:\n",
        "                    contexts[pidx, c1, c2] = 0.0\n",
        "\n",
        "    contexts = torch.flatten(contexts, start_dim=1) # [bs, num_cands * num_cands]\n",
        "    return contexts\n",
        "\n",
        "\n",
        "def generate_sincere_winners_contexts(profs, num_cands, vm, device=DEVICE):\n",
        "    # generates sincere winners contexts\n",
        "    # \n",
        "    # profs: list of profiles\n",
        "    # vm: voting method function\n",
        "    # outputs: [bs, num_cands] (one-hot encoding)\n",
        "\n",
        "    bs = len(profs)\n",
        "\n",
        "    contexts = torch.zeros((bs, num_cands), device=device)\n",
        "\n",
        "    for pidx, prof in enumerate(profs):\n",
        "        ws = vm(prof)\n",
        "        for c in ws:\n",
        "            contexts[pidx, c] = 1.0\n",
        "\n",
        "    return contexts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scores_to_qual_scores(scores):\n",
        "\n",
        "    # sort the scores\n",
        "    sorted_scores = sorted(list(set(scores)))\n",
        "\n",
        "    return [sorted_scores.index(s) + 1 for s in scores]\n",
        "\n",
        "def generate_score_ranking_context(profs, scoring_rule = 'plurality', device=DEVICE):\n",
        "    # generates score rankings from utility profiles for each candidate\n",
        "    #\n",
        "    # profs: list of profiles\n",
        "    # scoring_rule: ('plurality', 'borda')\n",
        "    # returns: [bs, num_cands]\n",
        "\n",
        "    if scoring_rule == 'plurality':\n",
        "        scores = [ prof.plurality_scores() for prof in profs] # list of dicts\n",
        "    elif scoring_rule == 'borda':\n",
        "        scores = [ prof.borda_scores() for prof in profs] # list of dicts\n",
        "\n",
        "    final_rankings = []\n",
        "    for score in scores:\n",
        "\n",
        "        scores_list = []\n",
        "        for cand_index in sorted(score.keys()):\n",
        "            scores_list.append(score[cand_index])\n",
        "\n",
        "        final_rankings.append(scores_to_qual_scores(scores_list))\n",
        "\n",
        "    return torch.tensor(\n",
        "        final_rankings,\n",
        "        device=device,\n",
        "    ) # [bs, num_cands]\n",
        "\n",
        "def generate_full_profile_contexts(profs, num_cands, num_voters, device=DEVICE):\n",
        "    # generates full profile contexts\n",
        "    # \n",
        "    # profs: list of profiles\n",
        "    # outputs: [bs, num_voters * num_actions]\n",
        "\n",
        "    bs = len(profs)\n",
        "    \n",
        "    action_space_size = math.factorial(num_cands)\n",
        "\n",
        "    contexts = torch.zeros((bs, num_voters, action_space_size), device=device)\n",
        "\n",
        "    # generate linear rankings as list comprehension\n",
        "\n",
        "    for pidx, prof in enumerate(profs): \n",
        "        for ridx, r in enumerate(prof.rankings): \n",
        "            contexts[pidx, ridx, permutations_index_dict[num_cands][tuple(r)]] = 1.0\n",
        "\n",
        "    contexts = torch.flatten(contexts, start_dim=1) # [bs, num_voters * num_actions]\n",
        "    return contexts\n",
        "\n",
        "def generate_anon_prof_contexts(profs, num_cands,  device=DEVICE):\n",
        "    # generates anonymous profile contexts\n",
        "    # \n",
        "    # profs: list of profiles\n",
        "    # outputs: [bs, num_actions]\n",
        "\n",
        "    bs = len(profs)\n",
        "    \n",
        "    action_space_size = math.factorial(num_cands)\n",
        "\n",
        "    contexts = torch.zeros((bs, action_space_size), device=device)\n",
        "    \n",
        "    for pidx, prof in enumerate(profs):\n",
        "        rankings, counts = prof.rankings_counts \n",
        "        for ridx, r in enumerate(rankings): \n",
        "            contexts[pidx, permutations_index_dict[num_cands][tuple(r)]] = counts[ridx]\n",
        "\n",
        "    return contexts\n",
        "\n",
        "def generate_margin_contexts(profs, num_cands, device=DEVICE):\n",
        "    # generates margin matrix contexts\n",
        "    # \n",
        "    # profs: list of profiles\n",
        "    # outputs: [bs, num_cands * num_cands]\n",
        "\n",
        "    bs = len(profs)\n",
        "    \n",
        "    contexts = torch.zeros((bs, num_cands, num_cands), device=device)\n",
        "    \n",
        "    for pidx, prof in enumerate(profs): \n",
        "        for c1 in prof.candidates: \n",
        "            for c2 in prof.candidates: \n",
        "                contexts[pidx, c1, c2] = prof.margin(c1, c2)\n",
        "\n",
        "    contexts = torch.flatten(contexts, start_dim=1) # [bs, num_cands * num_cands]\n",
        "    return contexts\n",
        "\n",
        "def generate_qual_margin_contexts(profs, num_cands, device=DEVICE):\n",
        "    # generates qualitative margin contexts\n",
        "    # \n",
        "    # profs: list of profiles\n",
        "    # outputs: [bs, num_cands * num_cands]\n",
        "\n",
        "    bs = len(profs)\n",
        "    \n",
        "    contexts = torch.zeros((bs, num_cands, num_cands), device=device)\n",
        "    \n",
        "    for pidx, prof in enumerate(profs): \n",
        "        pos_margins = [prof.margin(c1, c2) for c1 in prof.candidates for c2 in prof.candidates if prof.margin(c1, c2) > 0]\n",
        "        qual_pos_margins = scores_to_qual_scores(pos_margins)\n",
        "        pos_margin_to_qual_margin = dict(zip(pos_margins, qual_pos_margins))\n",
        "        for c1 in prof.candidates: \n",
        "            for c2 in prof.candidates: \n",
        "                if prof.margin(c1, c2) > 0:\n",
        "                    contexts[pidx, c1, c2] = pos_margin_to_qual_margin[prof.margin(c1, c2)]\n",
        "                elif prof.margin(c1, c2) < 0:\n",
        "                    contexts[pidx, c1, c2] = -1.0 * pos_margin_to_qual_margin[-prof.margin(c1, c2)]\n",
        "                else:\n",
        "                    contexts[pidx, c1, c2] = 0.0\n",
        "\n",
        "    contexts = torch.flatten(contexts, start_dim=1) # [bs, num_cands * num_cands]\n",
        "    return contexts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.6 Labeling Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.6.1 Satisficing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def profitable_manipulations(vm, uprof, manip_weight):\n",
        "    # Calculates all profitable manipulations for voter 0 in a utility profile\n",
        "\n",
        "    u = uprof.utilities[0]\n",
        "    prof = to_linear_prof(uprof)\n",
        "    prof_with_clones = clone_voter(prof, manip_weight=manip_weight)\n",
        "    strat_rankings = list()\n",
        "    for new_r in itertools.permutations(prof.candidates):\n",
        "        if new_r != prof.rankings[0]:\n",
        "            new_prof = apply_manipulation(prof_with_clones, new_r, manip_weight)\n",
        "            ws = vm(prof)\n",
        "            new_ws = vm(new_prof)\n",
        "            if ws != new_ws:\n",
        "                exp_u_ws  = np.average([u(w) for w in ws])\n",
        "                new_exp_u_ws = np.average([u(w) for w in new_ws])\n",
        "                if new_exp_u_ws > exp_u_ws:\n",
        "                    strat_rankings.append(new_r)\n",
        "    return strat_rankings\n",
        "\n",
        "\n",
        "def mask_manipulations(election, vm, manip_weight):\n",
        "    # Generates the binary mask of which manipulations are\n",
        "    # profitable.\n",
        "    # \n",
        "    # election is a tuple of (utility_fn, profile)\n",
        "    \n",
        "    utility_fn, prof = election\n",
        "\n",
        "    prof_with_clones = clone_voter(prof, manip_weight=manip_weight)\n",
        "\n",
        "    output_mask = torch.zeros((math.factorial(len(prof.candidates)),))\n",
        "\n",
        "    ws = vm(prof)\n",
        "    exp_u_ws  = np.average([utility_fn(w) for w in ws])\n",
        "\n",
        "    for i, new_r in enumerate(itertools.permutations(prof.candidates)):\n",
        "        new_prof = apply_manipulation(prof_with_clones, new_r, manip_weight)\n",
        "\n",
        "        new_ws = vm(new_prof)\n",
        "        if new_ws == ws:\n",
        "            output_mask[i] = 0.0\n",
        "        else:\n",
        "            new_exp_u_ws = np.average([utility_fn(w) for w in new_ws])\n",
        "            if new_exp_u_ws > exp_u_ws:\n",
        "                output_mask[i] = 1.0\n",
        "            else:\n",
        "                output_mask[i] = -1.0\n",
        "    return output_mask\n",
        "\n",
        "\n",
        "def generate_labels_reduced_actions(\n",
        "    action_dists, \n",
        "    utility_fns, \n",
        "    profs, \n",
        "    num_cands, \n",
        "    vm, \n",
        "    manip_weight, \n",
        "    reduction_contexts=None\n",
        "):\n",
        "    # Reduce the distribution of actions (action_dists) to the probability\n",
        "    # of choosing a profitable manipulation.\n",
        "\n",
        "    # Returns: output_dists (bs, 2), output_labels (bs, 2)\n",
        "\n",
        "    batch_size = len(profs)\n",
        "\n",
        "    pool = multiprocess.Pool(processes=cpus)\n",
        "\n",
        "    if reduction_contexts is None:\n",
        "        manipulation_responses = pool.map(\n",
        "            lambda x: mask_manipulations(x, vm, manip_weight),\n",
        "            zip(utility_fns, profs)\n",
        "        )\n",
        "\n",
        "        masks = torch.stack(\n",
        "            manipulation_responses\n",
        "        ).to(action_dists.device) # [bs, num_actions]\n",
        "    else:\n",
        "        masks = reduction_contexts\n",
        "\n",
        "    positive_mask = masks > 0 # [bs, num_actions]\n",
        "\n",
        "    # check for places in the batch where there is no positive choice\n",
        "    no_positive = ~positive_mask.any(dim=-1) # [bs, ]\n",
        "\n",
        "    positive_mask[no_positive] = masks[no_positive] >= 0\n",
        "\n",
        "    negative_mask = ~positive_mask\n",
        "\n",
        "\n",
        "    output_dists = torch.zeros((batch_size, 2)).to(action_dists.device) # [pos, neg]\n",
        "\n",
        "    output_dists[:, 0] = torch.sum(action_dists * positive_mask, dim=-1)\n",
        "    output_dists[:, 1] = torch.sum(action_dists * negative_mask, dim=-1)\n",
        "\n",
        "    output_labels = torch.zeros_like(output_dists)\n",
        "    output_labels[:, 0] = 1.0\n",
        "\n",
        "    return output_dists, output_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  1.6.2. Optimizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC0i-RSCvdM7"
      },
      "outputs": [],
      "source": [
        "def find_best_response(election, vm, manip_weight):\n",
        "    # Find best response(s) for voter 0 in an election\n",
        "    # \n",
        "    # Return: list of best responses\n",
        "\n",
        "    utility_fn, prof = election\n",
        "\n",
        "    prof_with_clones = clone_voter(prof, manip_weight=manip_weight)\n",
        "\n",
        "    ws = vm(prof)\n",
        "    eu_ws = np.average([utility_fn(w) for w in ws])\n",
        "    best_rankings = list()\n",
        "    current_best_eu = eu_ws\n",
        "    for new_ranking in itertools.permutations(prof.candidates):\n",
        "        new_prof = apply_manipulation(prof_with_clones, new_ranking, manip_weight)\n",
        "        new_ws = vm(new_prof)\n",
        "        new_eu_ws = np.average([utility_fn(w) for w in new_ws])\n",
        "        if new_eu_ws > current_best_eu:\n",
        "            current_best_eu = new_eu_ws\n",
        "            best_rankings = [new_ranking]\n",
        "        elif new_eu_ws == current_best_eu:\n",
        "            best_rankings.append(new_ranking)\n",
        "    return best_rankings\n",
        "\n",
        "\n",
        "def threaded_generate_classification_labels(utility_fns, profs, num_cands, vm, manip_weight):\n",
        "    # generate labels for the optimize labeling.\n",
        "\n",
        "    batch_size = len(profs)\n",
        "\n",
        "    pool = multiprocess.Pool(processes=cpus)\n",
        "    best_responses = pool.map(\n",
        "        lambda x: find_best_response(x, vm, manip_weight),\n",
        "        zip(utility_fns, profs)\n",
        "        # zip(uprofs, [vm for _ in range(batch_size)]),\n",
        "    )\n",
        "\n",
        "    output = torch.zeros((batch_size, len(permutations_of[num_cands])))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        best_response = best_responses[i]\n",
        "        for j, perm in enumerate(permutations_of[num_cands]):\n",
        "            if perm in best_response:\n",
        "                output[i, j] = 1.0\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def generate_labels_reduced_actions_opt(\n",
        "    action_dists, \n",
        "    utility_fns, \n",
        "    profs, \n",
        "    num_cands, \n",
        "    vm, \n",
        "    manip_weight, \n",
        "    reduction_contexts=None):\n",
        "    # Reduce the distribution of actions (action_dists) to the probability\n",
        "    # of choosing an optimal manipulation. \n",
        "\n",
        "    # Uses threaded_generate_classification_labels to generate the labels.\n",
        "\n",
        "    # Returns: output_dists (bs, 2), output_labels (bs, 2)\n",
        "\n",
        "    batch_size = len(profs)\n",
        "\n",
        "    if reduction_contexts is None:\n",
        "        optimal_labels = threaded_generate_classification_labels(\n",
        "            utility_fns,\n",
        "            profs,\n",
        "            num_cands=num_cands, \n",
        "            vm=vm, \n",
        "            manip_weight=manip_weight\n",
        "        ).to(action_dists.device)\n",
        "    else:\n",
        "        optimal_labels = reduction_contexts\n",
        "\n",
        "    output_dists = torch.zeros((batch_size, 2)).to(action_dists.device) # [pos, neg]\n",
        "\n",
        "    positive_mask = optimal_labels == 1\n",
        "    negative_mask = ~positive_mask\n",
        "\n",
        "    output_dists[:, 0] = torch.sum(action_dists * positive_mask, dim = -1)\n",
        "    output_dists[:, 1] = torch.sum(action_dists * negative_mask, dim=-1)\n",
        "\n",
        "    output_labels = torch.zeros_like(output_dists)\n",
        "    output_labels[:, 0] = 1.0\n",
        "\n",
        "    return output_dists, output_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.7. Validation setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reward_function(\n",
        "    actions, \n",
        "    utility_fns, \n",
        "    profs, \n",
        "    vm, \n",
        "    num_cands, \n",
        "    manip_weight, \n",
        "    metric_op='normalized_subtract'):\n",
        "    # find the profitability of manipulation (as in Equation 4 from the paper)\n",
        "    # for voter 0 in each profile.\n",
        "\n",
        "    # actions = [BS,]\n",
        "    # uprofs = list of uprofs\n",
        "    # vm = voting method fn\n",
        "\n",
        "    profs_with_clones = [clone_voter(prof, manip_weight=manip_weight) for prof in profs]\n",
        "\n",
        "    ws_batch = [vm(prof) for prof in profs_with_clones]\n",
        "    cands_batch = [prof.candidates for prof in profs_with_clones]\n",
        "\n",
        "    exp_util_ws_batch = torch.tensor([\n",
        "        np.average([utility_fn(w) for w in ws])\n",
        "        for utility_fn, ws in zip(utility_fns, ws_batch)\n",
        "    ]).float() # [BS,]\n",
        "    exp_util_ws_batch = exp_util_ws_batch.to(DEVICE)\n",
        "\n",
        "    max_util_batch = torch.tensor([\n",
        "        np.max([utility_fn(c) for c in cands])\n",
        "        for utility_fn, cands in zip(utility_fns, cands_batch)\n",
        "    ]).float() # [BS,]\n",
        "    max_util_batch = max_util_batch.to(DEVICE)\n",
        "\n",
        "    min_util_batch = torch.tensor([\n",
        "        np.min([utility_fn(c) for c in cands])\n",
        "        for utility_fn, cands in zip(utility_fns, cands_batch)\n",
        "    ]).float() # [BS,]\n",
        "    min_util_batch = min_util_batch.to(DEVICE)\n",
        "\n",
        "    new_profs = [\n",
        "        apply_manipulation(prof, permutations_of[num_cands][action], manip_weight)\n",
        "        for prof, action in zip(profs_with_clones, actions)\n",
        "    ]\n",
        "\n",
        "    new_ws_batch = [vm(new_prof) for new_prof in new_profs]\n",
        "    \n",
        "    new_exp_util_ws_batch = torch.tensor([\n",
        "        np.average([utility_fn(w) for w in new_ws])\n",
        "        for utility_fn, new_ws in zip(utility_fns, new_ws_batch)\n",
        "    ]).float() # [BS,]\n",
        "\n",
        "    new_exp_util_ws_batch = new_exp_util_ws_batch.to(DEVICE)\n",
        "\n",
        "    if metric_op == 'subtract':\n",
        "        reward = new_exp_util_ws_batch - exp_util_ws_batch\n",
        "    elif metric_op == 'normalized_subtract':\n",
        "        reward = (new_exp_util_ws_batch - exp_util_ws_batch) / (max_util_batch - min_util_batch)\n",
        "    # create mask for which indices are different\n",
        "    # add the cost to those indices\n",
        "\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validation_function(\n",
        "    agent, \n",
        "    batch_size, \n",
        "    vm, \n",
        "    num_cands, \n",
        "    num_voters, \n",
        "    manip_weight, \n",
        "    elections, \n",
        "    decision_rule='argmax', \n",
        "    metric_op=\"normalized_subtract\", \n",
        "    agent_infos=('plurality_scores',)):\n",
        "    # computes the profitability of submitted rankings chosen by \n",
        "    # the agent according to the decision_rule.\n",
        "\n",
        "    manipulator_utility_fns, profiles = elections\n",
        "\n",
        "    manipulator_utilities = torch.tensor(\n",
        "        [\n",
        "            [m_util_fn(i) for i in range(num_cands)]\n",
        "            for m_util_fn in manipulator_utility_fns\n",
        "        ],\n",
        "    ).float().to(DEVICE)\n",
        "\n",
        "    additional_contexts = None # guarantee that this is of shape [bs, x]\n",
        "\n",
        "    additional_contexts = [] # guarantee that each entry is of shape [bs, x]\n",
        "\n",
        "    for agent_info in agent_infos:\n",
        "        additional_context = None\n",
        "\n",
        "        if agent_info == 'full':\n",
        "            additional_context = generate_full_profile_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                num_voters=num_voters,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'anon_prof':\n",
        "            additional_context = generate_anon_prof_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE,\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'plurality_scores':\n",
        "\n",
        "            additional_context = generate_score_context(\n",
        "                profiles,\n",
        "                scoring_rule='plurality',\n",
        "                device=DEVICE,\n",
        "            ).float()\n",
        "            \n",
        "        elif agent_info == 'plurality_ranking':\n",
        "\n",
        "            additional_context = generate_score_ranking_context(\n",
        "                profiles,\n",
        "                scoring_rule='plurality',\n",
        "                device=DEVICE,\n",
        "            ).float()\n",
        "            \n",
        "        elif agent_info == 'borda_scores':\n",
        "            additional_context = generate_score_context(\n",
        "                profiles,\n",
        "                scoring_rule='borda',\n",
        "                device=DEVICE,\n",
        "            ).float()\n",
        "\n",
        "        elif agent_info == 'margin':\n",
        "            additional_context = generate_margin_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE\n",
        "            )\n",
        "        elif agent_info == 'qual_margin':\n",
        "            additional_context = generate_qual_margin_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'majority':\n",
        "            additional_context = generate_majority_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "\n",
        "        elif agent_info == 'sincere_winners':\n",
        "            additional_context = generate_sincere_winners_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                vm=vm,\n",
        "                device=DEVICE\n",
        "            )\n",
        "            \n",
        "        additional_contexts.append(additional_context)\n",
        "\n",
        "    additional_contexts = torch.cat(additional_contexts, dim=-1)\n",
        "\n",
        "    action_probs_batch, actions_batch = agent(manipulator_utilities, additional_contexts)\n",
        "\n",
        "    if decision_rule == 'expectation':\n",
        "\n",
        "        eval_result = torch.zeros((batch_size,)).to(DEVICE)\n",
        "\n",
        "        for i in range(action_probs_batch.shape[-1]):\n",
        "            actions_batch = torch.ones_like(actions_batch) * i\n",
        "\n",
        "            reward_val = reward_function(\n",
        "                actions=actions_batch,\n",
        "                utility_fns=manipulator_utility_fns,\n",
        "                profs=profiles,\n",
        "                vm=vm,\n",
        "                num_cands=num_cands,\n",
        "                manip_weight=manip_weight,\n",
        "                metric_op=metric_op,\n",
        "            )\n",
        "\n",
        "            eval_result += reward_val * action_probs_batch[:, i]\n",
        "    elif decision_rule == 'argmax':\n",
        "        # using the argmax\n",
        "        actions_batch = torch.argmax(action_probs_batch, dim=-1)\n",
        "        eval_result = reward_function(\n",
        "                actions=actions_batch,\n",
        "                utility_fns=manipulator_utility_fns,\n",
        "                profs=profiles,\n",
        "                vm=vm,\n",
        "                num_cands=num_cands,\n",
        "                manip_weight=manip_weight,\n",
        "                metric_op=metric_op,\n",
        "            )\n",
        "    elif decision_rule == 'distribution':\n",
        "        eval_result = reward_function(\n",
        "            actions=actions_batch,\n",
        "            utility_fns=manipulator_utility_fns,\n",
        "            profs=profiles,\n",
        "            vm=vm,\n",
        "            num_cands=num_cands,\n",
        "            manip_weight=manip_weight,\n",
        "            metric_op=metric_op,\n",
        "        )\n",
        "    else:\n",
        "        raise Exception(\"pick one\")\n",
        "    \n",
        "    return eval_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.8. Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDC6GiL69Dmh"
      },
      "outputs": [],
      "source": [
        "def train_strategy_classification(\n",
        "        num_voters, \n",
        "        num_candidates, \n",
        "        training_elections,\n",
        "        validation_elections,\n",
        "        model_size=[128, 64, 32], \n",
        "        vm=pref_voting.voting_methods.plurality,\n",
        "        max_num_iterations = 2000,\n",
        "        validate_every = 20, \n",
        "        reduction_contexts = None, \n",
        "        training_batch_size = 512, \n",
        "        validation_batch_size = 4096, \n",
        "        patience = 20, \n",
        "        threshold = 0.001, \n",
        "        learning_rate = 6e-3, \n",
        "        labeling='satisfice', \n",
        "        agent_infos=[('plurality_scores',)], \n",
        "        manip_weight = 1):  \n",
        "    # the main training function, including periodic validation checks. \n",
        "\n",
        "    input_dim = 0\n",
        "\n",
        "    input_dim += num_candidates\n",
        "\n",
        "    agent_infos = sorted(agent_infos)\n",
        "\n",
        "    # additional input dimension for each type of context\n",
        "    dim_additions = {\n",
        "        'full': num_voters * math.factorial(num_candidates),\n",
        "        'anon_prof':  math.factorial(num_candidates),\n",
        "        'plurality_scores': num_candidates,\n",
        "        'plurality_ranking': num_candidates,\n",
        "        'borda_scores': num_candidates,\n",
        "        'margin': num_candidates * num_candidates,\n",
        "        'qual_margin': num_candidates * num_candidates,\n",
        "        'majority': num_candidates * num_candidates,\n",
        "        'sincere_winners': num_candidates,\n",
        "    }\n",
        "\n",
        "    for agent_info in agent_infos:\n",
        "        input_dim += dim_additions[agent_info]\n",
        "\n",
        "    # Create agent\n",
        "    agent = Agent(\n",
        "        input_dim=input_dim,\n",
        "        output_dim=num_candidates, classification=False,\n",
        "        layers=model_size,\n",
        "    )\n",
        "\n",
        "    agent = agent.to(DEVICE)\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # set up optimizer\n",
        "    optimizer = torch.optim.Adam(\n",
        "        agent.parameters(),\n",
        "        learning_rate, \n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    train_evals = []\n",
        "    #assert len(elections[0]) >= max_num_iterations * batch_size and len(elections[0]) == len(elections[1])\n",
        "\n",
        "    best_eval = float('-inf')\n",
        "    patience_counter = 0\n",
        "    val_iter = 0\n",
        "    for iter in tqdm(range(max_num_iterations)):\n",
        "\n",
        "        #print(\"[INFO] iter: {}...\".format(iter + 1))\n",
        "\n",
        "        # build batch up\n",
        "\n",
        "        iter_training_elections = (\n",
        "            training_elections[0][iter * training_batch_size: iter * training_batch_size + training_batch_size],\n",
        "            training_elections[1][iter * training_batch_size: iter * training_batch_size + training_batch_size]\n",
        "        )\n",
        "\n",
        "        iter_manipulator_utility_functions = iter_training_elections[0]\n",
        "        iter_profiles = iter_training_elections[1]\n",
        "\n",
        "        iter_reduction_contexts = reduction_contexts[iter * training_batch_size: iter * training_batch_size + training_batch_size]\n",
        "        \n",
        "        manipulator_utilities = torch.tensor(\n",
        "            [\n",
        "                [m_util_fn(i) for i in range(num_candidates)]\n",
        "                for m_util_fn in iter_manipulator_utility_functions\n",
        "            ],\n",
        "        ).float().to(DEVICE)\n",
        "\n",
        "        additional_contexts = [] # guarantee that each entry is of shape [bs, x]\n",
        "\n",
        "        for agent_info in agent_infos:\n",
        "            additional_context = None\n",
        "\n",
        "            if agent_info == 'full':\n",
        "                additional_context = generate_full_profile_contexts(\n",
        "                    iter_profiles,\n",
        "                    num_cands=num_candidates,\n",
        "                    num_voters=num_voters,\n",
        "                    device=DEVICE\n",
        "                )\n",
        "\n",
        "            elif agent_info == 'anon_prof':\n",
        "                additional_context = generate_anon_prof_contexts(\n",
        "                    iter_profiles,\n",
        "                    num_cands=num_candidates,\n",
        "                    device=DEVICE,\n",
        "                )\n",
        "\n",
        "            elif agent_info == 'plurality_scores':\n",
        "\n",
        "                additional_context = generate_score_context(\n",
        "                    iter_profiles,\n",
        "                    scoring_rule='plurality',\n",
        "                    device=DEVICE,\n",
        "                ).float()\n",
        "                \n",
        "            elif agent_info == 'borda_scores':\n",
        "                additional_context = generate_score_context(\n",
        "                    iter_profiles,\n",
        "                    scoring_rule='borda',\n",
        "                    device=DEVICE,\n",
        "                ).float()\n",
        "\n",
        "            elif agent_info == 'plurality_ranking':\n",
        "\n",
        "                additional_context = generate_score_ranking_context(\n",
        "                    iter_profiles,\n",
        "                    scoring_rule='plurality',\n",
        "                    device=DEVICE,\n",
        "                ).float()\n",
        "\n",
        "            elif agent_info == 'margin':\n",
        "                additional_context = generate_margin_contexts(\n",
        "                    iter_profiles,\n",
        "                    num_cands=num_candidates,\n",
        "                    device=DEVICE\n",
        "                )\n",
        "\n",
        "            elif agent_info == 'majority':\n",
        "                additional_context = generate_majority_contexts(\n",
        "                    iter_profiles,\n",
        "                    num_cands=num_candidates,\n",
        "                    device=DEVICE\n",
        "                )\n",
        "            elif agent_info == 'qual_margin':\n",
        "                additional_context = generate_qual_margin_contexts(\n",
        "                    iter_profiles,\n",
        "                    num_cands=num_candidates,\n",
        "                    device=DEVICE\n",
        "                )\n",
        "\n",
        "            elif agent_info == 'sincere_winners':\n",
        "                additional_context = generate_sincere_winners_contexts(\n",
        "                    iter_profiles,\n",
        "                    num_cands=num_candidates,\n",
        "                    vm=vm,\n",
        "                    device=DEVICE\n",
        "                )\n",
        "            \n",
        "            additional_contexts.append(additional_context)\n",
        "\n",
        "        additional_contexts = torch.cat(additional_contexts, dim=-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # get agent actions\n",
        "        action_probs_batch, actions_batch = agent(manipulator_utilities, additional_contexts)\n",
        "\n",
        "        # print(action_probs_batch)\n",
        "\n",
        "        if labeling == 'satisfice':\n",
        "\n",
        "            reduced_actions, labels = generate_labels_reduced_actions(\n",
        "                action_probs_batch, \n",
        "                iter_manipulator_utility_functions,\n",
        "                iter_profiles,\n",
        "                num_candidates, \n",
        "                vm=vm,\n",
        "                manip_weight=manip_weight,\n",
        "                reduction_contexts=iter_reduction_contexts,\n",
        "            )\n",
        "\n",
        "        elif labeling == 'optimize':\n",
        "\n",
        "            reduced_actions, labels = generate_labels_reduced_actions_opt(\n",
        "                action_probs_batch, \n",
        "                iter_manipulator_utility_functions,\n",
        "                iter_profiles,\n",
        "                num_candidates, \n",
        "                vm=vm,\n",
        "                manip_weight=manip_weight,\n",
        "                reduction_contexts=iter_reduction_contexts,\n",
        "            )\n",
        "\n",
        "        loss = loss_fn(reduced_actions, labels)\n",
        "        #print(\"loss:\", loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        if (iter + 1) % validate_every == 0:\n",
        "            with torch.no_grad():\n",
        "                agent.eval()\n",
        "                iter_validation_elections = (\n",
        "                validation_elections[0][val_iter * validation_batch_size: val_iter * validation_batch_size + validation_batch_size],\n",
        "                validation_elections[1][val_iter * validation_batch_size: val_iter * validation_batch_size + validation_batch_size]\n",
        "                )\n",
        "                iter_evals = validation_function(agent, validation_batch_size, vm, num_candidates, num_voters, manip_weight, iter_validation_elections, decision_rule='argmax',metric_op=\"normalized_subtract\", agent_infos=agent_infos)\n",
        "                iter_eval = iter_evals.mean().cpu().detach().numpy()\n",
        "                print(\"iter_eval\", iter_eval)\n",
        "                train_evals.append(iter_eval)\n",
        "\n",
        "                val_iter += 1\n",
        "\n",
        "                if iter_eval - best_eval > threshold: \n",
        "                    best_eval = iter_eval\n",
        "                    patience_counter = 0\n",
        "                    best_agent = copy.deepcopy(agent)\n",
        "                    best_losses = train_losses\n",
        "                    best_evals = train_evals\n",
        "                else: \n",
        "                    patience_counter += 1\n",
        "\n",
        "                if patience_counter >= patience: \n",
        "                    print(\"best eval is \", best_eval)\n",
        "                    break\n",
        "            agent.train()\n",
        "    \n",
        "    return best_agent, best_losses, best_evals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.9. Evaluation setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluation_function(\n",
        "    agent, \n",
        "    vm, \n",
        "    num_cands, \n",
        "    num_voters, \n",
        "    manip_weight, \n",
        "    all_utility_profs, \n",
        "    decision_rule, \n",
        "    metric_op, \n",
        "    agent_infos, \n",
        "    num_samples, \n",
        "    step):\n",
        "    # evaluate the trained agent's submitted rankings\n",
        "\n",
        "    utility_profiles = all_utility_profs[num_samples*step:num_samples*(step+1)]\n",
        "\n",
        "    manipulator_utility_fns = [uprof.utilities[0] for uprof in utility_profiles]\n",
        "    profiles = [to_linear_prof(uprof) for uprof in utility_profiles]\n",
        "\n",
        "    manipulator_utilities = torch.tensor(\n",
        "        [\n",
        "            [m_util_fn(i) for i in range(num_cands)]\n",
        "            for m_util_fn in manipulator_utility_fns\n",
        "        ],\n",
        "    ).float().to(DEVICE)\n",
        "\n",
        "    additional_contexts = None # guarantee that this is of shape [bs, x]\n",
        "\n",
        "    additional_contexts = [] # guarantee that each entry is of shape [bs, x]\n",
        "\n",
        "    for agent_info in agent_infos:\n",
        "        additional_context = None\n",
        "\n",
        "        if agent_info == 'full':\n",
        "            additional_context = generate_full_profile_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                num_voters=num_voters,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'anon_prof':\n",
        "            additional_context = generate_anon_prof_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE,\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'plurality_scores':\n",
        "\n",
        "            additional_context = generate_score_context(\n",
        "                profiles,\n",
        "                scoring_rule='plurality',\n",
        "                device=DEVICE,\n",
        "            ).float()\n",
        "            \n",
        "        elif agent_info == 'plurality_ranking':\n",
        "\n",
        "            additional_context = generate_score_ranking_context(\n",
        "                profiles,\n",
        "                scoring_rule='plurality',\n",
        "                device=DEVICE,\n",
        "            ).float()\n",
        "            \n",
        "        elif agent_info == 'borda_scores':\n",
        "            additional_context = generate_score_context(\n",
        "                profiles,\n",
        "                scoring_rule='borda',\n",
        "                device=DEVICE,\n",
        "            ).float()\n",
        "\n",
        "        elif agent_info == 'margin':\n",
        "            additional_context = generate_margin_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE\n",
        "            )\n",
        "        elif agent_info == 'qual_margin':\n",
        "            additional_context = generate_qual_margin_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'majority':\n",
        "            additional_context = generate_majority_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "        elif agent_info == 'sincere_winners':\n",
        "            additional_context = generate_sincere_winners_contexts(\n",
        "                profiles,\n",
        "                num_cands=num_cands,\n",
        "                vm=vm,\n",
        "                device=DEVICE\n",
        "            )\n",
        "            \n",
        "        additional_contexts.append(additional_context)\n",
        "\n",
        "    additional_contexts = torch.cat(additional_contexts, dim=-1)\n",
        "\n",
        "    action_probs_batch, actions_batch = agent(manipulator_utilities, additional_contexts)\n",
        "\n",
        "    if decision_rule == 'argmax':\n",
        "        actions_batch = torch.argmax(action_probs_batch, dim=-1)\n",
        "        eval_result = reward_function(\n",
        "                actions=actions_batch,\n",
        "                utility_fns=manipulator_utility_fns,\n",
        "                profs=profiles,\n",
        "                vm=vm,\n",
        "                num_cands=num_cands,\n",
        "                manip_weight=manip_weight,\n",
        "                metric_op=metric_op,\n",
        "            )\n",
        "\n",
        "    elif decision_rule == 'distribution':\n",
        "        eval_result = reward_function(\n",
        "            actions=actions_batch,\n",
        "            utility_fns=manipulator_utility_fns,\n",
        "            profs=profiles,\n",
        "            vm=vm,\n",
        "            num_cands=num_cands,\n",
        "            manip_weight=manip_weight,\n",
        "            metric_op=metric_op,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"pick one\")\n",
        "    \n",
        "    return np.array([eval_result.cpu().detach().numpy()])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Generate utility profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_utility_profile(\n",
        "    num_cands, \n",
        "    num_voters, \n",
        "    probmodel = 'uniform', \n",
        "    num_profiles = 1\n",
        "):\n",
        "    # Generate utility profiles\n",
        "    # \n",
        "    # probmodel = ('uniform', 'spatial_2dim')\n",
        "    # Return: list of utility profiles\n",
        "\n",
        "    if probmodel == 'uniform': \n",
        "        return pref_voting.generate_utility_profiles.generate_utility_profile_uniform(num_cands, num_voters, num_profiles = num_profiles)\n",
        "    \n",
        "    elif probmodel == 'spatial_2dim':\n",
        "        ndims = 2\n",
        "        sprofs = pref_voting.generate_spatial_profiles.generate_spatial_profile(num_cands, num_voters, ndims, num_profiles = num_profiles)\n",
        "        return [sprof.to_utility_profile() for sprof in sprofs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate utility profiles for training, validation, and evaluation \n",
        "# and save them to disk as pickle files\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "training_utility_base_dir = \"training_utility_profiles\"\n",
        "validation_utility_base_dir = \"validation_utility_profiles\"\n",
        "evaluation_utility_base_dir = \"evaluation_utility_profiles\"\n",
        "\n",
        "if GENERATE_UTILITY_PROFILES:\n",
        "\n",
        "    os.makedirs(training_utility_base_dir, exist_ok=True)\n",
        "    os.makedirs(validation_utility_base_dir, exist_ok=True)\n",
        "    os.makedirs(evaluation_utility_base_dir, exist_ok=True)\n",
        "\n",
        "    for probmodel in prob_models_list:\n",
        "        for num_cands in num_cands_list:\n",
        "            for num_voters in num_voters_list:\n",
        "                \n",
        "                # creating training utility profiles.\n",
        "\n",
        "                total_utility_profiles = generate_utility_profile(\n",
        "                    num_cands, \n",
        "                    num_voters, \n",
        "                    probmodel = probmodel, \n",
        "                    num_profiles = training_batch_size * max_num_iterations * len(generation_list))\n",
        "                \n",
        "                for gen_idx, gen in enumerate(generation_list):\n",
        "\n",
        "                    start_index = gen_idx * training_batch_size * max_num_iterations\n",
        "                    end_index = (gen_idx + 1) * training_batch_size * max_num_iterations\n",
        "\n",
        "                    elections = total_utility_profiles[start_index: end_index] \n",
        "                    pickle.dump(\n",
        "                        elections, \n",
        "                        open(\n",
        "                            os.path.join(\n",
        "                                training_utility_base_dir,\n",
        "                                f\"training_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                            ), \"wb\",\n",
        "                        ),   \n",
        "                    )\n",
        "\n",
        "                # creating validation utility profiles\n",
        "                total_utility_profiles = generate_utility_profile(\n",
        "                    num_cands, \n",
        "                    num_voters, \n",
        "                    probmodel = probmodel, \n",
        "                    num_profiles = validation_batch_size * math.ceil(max_num_iterations / validate_every) * len(generation_list))\n",
        "\n",
        "                for gen_idx, gen in enumerate(generation_list):\n",
        "\n",
        "                    start_index = gen_idx * validation_batch_size * math.ceil(max_num_iterations / validate_every)\n",
        "                    end_index = (gen_idx + 1) * validation_batch_size * math.ceil(max_num_iterations / validate_every)\n",
        "\n",
        "                    elections = total_utility_profiles[start_index: end_index] \n",
        "                    pickle.dump(\n",
        "                        elections, \n",
        "                        open(\n",
        "                            os.path.join(\n",
        "                                validation_utility_base_dir,\n",
        "                                f\"validation_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                            ), \"wb\",\n",
        "                        ),   \n",
        "                    )\n",
        "\n",
        "                # creating evaluation utility profiles\n",
        "                total_utility_profiles = generate_utility_profile(\n",
        "                    num_cands, \n",
        "                    num_voters, \n",
        "                    probmodel = probmodel, \n",
        "                    num_profiles = evaluation_batch_size * max_num_evaluation_rounds *  len(generation_list))\n",
        "\n",
        "                for gen_idx, gen in enumerate(generation_list):\n",
        "\n",
        "                    start_index = gen_idx * evaluation_batch_size * max_num_evaluation_rounds\n",
        "                    end_index = (gen_idx + 1) * evaluation_batch_size * max_num_evaluation_rounds\n",
        "\n",
        "                    elections = total_utility_profiles[start_index: end_index] \n",
        "                    pickle.dump(\n",
        "                        elections, \n",
        "                        open(\n",
        "                            os.path.join(\n",
        "                                evaluation_utility_base_dir,\n",
        "                                f\"evaluation_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                            ), \"wb\",\n",
        "                        ),   \n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Generate labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate the labels for the stored training profiles \n",
        "# and save to disk as pickle files. \n",
        "\n",
        "if GENERATE_LABELS:        \n",
        "    for gen in generation_list:\n",
        "        for probmodel in prob_models_list:\n",
        "            for num_cands in num_cands_list:\n",
        "                for num_voters in num_voters_list:\n",
        "\n",
        "                    utility_profiles = pickle.load(\n",
        "                        open(\n",
        "                            os.path.join(\n",
        "                                training_utility_base_dir,\n",
        "                                f\"training_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                            ), \"rb\",\n",
        "                        ),\n",
        "                        )\n",
        "\n",
        "                    elections = (\n",
        "                        [uprof.utilities[0] for uprof in utility_profiles],\n",
        "                        [to_linear_prof(uprof) for uprof in utility_profiles],\n",
        "                    )\n",
        "                                \n",
        "                    for manip_weight in manip_weight_list:\n",
        "                        for vm in voting_methods_list:\n",
        "                            for labeling in labeling_list:\n",
        "\n",
        "                                labels_dict = {} \n",
        "\n",
        "                                print(\"Generation:\", gen)\n",
        "                                print(\"Num cands:\", num_cands)\n",
        "                                print(\"Num voters:\", num_voters)\n",
        "                                print(\"Prob model:\", probmodel)\n",
        "                                print(\"Manip weight:\", manip_weight)\n",
        "                                print(\"Voting method:\", vm.name)\n",
        "                                print(\"Labeling:\", labeling)\n",
        "                                print(\"\")\n",
        "\n",
        "                                if labeling == 'satisfice':\n",
        "                                    pool = multiprocess.Pool(processes=cpus)\n",
        "                                    manipulation_responses = pool.map(\n",
        "                                        lambda x: mask_manipulations(x, vm, manip_weight),\n",
        "                                        zip(*elections),\n",
        "                                    )\n",
        "\n",
        "                                    masks = torch.stack(\n",
        "                                        manipulation_responses\n",
        "                                    ).to(\"cpu\") # [bs, num_actions]\n",
        "\n",
        "                                    reduction_contexts = masks\n",
        "\n",
        "                                elif labeling == 'optimize':\n",
        "                                    reduction_contexts = threaded_generate_classification_labels(\n",
        "                                        utility_fns=elections[0],\n",
        "                                        profs=elections[1], \n",
        "                                        num_cands=num_cands,\n",
        "                                        vm=vm, \n",
        "                                        manip_weight=manip_weight,\n",
        "                                        ).to(\"cpu\")\n",
        "                                \n",
        "                                pickle.dump(reduction_contexts, open(f\"labels/labels_{gen}_{num_cands}_{num_voters}_{probmodel}_{manip_weight}_{vm.name}_{labeling}.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Train models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_run_directory():\n",
        "    # helper function to create a run directory\n",
        "    \n",
        "    current_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=-7)))\n",
        "    current_time_str = current_time.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
        "    run_dir = os.path.join(os.getcwd(), \"run_of_{}\".format(current_time_str))\n",
        "    os.mkdir(run_dir)\n",
        "    os.mkdir(os.path.join(run_dir, \"models\"))\n",
        "    os.mkdir(os.path.join(run_dir, \"torch_models\"))\n",
        "    os.mkdir(os.path.join(run_dir, \"losses\"))\n",
        "    os.mkdir(os.path.join(run_dir, \"validation\"))\n",
        "    os.mkdir(os.path.join(run_dir, \"evaluation\"))\n",
        "    os.mkdir(os.path.join(run_dir, \"graphs\"))\n",
        "    return run_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42pF-hHWv3rL",
        "outputId": "9ec4232c-9d1d-4dc2-88c8-770f5bdb8664"
      },
      "outputs": [],
      "source": [
        "# used the stored training and validation profiles and \n",
        "# the stored labels to train models with configurations \n",
        "# from model_size_lists \n",
        "\n",
        "if TRAIN_MODELS: \n",
        "    run_dir = create_run_directory()\n",
        "    print(run_dir)\n",
        "\n",
        "    model_size_lists = [\n",
        "        [4],\n",
        "        [8],\n",
        "        [16],\n",
        "        [32],\n",
        "        [64],\n",
        "        [128],\n",
        "        [256],\n",
        "        [512],\n",
        "\n",
        "        [4,4],\n",
        "        [8,8],\n",
        "        [16,8],\n",
        "        [16,16],\n",
        "        [32,32],\n",
        "        [64,32],\n",
        "        [64,64],\n",
        "        [128,128],\n",
        "        [256,128],\n",
        "        [256,256],\n",
        "\n",
        "        [8,8,8],\n",
        "        [32,16,8],\n",
        "        [32,32,32],\n",
        "        [64,64,64],\n",
        "        [128,64,32],\n",
        "        [128,128,128],\n",
        "        [256,256,256],\n",
        "        [512,256,128],\n",
        "    ]\n",
        "\n",
        "    for gen in generation_list:\n",
        "        for probmodel in prob_models_list:\n",
        "            for num_cands in num_cands_list:\n",
        "                for num_voters in num_voters_list: \n",
        "                    \n",
        "                    training_utility_profiles = pickle.load(\n",
        "                        open(\n",
        "                            os.path.join(\n",
        "                                training_utility_base_dir,\n",
        "                                f\"training_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                            ), \"rb\",\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "                    training_elections = (\n",
        "                        [uprof.utilities[0] for uprof in training_utility_profiles],\n",
        "                        [to_linear_prof(uprof) for uprof in training_utility_profiles],\n",
        "                    )\n",
        "\n",
        "                    validation_utility_profiles = pickle.load(\n",
        "                        open(\n",
        "                            os.path.join(\n",
        "                                validation_utility_base_dir,\n",
        "                                f\"validation_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                            ), \"rb\",\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "                    validation_elections = (\n",
        "                        [uprof.utilities[0] for uprof in validation_utility_profiles],\n",
        "                        [to_linear_prof(uprof) for uprof in validation_utility_profiles],\n",
        "                    )\n",
        "\n",
        "                    print(\"len(validation_elections[0])\", len(validation_elections[0]))\n",
        "                    print(\"len(validation_elections[1])\", len(validation_elections[1]))\n",
        "                    for manip_weight in manip_weight_list:\n",
        "                        for agent_infos in agent_infos_list:\n",
        "                            for vm in voting_methods_list:\n",
        "                                results_dict = {}\n",
        "\n",
        "                                for labeling in labeling_list:\n",
        "\n",
        "                                    reduction_contexts = pickle.load(\n",
        "                                        open(\n",
        "                                            f\"labels/labels_{gen}_{num_cands}_{num_voters}_{probmodel}_{manip_weight}_{vm.name}_{labeling}.pkl\", \n",
        "                                            \"rb\",\n",
        "                                        ),\n",
        "                                    )\n",
        "                                    \n",
        "                                    reduction_contexts = reduction_contexts.to(DEVICE)\n",
        "                                    \n",
        "                                    for model_size in model_size_lists:\n",
        "                                        for learning_rate in learning_rates:\n",
        "                                            \n",
        "                                            print(\"Generation:\", gen)\n",
        "                                            print(\"Num cands:\", num_cands)\n",
        "                                            print(\"Num voters:\", num_voters)\n",
        "                                            print(\"Prob model:\", probmodel)\n",
        "                                            print(\"Manip weight:\", manip_weight)\n",
        "                                            print(\"Voting method:\", vm.name)\n",
        "                                            print(\"Labeling:\", labeling)\n",
        "                                            print(\"Agent info:\", agent_infos)\n",
        "                                            print(\"Learning rate:\", learning_rate)\n",
        "                                            print(\"\")\n",
        "\n",
        "\n",
        "                                            torch.manual_seed(144 + gen)\n",
        "                                            np.random.seed(144 + gen)\n",
        "\n",
        "                                            agent, losses, evals = train_strategy_classification(\n",
        "                                                num_voters=num_voters,\n",
        "                                                num_candidates=num_cands,\n",
        "                                                training_elections=training_elections,\n",
        "                                                validation_elections=validation_elections,\n",
        "                                                model_size=model_size,\n",
        "                                                vm=vm,\n",
        "                                                max_num_iterations = max_num_iterations,\n",
        "                                                validate_every=validate_every,\n",
        "                                                reduction_contexts=reduction_contexts,\n",
        "                                                training_batch_size=training_batch_size,\n",
        "                                                validation_batch_size=validation_batch_size,\n",
        "                                                learning_rate = learning_rate,\n",
        "                                                labeling=labeling,\n",
        "                                                agent_infos=agent_infos,\n",
        "                                                manip_weight=manip_weight,\n",
        "                                                patience = patience,\n",
        "                                            )\n",
        "\n",
        "                                            torch.save(agent,\n",
        "                                                os.path.join(\n",
        "                                                    f'{run_dir}/torch_models', \n",
        "                                                    f'{vm.name}_{gen}_{num_cands}_{num_voters}_{probmodel}_{learning_rate}_{tuple(model_size)}_{max_num_iterations}_{labeling}_{tuple(agent_infos)}_{manip_weight}.pth', \n",
        "                                                )  \n",
        "                                            )\n",
        "\n",
        "                                            results_dict[\n",
        "                                                (vm.name, gen, tuple(model_size), num_cands, num_voters, probmodel, learning_rate, training_batch_size, max_num_iterations, labeling, tuple(agent_infos), manip_weight)\n",
        "                                            ] = (agent.to(\"cpu\"), losses)\n",
        "                                            print('len(evals)', len(evals))\n",
        "                                            plt.plot(losses, label=\"loss\")\n",
        "                                            plt.plot([((i+1) * validate_every) - 1 for i in range(len(evals))], evals, label=\"evals\")\n",
        "                                            plt.legend()\n",
        "                                            plt.title(f\"{vm.name}, {gen}, {tuple(model_size)}, {num_cands}, {num_voters}, {probmodel}, {learning_rate}, {training_batch_size}, {max_num_iterations}, {labeling}, {tuple(agent_infos)}, {manip_weight}\")\n",
        "\n",
        "                                            plt.savefig(f'{run_dir}/losses/{vm.name}_{gen}_{num_cands}_{num_voters}_{probmodel}_{learning_rate}_{tuple(model_size)}_{max_num_iterations}_{labeling}_{tuple(agent_infos)}_{manip_weight}_{CURRENT_TIME_STR}.png')\n",
        "\n",
        "                                            plt.savefig(f'losses/losses_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/{vm.name}_{gen}_{num_cands}_{num_voters}_{probmodel}_{learning_rate}_{tuple(model_size)}_{max_num_iterations}_{labeling}_{tuple(agent_infos)}_{manip_weight}_{CURRENT_TIME_STR}.png')\n",
        "\n",
        "                                            plt.close()\n",
        "\n",
        "                                            clear_output(wait=True)\n",
        "\n",
        "                                    with open(f'{run_dir}/models/{vm.name}_{gen}_{num_cands}_{num_voters}_{probmodel}_{tuple(agent_infos)}_{manip_weight}_{CURRENT_TIME_STR}.pickle', 'wb') as handle:\n",
        "                                        pickle.dump(results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)                        \n",
        "                                    with open(f'models/models_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/{vm.name}_{gen}_{num_cands}_{num_voters}_{probmodel}_{tuple(agent_infos)}_{manip_weight}_{CURRENT_TIME_STR}.pickle', 'wb') as handle:\n",
        "                                        pickle.dump(results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Evaluate models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBkb2DLmkbGT"
      },
      "outputs": [],
      "source": [
        "def evaluate_models(\n",
        "        models_dict, \n",
        "        num_cands, \n",
        "        num_voters, \n",
        "        probmodel, \n",
        "        all_utility_profs, \n",
        "        min_num_trials, \n",
        "        max_num_trials, \n",
        "        decision_rule, \n",
        "        metric_op):\n",
        "    # evaluate all models in models_dict using all_utility_profs\n",
        "    # the number of profiles used for evaluation is determined by \n",
        "    # repeated application of the percentile bootstrap method \n",
        "    # for determining confidence intervals (see https://pref-voting.readthedocs.io/en/latest/analysis_overview.html#pref_voting.analysis.bootstrap_cia)\n",
        "\n",
        "    vm_dict = {\n",
        "        vm.name : vm for vm in voting_methods_list\n",
        "    }\n",
        "\n",
        "    metrics_dict = {}\n",
        "\n",
        "    for key in tqdm(models_dict.keys()):\n",
        "        \n",
        "        if num_cands == key[3] and num_voters == key[4] and probmodel == key[5]:\n",
        "            vm_name, _, _, num_cands, num_voters,probmodel, learning_rate, _, _, _, agent_infos, manip_weight = key\n",
        "\n",
        "            agent = models_dict[key][0]\n",
        "            agent = agent.eval()\n",
        "\n",
        "            vm = vm_dict[vm_name]\n",
        "\n",
        "            generate_samples = partial(\n",
        "                evaluation_function, \n",
        "                agent=agent, \n",
        "                vm=vm, \n",
        "                num_cands=num_cands, \n",
        "                num_voters=num_voters,\n",
        "                manip_weight=manip_weight, \n",
        "                all_utility_profs=all_utility_profs, \n",
        "                decision_rule=decision_rule, \n",
        "                metric_op=metric_op,\n",
        "                agent_infos=agent_infos\n",
        "            )\n",
        "            means,est_std_errors,variances, num_trials = means_with_estimated_standard_error(\n",
        "                generate_samples,\n",
        "                max_est_std_error,             \n",
        "                initial_trials=4096, \n",
        "                step_trials=4096, \n",
        "                min_num_trials=min_num_trials,\n",
        "                max_num_trials=max_num_trials,\n",
        "                verbose=False,)\n",
        "\n",
        "            metrics_dict[key] = {\n",
        "                \"means\":means, \n",
        "                \"est_std_errors\": est_std_errors, \n",
        "                \"variances\": variances, \n",
        "                \"num_trials\": num_trials\n",
        "                }\n",
        "\n",
        "    return metrics_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the stored evaluation profiles and trained models for evaluation\n",
        "\n",
        "if EVALUATE_MODELS:\n",
        "    eval_batch_size = 4096\n",
        "    decision_rule = 'argmax'\n",
        "    metric_op = 'normalized_subtract'\n",
        "    min_num_trials = 4095\n",
        "\n",
        "    for labeling in labeling_list:\n",
        "        for manip_weight in manip_weight_list:\n",
        "            for agent_infos in agent_infos_list:\n",
        "                for probmodel in prob_models_list:\n",
        "                    for gen in generation_list: \n",
        "                        for num_cands in num_cands_list: \n",
        "                            for num_voters in num_voters_list: \n",
        "                                # read in the utility profiles\n",
        "                                all_utility_profiles = pickle.load(\n",
        "                                    open(\n",
        "                                        os.path.join(\n",
        "                                            evaluation_utility_base_dir,\n",
        "                                            f\"evaluation_util_profs_{gen}_{num_cands}_{num_voters}_{probmodel}.pkl\",\n",
        "                                            ), \"rb\",\n",
        "                                            ),\n",
        "                                            )\n",
        "\n",
        "                                # read in the models\n",
        "                                models_dict = {}\n",
        "                                for file in glob.glob(f\"models/models_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/*_{gen}_{num_cands}_{num_voters}_{probmodel}_{tuple(agent_infos)}_{manip_weight}_*.pickle\"):\n",
        "                                    with open(file, 'rb') as handle:\n",
        "                                        m_dict = pickle.load(handle)\n",
        "                                        new_m_dict = {}\n",
        "                                        for key in m_dict.keys():\n",
        "                                            if key[1] == gen and key[3] == num_cands and key[4] == num_voters and key[9] == labeling: \n",
        "                                                new_m_dict[key] = (m_dict[key][0].to(DEVICE), m_dict[key][1])\n",
        "                                        models_dict.update(new_m_dict)\n",
        "                                print(\"Evaluating\")\n",
        "                                print(f\"labeling: {labeling}\")\n",
        "                                print(f\"manip_weight: {manip_weight}\")\n",
        "                                print(f\"agent_infos: {agent_infos}\")\n",
        "                                print(f\"probmodel: {probmodel}\")\n",
        "                                print(f\"gen: {gen}\")\n",
        "                                print(f\"num_cands: {num_cands}\")\n",
        "                                print(f\"num_voters: {num_voters}\")\n",
        "                                print(f\"decision_rule: {decision_rule}\")\n",
        "                                print(f\"metric_op: {metric_op}\")\n",
        "                                print(f\"evaluation_batch_size: {evaluation_batch_size}\")\n",
        "                                print(f\"max_est_std_error: {max_est_std_error}\")\n",
        "                                print(\"Number of models: \", len(models_dict.keys()))\n",
        "                                metrics_dict = evaluate_models(\n",
        "                                    models_dict, \n",
        "                                    num_cands, \n",
        "                                    num_voters, \n",
        "                                    probmodel,\n",
        "                                    all_utility_profiles,\n",
        "                                    min_num_trials,\n",
        "                                    evaluation_batch_size * max_num_evaluation_rounds - 1,\n",
        "                                    decision_rule, \n",
        "                                    metric_op)\n",
        "\n",
        "\n",
        "                                if run_dir is not None:\n",
        "                                    with open(f'{run_dir}/evaluation/{gen}_{num_cands}_{num_voters}_{probmodel}_{tuple(agent_infos)}_{manip_weight}_{decision_rule}_{metric_op}_{evaluation_batch_size}_{max_est_std_error}_{CURRENT_TIME_STR}.pickle', 'wb') as handle:\n",
        "                                        pickle.dump(metrics_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "                                with open(f'evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/{gen}_{num_cands}_{num_voters}_{probmodel}_{tuple(agent_infos)}_{manip_weight}_{decision_rule}_{metric_op}_{evaluation_batch_size}_{max_est_std_error}_{CURRENT_TIME_STR}.pickle', 'wb') as handle:\n",
        "                                    pickle.dump(metrics_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fix an ordering of model sizes and voting methods for \n",
        "# the graphs\n",
        "\n",
        "graphs_dir = 'graphs'\n",
        "\n",
        "model_sizes_order = [\n",
        "    \"(4,)\", \n",
        "    \"(8,)\", \n",
        "    \"(16,)\", \n",
        "    \"(32,)\", \n",
        "    \"(64,)\", \n",
        "    \"(128,)\", \n",
        "    \"(256,)\", \n",
        "    \"(512,)\",\n",
        "    \"(4, 4)\", \n",
        "    \"(8, 8)\", \n",
        "    \"(16, 8)\", \n",
        "    \"(16, 16)\", \n",
        "    \"(32, 32)\", \n",
        "    \"(64, 32)\", \n",
        "    \"(64, 64)\", \n",
        "    \"(128, 128)\", \n",
        "    \"(256, 128)\", \n",
        "    \"(256, 256)\",\n",
        "    \"(8, 8, 8)\", \n",
        "    \"(32, 16, 8)\", \n",
        "    \"(32, 32, 32)\", \n",
        "    \"(64, 64, 64)\", \n",
        "    \"(128, 64, 32)\", \n",
        "    \"(128, 128, 128)\", \n",
        "    \"(256, 256, 256)\", \n",
        "    \"(512, 256, 128)\"\n",
        "]\n",
        "\n",
        "voting_methods_order = [\n",
        "    'Plurality', \n",
        "    'Borda', \n",
        "    'Instant Runoff', \n",
        "    'Instant Runoff PUT',\n",
        "    'Minimax', \n",
        "    'Strict Nanson', \n",
        "    'Split Cycle', \n",
        "    'Stable Voting']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Color palettes\n",
        "\n",
        "colors_for_bars = {\n",
        "        \"blue\": \"blue\",\n",
        "        \"green\": \"green\",\n",
        "        \"orange\": \"orange\",\n",
        "        \"red\": \"red\",\n",
        "        \"yellow\": \"yellow\",\n",
        "        \"purple\": \"purple\",\n",
        "        \"brown\": \"brown\",\n",
        "        \"lightbrown\": \"#D2B48C\",\n",
        "        \"darkbrown\": \"#8B4513\"\n",
        "    }\n",
        "\n",
        "colorblind = {\n",
        "        \"blue\": \"#0400fb\", #\"#4171ff\",\n",
        "        \"green\": \"#20ebff\",\n",
        "        \"orange\": \"#ffc200\",\n",
        "        \"red\": \"#c61c00\",\n",
        "    }\n",
        "\n",
        "pastel={\n",
        "        \"blue\": \"#a1c9f4\",\n",
        "        \"green\": \"#8de0a1\",\n",
        "        \"orange\": \"#ffb482\",\n",
        "        \"red\": \"#f768a1\",\n",
        "        \"purple\": \"#d0bbff\",\n",
        "        \"brown\": \"#debb9b\"\n",
        "    }\n",
        "\n",
        "dark = {\n",
        "        \"blue\": \"#001f3f\",\n",
        "        \"green\": \"#3f9b0b\",\n",
        "        \"orange\": \"#ff851b\",\n",
        "        \"red\": \"#ff4136\",\n",
        "        \"purple\": \"#a23582\",\n",
        "        \"brown\": \"#8B4513\",\n",
        "    }\n",
        "\n",
        "muted = {\n",
        "        \"blue\": \"#4878d0\",\n",
        "        \"green\": \"#6acc64\",\n",
        "        \"orange\": \"#d65f5f\",\n",
        "        \"red\": \"#d5bb67\",\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1. Performance by model size (Figure 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read in the evaluation data\n",
        "\n",
        "if VISUALIZE_RESULTS: \n",
        "    gen = 1\n",
        "    num_cands = 6\n",
        "    num_voters1 = 10\n",
        "    num_voters2 = 11\n",
        "    manip_weight = 1\n",
        "\n",
        "    agent_infos1 = 'plurality_ranking'\n",
        "    agent_infos1_str = 'plurality ranking'  \n",
        "    agent_infos2 = 'majority'\n",
        "    agent_infos2_str = 'majority matrix'\n",
        "\n",
        "    num_voters1_colors = {\n",
        "        agent_infos1: colorblind[\"blue\"], \n",
        "        agent_infos2: colorblind[\"red\"]\n",
        "    }\n",
        "    num_voters2_colors = {\n",
        "        agent_infos1: colorblind[\"green\"], \n",
        "        agent_infos2: colorblind[\"orange\"]\n",
        "    }\n",
        "\n",
        "    labeling = 'optimize'\n",
        "    decision_rule = 'argmax'\n",
        "    metric_op = 'normalized_subtract'\n",
        "    probmodel = 'uniform'\n",
        "    evaluation_batch_size = 4096\n",
        "    max_est_std_error = 0.0005\n",
        "\n",
        "    filename = glob.glob(f\"evaluation/evaluation_('{agent_infos1}',)_{probmodel}_{labeling}_{manip_weight}/{gen}_{num_cands}_{num_voters1}_{probmodel}_('{agent_infos1}',)_{manip_weight}_{decision_rule}_{metric_op}_{evaluation_batch_size}_{max_est_std_error}_*.pickle\")[0]\n",
        "    print(f\"loading {agent_infos1} evaluation data:\\n {filename.split('/')[-1]}\")\n",
        "    evaluation_agent_infos1_data1 = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "\n",
        "    filename = glob.glob(f\"evaluation/evaluation_('{agent_infos1}',)_{probmodel}_{labeling}_{manip_weight}/{gen}_{num_cands}_{num_voters2}_{probmodel}_('{agent_infos1}',)_{manip_weight}_{decision_rule}_{metric_op}_{evaluation_batch_size}_{max_est_std_error}_*.pickle\")[0]\n",
        "    print(f\"loading {agent_infos1} evaluation data:\\n {filename.split('/')[-1]}\")\n",
        "    evaluation_agent_infos1_data2 = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "    # combine the evaluation data into one dictionary\n",
        "    evaluation_data_agent_infos1 = evaluation_agent_infos1_data1.copy()\n",
        "    evaluation_data_agent_infos1.update(evaluation_agent_infos1_data2)\n",
        "    print(f\"Number of keys: {len(evaluation_data_agent_infos1.keys())}\")\n",
        "\n",
        "    filename = glob.glob(f\"evaluation/evaluation_('{agent_infos2}',)_{probmodel}_{labeling}_{manip_weight}/{gen}_{num_cands}_{num_voters1}_{probmodel}_('{agent_infos2}',)_{manip_weight}_{decision_rule}_{metric_op}_{evaluation_batch_size}_{max_est_std_error}_*.pickle\")[0]\n",
        "    print(f\"loading {agent_infos2} evaluation data:\\n {filename.split('/')[-1]}\")\n",
        "    evaluation_agent_infos2_data1 = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "    filename = glob.glob(f\"evaluation/evaluation_('{agent_infos2}',)_{probmodel}_{labeling}_{manip_weight}/{gen}_{num_cands}_{num_voters2}_{probmodel}_('{agent_infos2}',)_{manip_weight}_{decision_rule}_{metric_op}_{evaluation_batch_size}_{max_est_std_error}_*.pickle\")[0]\n",
        "    print(f\"loading {agent_infos2} evaluation data:\\n {filename.split('/')[-1]}\")\n",
        "    evaluation_agent_infos2_data2 = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "    # combine the evaluation data into one dictionary\n",
        "    evaluation_data_agent_infos2 = evaluation_agent_infos2_data1.copy()\n",
        "    evaluation_data_agent_infos2.update(evaluation_agent_infos2_data2)\n",
        "    print(f\"Number of keys: {len(evaluation_data_agent_infos2.keys())}\")\n",
        "\n",
        "    # for key in evaluation_data_agent_infos2.keys():\n",
        "    #     print(key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS:\n",
        "    all_num_trials = []\n",
        "    for key in evaluation_data_agent_infos1.keys():\n",
        "        all_num_trials.append(evaluation_data_agent_infos1[key]['num_trials'])\n",
        "        \n",
        "    for key in evaluation_data_agent_infos2.keys():\n",
        "        all_num_trials.append(evaluation_data_agent_infos2[key]['num_trials'])\n",
        "\n",
        "    print(\"min number of evaluation trials\", min(all_num_trials))\n",
        "    print(\"max number of evaluation trials \", max(all_num_trials))\n",
        "    print(\"mean number of evaluation trials \", np.mean(all_num_trials))\n",
        "    print(\"standard deviation of the number of evaluation trials \", np.std(all_num_trials))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataframe(evaluation_dict): \n",
        "    # create a dataframe from the dictionary of evaluations of the models\n",
        "    \n",
        "    data_for_df = {\n",
        "        \"vm\": list(),\n",
        "        \"model_size\": list(),\n",
        "        \"num_cands\": list(),\n",
        "        \"num_voters\": list(),\n",
        "        \"probmodel\": list(),\n",
        "\n",
        "        \"mean_profitability\": list(),\n",
        "        \"est_std_error\": list(),\n",
        "        \"variance\": list(),\n",
        "        \"num_trials\": list(),\n",
        "\n",
        "        \"learning_rate\": list(),\n",
        "        \"labeling\": list(),\n",
        "        \"batch_size\": list(),\n",
        "        \"num_iterations\": list(),\n",
        "        \"generation\": list(),\n",
        "        \"agent_infos\": list(),\n",
        "        \"manip_weight\": list(),\n",
        "    }\n",
        "    for key in evaluation_dict.keys():\n",
        "        try:\n",
        "            vm, generation, model_size, num_cands, num_voters, probmodel, learning_rate, batch_size, num_iterations, labeling, agent_infos, manip_weight = key\n",
        "            data = evaluation_dict[key]\n",
        "            data_for_df[\"vm\"].append(vm)\n",
        "            data_for_df[\"model_size\"].append(model_size)\n",
        "            data_for_df[\"num_cands\"].append(num_cands)\n",
        "            data_for_df[\"num_voters\"].append(num_voters)\n",
        "            data_for_df[\"probmodel\"].append(probmodel)\n",
        "\n",
        "            data_for_df[\"mean_profitability\"].append(data['means'][0])\n",
        "            data_for_df[\"est_std_error\"].append(data['est_std_errors'][0])\n",
        "            data_for_df[\"variance\"].append(data['variances'][0])\n",
        "            data_for_df[\"num_trials\"].append(data['num_trials'])\n",
        "\n",
        "            data_for_df[\"learning_rate\"].append(learning_rate)\n",
        "            data_for_df[\"labeling\"].append(labeling) \n",
        "            data_for_df[\"batch_size\"].append(batch_size)\n",
        "            data_for_df[\"num_iterations\"].append(num_iterations)\n",
        "            data_for_df[\"generation\"].append(generation)\n",
        "            data_for_df[\"agent_infos\"].append(agent_infos)\n",
        "            data_for_df[\"manip_weight\"].append(manip_weight)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            print(f\"Skipping key {key}\")\n",
        "    print(f\"Finished creating dataframe for generation {generation} and agent_infos {agent_infos}\")\n",
        "    return pd.DataFrame(data_for_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def model_size_to_tuple(model_size):\n",
        "    # helper function for displaying the model sizes in the correct order on the x-axis\n",
        "\n",
        "    tuple_elements = model_size.strip().replace('(', '').replace(')', '').split(',')\n",
        "    tuple_elements = [element.strip() for element in tuple_elements]\n",
        "    if '' in tuple_elements:\n",
        "        return (0, 0)\n",
        "    num_layers = len(tuple_elements)\n",
        "    total_size = sum(map(int, tuple_elements))\n",
        "    return (num_layers, total_size)\n",
        "\n",
        "def generate_avg_df(df, num_cands, num_voters,  agent_infos, manip_weight, labeling, probmodel):\n",
        "    # generate the dataframe to for the graphs\n",
        "    # Note: In the current version, this function is always called for a fixed number of candidates, number of voters, and a generation, so there is a single mean_profitability for each model_size\n",
        "    df = df[((df['agent_infos'] == agent_infos) & (df['manip_weight'] == manip_weight)) & ((df['labeling'] == labeling) & (df['probmodel'] == probmodel))]\n",
        "\n",
        "    df['model_size'] = df['model_size'].astype(str)\n",
        "\n",
        "    df_avg_mean_profitability = df.groupby(['vm', 'model_size', 'labeling', 'est_std_error', 'generation'])['mean_profitability'].mean().reset_index()\n",
        "    df_avg_mean_profitability['num_cands'] = num_cands\n",
        "    df_avg_mean_profitability['num_voters'] = num_voters\n",
        "    df_avg_mean_profitability['model_size_tuple'] = df_avg_mean_profitability['model_size'].apply(model_size_to_tuple)\n",
        "\n",
        "    df_avg_mean_profitability = df_avg_mean_profitability.sort_values(by='model_size_tuple')\n",
        "\n",
        "    df_avg_mean_profitability.rename(columns={'mean_profitability': 'average_mean_profitability'}, inplace=True)\n",
        "\n",
        "    return df_avg_mean_profitability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if VISUALIZE_RESULTS:\n",
        "    # create the appropriate dataframes\n",
        "\n",
        "    df_agent_infos1 = create_dataframe(evaluation_data_agent_infos1)\n",
        "    df_agent_infos2 = create_dataframe(evaluation_data_agent_infos2)\n",
        "    dfs = {\n",
        "        (agent_infos1,): df_agent_infos1,\n",
        "        (agent_infos2,): df_agent_infos2,\n",
        "    }\n",
        "\n",
        "    agent_infos_list = [[agent_infos1], [agent_infos2] ]\n",
        "\n",
        "    for agent_infos in agent_infos_list:\n",
        "        print(agent_infos)\n",
        "                    \n",
        "        df = dfs[tuple(agent_infos)]\n",
        "        df_avg_1 = generate_avg_df(df[(df[\"num_cands\"] == num_cands) & (df[\"num_voters\"]==num_voters1)], num_cands, num_voters1,  tuple(agent_infos), manip_weight, labeling, probmodel)\n",
        "                    \n",
        "        df_avg_2 = generate_avg_df(df[(df[\"num_cands\"] == num_cands) & (df[\"num_voters\"]==num_voters2)], num_cands, num_voters2,  tuple(agent_infos), manip_weight, labeling, probmodel)\n",
        "                    \n",
        "        df_avg = pd.concat([df_avg_1, df_avg_2])\n",
        "        print(\"generated \", f\"evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/df_avg_{num_cands}_{agent_infos}_{manip_weight}_{gen}.csv\")\n",
        "                    \n",
        "        df_avg.to_csv(f\"evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/df_avg_{num_cands}_{agent_infos}_{manip_weight}_{gen}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_graphs_with_errorbars(df, ideal_manipulators_df, num_cands, manip_weight, legend_label_mapping, color_dict, filename, labeling='satisfice', probmodel='uniform', gen=1):\n",
        "    # function to generate the graphs for Figures 1 and 2\n",
        "    # multiply all est_std_errors by 2 to get 95% confidence intervals\n",
        "    df['est_std_error'] = df['est_std_error'] * 2\n",
        "    ideal_manipulators_df['est_std_error'] = ideal_manipulators_df['est_std_error'] * 2\n",
        "\n",
        "    fontsize=16\n",
        "    df = df[df['labeling'] == labeling]\n",
        "    y_ticks = list(np.arange(-0.08, 0.10, 0.01))\n",
        "    df = df[df['model_size'].isin(model_sizes_order)]\n",
        "    df['model_size'] = pd.Categorical(df['model_size'], categories=model_sizes_order, ordered=True)\n",
        "    df.sort_values('model_size', inplace=True)\n",
        "    \n",
        "    fig = plt.figure(figsize=(24, 33))\n",
        "    gs = fig.add_gridspec(len(voting_methods_order) // 2, 2, hspace=0.1, wspace=0.05)\n",
        "    \n",
        "    voters_order = list(legend_label_mapping.keys())\n",
        "    bar_offset = [-0.38, -0.125, 0.125, 0.38]\n",
        "    \n",
        "    # Reordered criteria for ideal manipulators\n",
        "    ideal_criteria = [(num_voters1, 'max'), (num_voters2, 'max')]\n",
        "    \n",
        "    # New colors for ideal manipulator bars\n",
        "    ideal_colors = ['#9370DB', '#4B0082', '#808080', '#404040']\n",
        "    ideal_legend_labels = [f'{num_voters1}', f'{num_voters2}']\n",
        "    \n",
        "    # Adjusted vertical line positions\n",
        "    vline_positions = [model_sizes_order.index(\"(512,)\"), model_sizes_order.index(\"(256, 256)\"), model_sizes_order.index(\"(512, 256, 128)\")]\n",
        "    \n",
        "    # Ideal voter bar offsets adjusted to place x-tick in the center\n",
        "    ideal_voter_bar_offsets = [-0.125, 0.125]\n",
        "    \n",
        "    for i, voting_method in enumerate(voting_methods_order):\n",
        "        ax = fig.add_subplot(gs[i // 2, i % 2])\n",
        "        df_filtered_vm = df[df['vm'] == voting_method]\n",
        "        \n",
        "        # Existing bars\n",
        "        for idx, voter_info in enumerate(voters_order):\n",
        "            current_data = df_filtered_vm[df_filtered_vm['voters_info'] == voter_info]\n",
        "            ax.bar(x=np.arange(len(model_sizes_order)) + bar_offset[idx], height=current_data['average_mean_profitability'],\n",
        "                   yerr=current_data['est_std_error'], width=0.25, align='center', label=legend_label_mapping[voter_info] if i == 0 else \"\", color=color_dict[voter_info])\n",
        "        \n",
        "        # Bars for ideal manipulators using the reordered criteria and updated colors\n",
        "        for idx, (num_voters, decision_rule) in enumerate(ideal_criteria):\n",
        "            ideal_data = ideal_manipulators_df[(ideal_manipulators_df['vm'] == voting_method) & (ideal_manipulators_df['num_voters'] == num_voters) & (ideal_manipulators_df['decision_rule'] == decision_rule)]\n",
        "            ax.bar(x=len(model_sizes_order) + ideal_voter_bar_offsets[idx], height=ideal_data['mean'].values[0], yerr=ideal_data['est_std_error'].values[0], width=0.25, align='center', color=ideal_colors[idx])\n",
        "        \n",
        "        # Drawing the vertical dashed lines\n",
        "        for pos in vline_positions:\n",
        "            ax.axvline(x=pos + 0.5, color='gray', linestyle='--', linewidth=1)\n",
        "        \n",
        "        # Other plotting settings remain the same\n",
        "        ax.set_title(f'{voting_method if voting_method != \"Strict Nanson\" else \"Nanson\"}', fontsize=18, weight='bold')\n",
        "        ax.tick_params(axis='x', rotation=90, labelsize=8 if i // 2 == (len(voting_methods_order) // 2 - 1) else 0)\n",
        "        ax.tick_params(axis='y', labelsize=10 if i % 2 == 0 else 0)\n",
        "        ax.set_yticks(y_ticks)\n",
        "        ax.set_ylim([-0.08, 0.09])\n",
        "        ax.set_yticklabels([round(y,2) for y in y_ticks] if i % 2 == 0 else [], fontsize=fontsize)\n",
        "        ax.set_xticks(list(np.arange(len(model_sizes_order))) + [len(model_sizes_order)])\n",
        "        ax.set_xticklabels(model_sizes_order + ['ideal manipulator'] if i // 2 == (len(voting_methods_order) // 2 - 1) else [], fontsize=fontsize)\n",
        "        ax.set_ylabel('average profitability of submitted ranking' if i % 2 == 0 else '', fontsize=18)\n",
        "        ax.set_xlabel('')\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.grid(True, which='both',    linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "        \n",
        "        if i == 0:\n",
        "            # Main legend\n",
        "            legend1 = ax.legend(loc='lower right', bbox_to_anchor=(0.725, 0), title=\"Voters, Manipulator Info\", title_fontsize = 18, fontsize=fontsize)\n",
        "            ax.add_artist(legend1)\n",
        "            \n",
        "            # Secondary legend for ideal manipulators with rectangles and labels\n",
        "            handles = [plt.Rectangle((0, 0), 1, 1, color=color, label=label) for color, label in zip(ideal_colors, ideal_legend_labels)]\n",
        "            ax.legend(handles=handles, loc='lower right', bbox_to_anchor=(1.0, 0), title=\"Ideal Manipulator\", title_fontsize = 18, fontsize=fontsize)\n",
        "    \n",
        "    plt.subplots_adjust(hspace=0.0, wspace=0.0, top=.83, bottom=0.15)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate the graphs\n",
        "\n",
        "if VISUALIZE_RESULTS:\n",
        "    filename = f\"{graphs_dir}/graphs_{gen}_{num_cands}_{num_voters1, num_voters2}_{agent_infos1}_vs_{agent_infos2}_with_ideal.pdf\"\n",
        "\n",
        "    agent_infos = [agent_infos1]\n",
        "    df_agent_infos1 = pd.read_csv(f\"evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/df_avg_{num_cands}_{agent_infos}_{manip_weight}_{gen}.csv\")\n",
        "\n",
        "    agent_infos = [agent_infos2]\n",
        "    df_agent_infos2 = pd.read_csv(f\"evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/df_avg_{num_cands}_{agent_infos}_{manip_weight}_{gen}.csv\")\n",
        "\n",
        "    df_agent_infos1['agent_infos'] = agent_infos1\n",
        "    df_agent_infos2['agent_infos'] = agent_infos2\n",
        "\n",
        "    # Combining both dataframes again\n",
        "    combined_df_new = pd.concat([df_agent_infos1, df_agent_infos2], ignore_index=True)\n",
        "\n",
        "    # Creating the 'voters_info' column which combines the number of voters and agent_infos\n",
        "    combined_df_new['voters_info'] = combined_df_new['num_voters'].astype(str) + ', ' + combined_df_new['agent_infos']\n",
        "\n",
        "    color_dict = {\n",
        "            f\"{num_voters1}, {agent_infos1}\": num_voters1_colors[agent_infos1],\n",
        "            f\"{num_voters2}, {agent_infos1}\": num_voters2_colors[agent_infos1],\n",
        "            f\"{num_voters1}, {agent_infos2}\": num_voters1_colors[agent_infos2],\n",
        "            f\"{num_voters2}, {agent_infos2}\": num_voters2_colors[agent_infos2],\n",
        "        }\n",
        "\n",
        "    \n",
        "    legend_label_mapping = {\n",
        "            f\"{num_voters1}, {agent_infos1}\": f\"{num_voters1}, {agent_infos1_str}\",\n",
        "            f\"{num_voters2}, {agent_infos1}\": f\"{num_voters2}, {agent_infos1_str}\",\n",
        "            f\"{num_voters1}, {agent_infos2}\": f\"{num_voters1}, {agent_infos2_str}\",\n",
        "            f\"{num_voters2}, {agent_infos2}\": f\"{num_voters2}, {agent_infos2_str}\"\n",
        "        }\n",
        "\n",
        "    if num_voters1 < num_voters2: \n",
        "        ideal_manipulators_df = pd.read_csv(f'ideal_manipulator_data/ideal_voter_1_uniform_{num_cands}_({num_voters1}, {num_voters2}).csv')\n",
        "    else:\n",
        "        ideal_manipulators_df = pd.read_csv(f'ideal_manipulator_data/ideal_voter_1_uniform_{num_cands}_({num_voters2}, {num_voters1}).csv')\n",
        "\n",
        "\n",
        "    # Now calling the modified function with the provided arguments\n",
        "    generate_graphs_with_errorbars(combined_df_new, ideal_manipulators_df[ideal_manipulators_df[\"num_cands\"] == num_cands], num_cands, manip_weight, legend_label_mapping, color_dict, filename, labeling=labeling, probmodel=probmodel, gen=gen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.2. Graphs to compare manipulator information types for a fixed model size (Figure 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS: \n",
        "    gen = 1\n",
        "    all_num_cands = [3, 6]\n",
        "    all_num_voters = [10, 11]\n",
        "    manip_weight = 1\n",
        "\n",
        "    all_agent_infos = [\n",
        "        'plurality_scores',\n",
        "        'plurality_ranking',\n",
        "        'majority',\n",
        "        'margin',\n",
        "        'sincere_winners',\n",
        "        'qual_margin']\n",
        "\n",
        "    model_sizes = [\n",
        "        (256, 128)\n",
        "        # (512,), \n",
        "        # (256, 256), \n",
        "        # (512, 256, 128)\n",
        "    ]\n",
        "    labeling = 'optimize'\n",
        "    decision_rule = 'argmax'\n",
        "    metric_op = 'normalized_subtract'\n",
        "    probmodel = 'uniform'\n",
        "    evaluation_batch_size = 4096\n",
        "    max_est_std_error = 0.0005\n",
        "    eval_dict = {}\n",
        "    for num_cands in all_num_cands:\n",
        "        for num_voters in all_num_voters: \n",
        "            for agent_info in all_agent_infos:\n",
        "                # get filename from regular expression\n",
        "                filenames = glob.glob(f\"evaluation/evaluation_('{agent_info}',)_{probmodel}_{labeling}_{manip_weight}/{gen}_{num_cands}_{num_voters}_{probmodel}_('{agent_info}',)_{manip_weight}_{decision_rule}_{metric_op}_{evaluation_batch_size}_{max_est_std_error}_*.pickle\")\n",
        "                if len(filenames) == 0:\n",
        "                    print(f\"WARNING: no file found for {num_cands} candidates, {num_voters} voters, and {agent_info}\\n\")\n",
        "                    continue\n",
        "                elif len(filenames) > 1:\n",
        "                    print(\"WARNING: more than one file found!\")\n",
        "                    continue\n",
        "                else: \n",
        "                    filename = filenames[0]\n",
        "                    print(f\"loading {agent_info} evaluation data:\\n {filename.split('/')[-1]}\")\n",
        "                    _eval_dict  = pickle.load(open(filename, 'rb'))\n",
        "                    eval_dict.update(_eval_dict)\n",
        "\n",
        "\n",
        "    print(len(eval_dict.keys()))\n",
        "    df = create_dataframe(eval_dict)\n",
        "    df.to_csv(\"./evaluation/all_agent_infos_diff_cands.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS:\n",
        "\n",
        "    file_path_all_agents = './evaluation/all_agent_infos_diff_cands.csv'\n",
        "    df_all_agents = pd.read_csv(file_path_all_agents)\n",
        "\n",
        "    # replace \"Strict Nanson\" with \"Nanson\" in the dataframe\n",
        "    df_all_agents['vm'] = df_all_agents['vm'].replace('Strict Nanson', 'Nanson')\n",
        "\n",
        "    filtered_df_all_agents = df_all_agents[(df_all_agents['model_size'].isin([f'{model_size}' for model_size in model_sizes])) & (df_all_agents['num_cands'].isin(all_num_cands))]\n",
        "\n",
        "    filtered_df_voters_all_agents = filtered_df_all_agents[filtered_df_all_agents['num_voters'].isin(all_num_voters)]\n",
        "\n",
        "    grouped_df_all_agents = filtered_df_voters_all_agents.groupby(['vm', 'agent_infos']).mean_profitability.mean().unstack()\n",
        "\n",
        "    specified_order = [ \n",
        "        \"('plurality_ranking',)\", \n",
        "        \"('plurality_scores',)\", \n",
        "        \"('majority',)\", \n",
        "        \"('qual_margin',)\", \n",
        "        \"('margin',)\", \n",
        "        \"('sincere_winners',)\"]\n",
        "    grouped_df_ordered = grouped_df_all_agents[specified_order]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS:\n",
        "    # Load the two CSV files\n",
        "    file_path_1 = './ideal_manipulator_data/ideal_voter_1_uniform_3_(10, 11).csv'\n",
        "    file_path_2 = './ideal_manipulator_data/ideal_voter_1_uniform_6_(10, 11).csv'\n",
        "    df1 = pd.read_csv(file_path_1)\n",
        "    df2 = pd.read_csv(file_path_2)\n",
        "    \n",
        "    fontsize = 16\n",
        "\n",
        "    # Combining the two dataframes\n",
        "    combined_df = pd.concat([df1, df2])\n",
        "    combined_df['vm'] = combined_df['vm'].replace('Strict Nanson', 'Nanson')\n",
        "\n",
        "\n",
        "    # Filtering for 'max' decision_rule\n",
        "    filtered_max_decision = combined_df[combined_df['decision_rule'] == 'max']\n",
        "\n",
        "    # Grouping by voting method and calculating the average of 'mean' for 3 and 6 candidates and 10 and 11 voters\n",
        "    average_means = filtered_max_decision.groupby('vm')['mean'].mean()\n",
        "\n",
        "    # Adding the 'ideal manipulator' data to the previous bar graph data\n",
        "    updated_grouped_df = grouped_df_ordered.assign(Ideal_Manipulator=average_means)\n",
        "    updated_grouped_df = updated_grouped_df.fillna(0)  # Fill NaNs with 0 for plotting\n",
        "\n",
        "    specified_order = [ \n",
        "        \"('plurality_ranking',)\",\n",
        "        \"('plurality_scores',)\", \n",
        "        \"('majority',)\", \n",
        "        \"('qual_margin',)\", \n",
        "        \"('margin',)\", \n",
        "        \"('sincere_winners',)\", \n",
        "        \"Ideal_Manipulator\"]\n",
        "    \n",
        "    updated_grouped_df = updated_grouped_df[specified_order]\n",
        "\n",
        "    colors=sns.color_palette(\"colorblind\", len(updated_grouped_df.columns)-1) + [\"black\"]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    updated_grouped_df.plot(kind='bar', \n",
        "                            ax=ax,\n",
        "                            color=colors)\n",
        "\n",
        "\n",
        "    ax.set_xlabel(\"\")\n",
        "    ax.set_ylabel(\"Average Profitability\", fontsize=fontsize)\n",
        "    ax.set_title(\"\", fontsize=fontsize)\n",
        "\n",
        "    # Updating the legend with readable labels\n",
        "    readable_legend_labels = {\n",
        "        \"('plurality_ranking',)\": \"Plurality Ranking\",\n",
        "        \"('plurality_scores',)\": \"Plurality Scores\",\n",
        "        \"('majority',)\": \"Majority\",\n",
        "        \"('qual_margin',)\": \"Qualitative Margin\",\n",
        "        \"('margin',)\": \"Margin\",\n",
        "        \"('sincere_winners',)\": \"Sincere Winners\",\n",
        "        \"Ideal_Manipulator\": \"Ideal Manipulator\"\n",
        "    }\n",
        "\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    handles = handles[1:] + handles[0:1]\n",
        "    labels = labels[1:] + labels[0:1]\n",
        "\n",
        "    new_labels = [readable_legend_labels.get(label, label) for label in labels]\n",
        "\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "    # Divide handles and labels into three groups\n",
        "    handles_col1 = handles[:2]  \n",
        "    labels_col1 = labels[:2]\n",
        "    new_labels_col1 = [readable_legend_labels.get(label, label) for label in labels_col1]  \n",
        "    \n",
        "    handles_col2 = handles[2:5]  \n",
        "    labels_col2 = labels[2:5]\n",
        "    new_labels_col2 = [readable_legend_labels.get(label, label) for label in labels_col2]  \n",
        "\n",
        "    handles_col3 = handles[5:7]  \n",
        "    labels_col3 = labels[5:7]\n",
        "    new_labels_col3 = [readable_legend_labels.get(label, label) for label in labels_col3]  \n",
        "\n",
        "    leg_col1 = ax.legend(handles_col1, new_labels_col1, loc='upper left', bbox_to_anchor=(0.1, 1), ncol=1, frameon=False, fontsize=fontsize)\n",
        "\n",
        "    leg_col2 = ax.legend(handles_col2, new_labels_col2, loc='upper left', bbox_to_anchor=(0.375, 1), ncol=1, frameon=False, fontsize=fontsize)\n",
        "\n",
        "    leg_col3 = ax.legend(handles_col3, new_labels_col3, loc='upper left', bbox_to_anchor=(0.65, 1), ncol=1, frameon=False, fontsize=fontsize)\n",
        "\n",
        "    # Add the legends back to the Axes\n",
        "    ax.add_artist(leg_col1)\n",
        "    ax.add_artist(leg_col2)\n",
        "\n",
        "\n",
        "    plt.yticks(fontsize=fontsize)\n",
        "\n",
        "    plt.xticks(rotation=45, fontsize=fontsize)\n",
        "    sns.despine(right=True, top=True)\n",
        "    # add grid lines\n",
        "    ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{graphs_dir}/voting_methods_different_agent_infos.pdf\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ratio of mean profitability to the mean of the best possible profitability for a fixed model size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS:\n",
        "    # Copy the original dataframe\n",
        "    df_ratio = updated_grouped_df.copy()\n",
        "\n",
        "    # Applying the ratio transformation to each column except 'vm' and 'Ideal_Manipulator'\n",
        "    for column in df_ratio.columns:\n",
        "        if column != 'vm' and column != 'Ideal_Manipulator':\n",
        "            df_ratio[column] = updated_grouped_df[column] / updated_grouped_df['Ideal_Manipulator']\n",
        "    df_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS:\n",
        "    # Reordering the columns in the dataframe to match the specified order\n",
        "    specified_order = [ \n",
        "        \"('plurality_ranking',)\", \n",
        "        \"('plurality_scores',)\", \n",
        "        \"('majority',)\", \n",
        "        \"('qual_margin',)\", \n",
        "        \"('margin',)\", \n",
        "        \"('sincere_winners',)\"\n",
        "        ]\n",
        "    ratio_df_ordered = df_ratio[specified_order]\n",
        "\n",
        "    colors=sns.color_palette(\"colorblind\", len(updated_grouped_df.columns)-1) + [\"black\"]\n",
        "\n",
        "    plt.style.use('seaborn-colorblind')\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    ratio_df_ordered.plot(kind='bar', ax=ax, color=colors)\n",
        "\n",
        "    ax.set_xlabel(\"\")\n",
        "    ax.set_ylabel(\"Ratio\",fontsize=fontsize)\n",
        "    ax.set_title(\"\",fontsize=fontsize)\n",
        "\n",
        "    readable_legend_labels = {\n",
        "        '(plurality_ranking,)': \"Plurality Ranking\",\n",
        "        '(plurality_scores,)': \"Plurality Scores\",\n",
        "        '(majority,)': \"Majority\",\n",
        "        '(qual_margin,)': \"Qual Margin\",\n",
        "        '(margin,)': \"Margin\",\n",
        "        '(sincere_winners,)': \"Sincere Winners\"\n",
        "        }\n",
        "\n",
        "    ax.get_legend().remove()\n",
        "\n",
        "    plt.ylim(-0.4, 1.0)\n",
        "    plt.yticks(fontsize=fontsize)\n",
        "    plt.xticks(rotation=45, fontsize=fontsize)\n",
        "    sns.despine(right=True, top=True)\n",
        "    ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{graphs_dir}/voting_methods_different_agent_infos_ratio.pdf\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.3. Graphs to compare across different numbers of candidates (Figure 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS: \n",
        "    gen = 1\n",
        "    all_num_cands = [3, 4, 5, 6]\n",
        "    all_num_voters = [5, 6, 10, 11, 20, 21]\n",
        "    all_num_voters_pairs = [\n",
        "        '(5, 6)', \n",
        "        '(10, 11)', \n",
        "        '(20, 21)'\n",
        "    ] \n",
        "    manip_weight = 1\n",
        "    model_sizes = [\n",
        "        (256, 128)\n",
        "        # (512,), \n",
        "        # (256, 256), \n",
        "        # (512, 256, 128)\n",
        "    ]\n",
        "    agent_infos = 'majority'\n",
        "    agent_infos_str = 'majority matrix'\n",
        "\n",
        "    labeling = 'optimize'\n",
        "    decision_rule = 'argmax'\n",
        "    metric_op = 'normalized_subtract'\n",
        "    probmodel = 'uniform'\n",
        "    evaluation_batch_size = 4096\n",
        "    max_est_std_error = 0.0005\n",
        "    eval_dict = {}\n",
        "    for num_cands in all_num_cands:\n",
        "        for num_voters in all_num_voters: \n",
        "            # get filename from regular expression\n",
        "            filenames = glob.glob(f\"evaluation/evaluation_('{agent_infos}',)_{probmodel}_{labeling}_{manip_weight}/{gen}_{num_cands}_{num_voters}_{probmodel}_('{agent_infos}',)_{manip_weight}_{decision_rule}_{metric_op}_{evaluation_batch_size}_{max_est_std_error}_*.pickle\")\n",
        "            if len(filenames) == 0:\n",
        "                print(f\"WARNING: no file found for {num_cands} candidates, {num_voters} voters, and {agent_infos}\\n\")\n",
        "                continue\n",
        "            elif len(filenames) > 1:\n",
        "                print(\"WARNING: more than one file found!\")\n",
        "                continue\n",
        "            else: \n",
        "                filename = filenames[0]\n",
        "                print(f\"loading {agent_info} evaluation data:\\n {filename.split('/')[-1]}\")\n",
        "                _eval_dict  = pickle.load(open(filename, 'rb'))\n",
        "                eval_dict.update(_eval_dict)\n",
        "\n",
        "    print(len(eval_dict.keys()))\n",
        "    df = create_dataframe(eval_dict)\n",
        "    df_fixed_model_size = df[df['model_size'].isin(model_sizes)]\n",
        "    df_fixed_model_size['agent_infos'] = df_fixed_model_size['agent_infos'].astype(str)\n",
        "    df_fixed_model_size['agent_infos'] = df_fixed_model_size['agent_infos'].replace({\n",
        "        \"('plurality_scores',)\": \"plurality_scores\", \n",
        "        \"('majority',)\": \"majority\",\n",
        "        \"('plurality_ranking',)\": \"plurality_ranking\",\n",
        "        \"('margin',)\": \"margin\",\n",
        "        \"('qual_margin',)\": \"qual_margin\",\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS: \n",
        "    dfs = []\n",
        "    for num_cands in all_num_cands:\n",
        "        for num_voters_pairs in all_num_voters_pairs:\n",
        "            file_path = f'./ideal_manipulator_data/ideal_voter_1_uniform_{num_cands}_{num_voters_pairs}.csv'\n",
        "            df = pd.read_csv(file_path)\n",
        "            dfs.append(df)\n",
        "    fontsize = 14\n",
        "    # Combining the two dataframes\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "        \n",
        "    combined_df['vm'] = combined_df['vm'].replace('Strict Nanson', 'Nanson')\n",
        "\n",
        "    # Filtering for 'max' decision_rule\n",
        "    filtered_max_decision = combined_df[combined_df['decision_rule'] == 'max']\n",
        "    filtered_max_decision['num_cands_str'] = filtered_max_decision['num_cands'].astype(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if VISUALIZE_RESULTS:\n",
        "\n",
        "    # replace Strict Nanson with Nanson\n",
        "    df_fixed_model_size['vm'] = df_fixed_model_size['vm'].replace('Strict Nanson', 'Nanson')\n",
        "    fontsize=16\n",
        "    df_fixed_model_size['num_cands_str'] = df_fixed_model_size['num_cands'].astype(str)\n",
        "\n",
        "    # Extracting unique voting methods and preparing a color palette\n",
        "    unique_vms = sorted(df_fixed_model_size['vm'].unique())\n",
        "    palette = sns.color_palette(\"colorblind\", n_colors=len(unique_vms))\n",
        "\n",
        "    # Creating the plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Plotting lines for each voting method and preparing legend elements\n",
        "    legend_elements = []\n",
        "    for i, vm in enumerate(unique_vms):\n",
        "\n",
        "        _data = df_fixed_model_size[(df_fixed_model_size['vm'] == vm) & (df_fixed_model_size['agent_infos'] ==  agent_infos)]\n",
        "\n",
        "        mean_profit_data = _data.pivot_table(index='num_cands_str', values='mean_profitability', aggfunc='mean')\n",
        "\n",
        "        plt.plot(mean_profit_data.index, mean_profit_data.values, linestyle='-', color=palette[i])\n",
        "\n",
        "\n",
        "        legend_elements.append(plt.Line2D([0], [0], color=palette[i], lw=2, label=vm))\n",
        "\n",
        "    # Adding plot details\n",
        "    plt.title(f'{agent_infos_str}', fontsize=fontsize)\n",
        "    plt.xlabel('number of candidates', fontsize=fontsize)\n",
        "    plt.ylabel('average profitability', fontsize=fontsize)\n",
        "\n",
        "    # Adding the legend with only solid lines for voting methods\n",
        "    plt.legend(handles=legend_elements, title='', loc='upper left', bbox_to_anchor=(0.45, 0.8), fontsize=fontsize, ncols=2, frameon=False)\n",
        "    plt.xticks(fontsize=fontsize)\n",
        "    plt.yticks(fontsize=fontsize)\n",
        "\n",
        "    sns.despine(right=True, top=True)\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{graphs_dir}/num_cands_different_voting_methods_{agent_infos}.pdf\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ideal manipulator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS:\n",
        " \n",
        "    # replace Strict Nanson with Nanson\n",
        "    df_fixed_model_size['vm'] = df_fixed_model_size['vm'].replace('Strict Nanson', 'Nanson')\n",
        "    \n",
        "    fontsize=16\n",
        "\n",
        "    df_fixed_model_size['num_cands_str'] = df_fixed_model_size['num_cands'].astype(str)\n",
        "\n",
        "    unique_vms = sorted(df_fixed_model_size['vm'].unique())\n",
        "    palette = sns.color_palette(\"colorblind\", n_colors=len(unique_vms))\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    legend_elements = []\n",
        "\n",
        "    for i, vm in enumerate(unique_vms):\n",
        "\n",
        "        ideal_data = filtered_max_decision[filtered_max_decision['vm'] == vm]\n",
        "        ideal_data_avg = ideal_data.pivot_table(index='num_cands_str', values='mean', aggfunc='mean')\n",
        "\n",
        "        plt.plot(ideal_data_avg.index, ideal_data_avg.values, linestyle='-', color=palette[i])\n",
        "\n",
        "        legend_elements.append(plt.Line2D([0], [0], color=palette[i], lw=2, label=vm))\n",
        "\n",
        "    plt.title('ideal manipulator', fontsize=fontsize)\n",
        "    plt.xlabel('number of candidates', fontsize=fontsize)\n",
        "    plt.ylabel('average profitability', fontsize=fontsize)\n",
        "\n",
        "    plt.legend(handles=legend_elements, title='', loc='upper left', bbox_to_anchor=(0., 1.0), fontsize=fontsize, ncols=2, frameon=False)\n",
        "    plt.xticks(fontsize=fontsize)\n",
        "    plt.yticks(fontsize=fontsize)\n",
        "\n",
        "    sns.despine(right=True, top=True)\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{graphs_dir}/num_cands_different_voting_methods_ideal.pdf\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ratio of mean profitability to the mean of the best possible profitability for a fixed model size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS:\n",
        "    dfs = []\n",
        "    for num_cands in all_num_cands:\n",
        "        for num_voters_pairs in all_num_voters_pairs:\n",
        "            file_path = f'./ideal_manipulator_data/ideal_voter_1_uniform_{num_cands}_{num_voters_pairs}.csv'\n",
        "            df = pd.read_csv(file_path)\n",
        "            dfs.append(df)\n",
        "    fontsize = 14\n",
        "    # Combining the two dataframes\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "        \n",
        "    combined_df['vm'] = combined_df['vm'].replace('Strict Nanson', 'Nanson')\n",
        "\n",
        "    # Filtering for 'max' decision_rule\n",
        "    filtered_max_decision = combined_df[combined_df['decision_rule'] == 'max']\n",
        "\n",
        "    df_fixed_model_size_ratio = df_fixed_model_size.copy()\n",
        "    df_fixed_model_size_ratio['vm'] = df_fixed_model_size_ratio['vm'].replace('Strict Nanson', 'Nanson')\n",
        "\n",
        "    data_for_df = {\n",
        "        'vm': list(), \n",
        "        'num_cands': list(),\n",
        "        \"agent_infos\": list(),\n",
        "        \"ratio\": list(),\n",
        "    }\n",
        "\n",
        "    for num_cands in all_num_cands:\n",
        "        for vm in voting_methods_order:\n",
        "            if vm == \"Strict Nanson\": \n",
        "                vm = \"Nanson\"\n",
        "            ideal_voter_mean = filtered_max_decision[(filtered_max_decision[\"num_cands\"] == num_cands) & (filtered_max_decision['vm'] == vm)]['mean'].mean()\n",
        "            \n",
        "            # # # get the mean of the current vm for the same number of candidates and voters\n",
        "            vm_mean = df_fixed_model_size_ratio[(df_fixed_model_size_ratio[\"num_cands\"] == num_cands) & ((df_fixed_model_size_ratio['vm'] == vm) & (df_fixed_model_size_ratio['agent_infos'] == agent_infos))]['mean_profitability'].mean()\n",
        "                                # calculate the ratio\n",
        "            ratio = vm_mean / ideal_voter_mean\n",
        "                            # add the ratio to the dataframe\n",
        "            data_for_df['vm'].append(vm)\n",
        "            data_for_df['num_cands'].append(num_cands)\n",
        "            data_for_df['agent_infos'].append(agent_infos)\n",
        "            data_for_df['ratio'].append(ratio)\n",
        "\n",
        "    df_fixed_model_size_ratio = pd.DataFrame(data_for_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS:\n",
        "    \n",
        "    df_fixed_model_size_ratio['num_cands_str'] = df_fixed_model_size_ratio['num_cands'].astype(str)\n",
        "\n",
        "    unique_vms = sorted(df_fixed_model_size_ratio['vm'].unique())\n",
        "    palette = sns.color_palette(\"colorblind\", n_colors=len(unique_vms))\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    legend_elements = []\n",
        "    for i, vm in enumerate(unique_vms):\n",
        "\n",
        "        _data = df_fixed_model_size_ratio[(df_fixed_model_size_ratio['vm'] == vm) & (df_fixed_model_size_ratio['agent_infos'] == agent_infos)]\n",
        "        \n",
        "        ratio_data = _data.pivot_table(index='num_cands_str', values='ratio', aggfunc='mean')\n",
        "\n",
        "        plt.plot(ratio_data.index, ratio_data.values, linestyle='-', color=palette[i])\n",
        "\n",
        "        legend_elements.append(plt.Line2D([0], [0], color=palette[i], lw=2, label=vm))\n",
        "\n",
        "    # Adding plot details\n",
        "    plt.title(f'{agent_infos_str}', fontsize=fontsize)\n",
        "    plt.xlabel('number of candidates', fontsize=fontsize)\n",
        "    plt.ylabel('average ratio', fontsize=fontsize)\n",
        "    min_ratio = df_fixed_model_size_ratio['ratio'].min()\n",
        "    plt.ylim(min_ratio if min_ratio < 0 else 0.0, 1.0)\n",
        "    plt.xticks(fontsize=fontsize)\n",
        "    plt.yticks(fontsize=fontsize)\n",
        "\n",
        "    # Adding the legend with only solid lines for voting methods\n",
        "    plt.legend(handles=legend_elements, title='', loc='upper left', bbox_to_anchor=(0.45, 1.0), fontsize=fontsize, ncols=2, frameon=False)\n",
        "\n",
        "    sns.despine(right=True, top=True)\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{graphs_dir}/num_cands_different_voting_methods_{agent_infos}_ratio.pdf\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  6.4. Compare different generations of models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS:\n",
        "    num_cands = 6\n",
        "    num_voters = 11\n",
        "    agent_info = 'majority'\n",
        "    agent_info_str = 'majority matrix'\n",
        "    barcolor = 'orange' #'blue' #'green' #'red' #'orange' #'purple' #'brown'\n",
        "    manip_weight = 1\n",
        "    probmodel = 'uniform'\n",
        "    labeling = 'optimize'\n",
        "\n",
        "    print(f\"loading {agent_info} evaluation data, generation 1\")\n",
        "    filename = glob.glob(f\"evaluation/evaluation_('{agent_info}',)_{probmodel}_{labeling}_{manip_weight}/1_{num_cands}_{num_voters}_{probmodel}_('{agent_info}',)_{manip_weight}_*.pickle\")[0]\n",
        "    print(f\"{filename.split('/')[-1]}\")\n",
        "    eval1 = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "    print(f\"loading {agent_info} evaluation data, generation 2\")\n",
        "    filename = glob.glob(f\"evaluation/evaluation_('{agent_info}',)_{probmodel}_{labeling}_{manip_weight}/2_{num_cands}_{num_voters}_{probmodel}_('{agent_info}',)_{manip_weight}_*.pickle\")[0]\n",
        "    print(f\"{filename.split('/')[-1]}\")\n",
        "    eval2 = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "\n",
        "    print(f\"loading {agent_info} evaluation data, generation 3\")\n",
        "    filename = glob.glob(f\"evaluation/evaluation_('{agent_info}',)_{probmodel}_{labeling}_{manip_weight}/3_{num_cands}_{num_voters}_{probmodel}_('{agent_info}',)_{manip_weight}_*.pickle\")[0]\n",
        "    print(f\"{filename.split('/')[-1]}\")\n",
        "    eval3 = pickle.load(open(filename, 'rb'))\n",
        "    df1 = create_dataframe(eval1)\n",
        "    df2 = create_dataframe(eval2)\n",
        "    df3 = create_dataframe(eval3)\n",
        "    dfs = {\n",
        "        ((f'{agent_info}',), 1): df1,\n",
        "        ((f'{agent_info}',), 2): df2,\n",
        "        ((f'{agent_info}',), 3): df3,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_graphs_with_errorbars_for_different_generations(df, num_cands, manip_weight, legend_label_mapping, color_dict, filename, labeling='satisfice', probmodel='uniform'):\n",
        "    # function to display graphs comparing 3 generations of models\n",
        "    \n",
        "    # multiply the est_std_errors by 2 to get 95% confidence intervals\n",
        "    df['est_std_error'] = df['est_std_error'] * 2\n",
        "    \n",
        "    fontsize=16\n",
        "    df = df[df['labeling'] == labeling]\n",
        "    y_ticks = list(np.arange(-0.08, 0.10, 0.01))\n",
        "    df = df[df['model_size'].isin(model_sizes_order)]\n",
        "    df['model_size'] = pd.Categorical(df['model_size'], categories=model_sizes_order, ordered=True)\n",
        "    df.sort_values('model_size', inplace=True)\n",
        "    \n",
        "    fig = plt.figure(figsize=(24, 33))\n",
        "    gs = fig.add_gridspec(len(voting_methods_order) // 2, 2, hspace=0.1, wspace=0.05)\n",
        "    \n",
        "    voters_order = list(legend_label_mapping.keys())\n",
        "    bar_offset = [-0.25, 0, 0.25]\n",
        "        \n",
        "    \n",
        "    # Adjusted vertical line positions\n",
        "    vline_positions = [model_sizes_order.index(\"(512,)\"), model_sizes_order.index(\"(256, 256)\")]\n",
        "    \n",
        "    \n",
        "    for i, voting_method in enumerate(voting_methods_order):\n",
        "        ax = fig.add_subplot(gs[i // 2, i % 2])\n",
        "        df_filtered_vm = df[df['vm'] == voting_method]\n",
        "        \n",
        "        # Existing bars\n",
        "        for idx, voter_info in enumerate(voters_order):\n",
        "            current_data = df_filtered_vm[df_filtered_vm['voters_info'] == voter_info]\n",
        "            ax.bar(x=np.arange(len(model_sizes_order)) + bar_offset[idx], height=current_data['average_mean_profitability'],\n",
        "                   yerr=current_data['est_std_error'], width=0.25, align='center', label=legend_label_mapping[voter_info] if i == 0 else \"\", color=color_dict[voter_info])\n",
        "        \n",
        "        # Drawing the vertical dashed lines\n",
        "        for pos in vline_positions:\n",
        "            ax.axvline(x=pos + 0.5, color='gray', linestyle='--', linewidth=1)\n",
        "        \n",
        "        # Other plotting settings remain the same\n",
        "        ax.set_title(f'{voting_method if voting_method != \"Strict Nanson\" else \"Nanson\"}', fontsize=18, weight='bold')\n",
        "        ax.tick_params(axis='x', rotation=90, labelsize=8 if i // 2 == (len(voting_methods_order) // 2 - 1) else 0)\n",
        "        ax.tick_params(axis='y', labelsize=10 if i % 2 == 0 else 0)\n",
        "        ax.set_yticks(y_ticks)\n",
        "        ax.set_ylim([-0.08, 0.09])\n",
        "        ax.set_yticklabels([round(y,2) for y in y_ticks] if i % 2 == 0 else [], fontsize=fontsize)\n",
        "        ax.set_xticks(list(np.arange(len(model_sizes_order))))\n",
        "        ax.set_xticklabels(model_sizes_order  if i // 2 == (len(voting_methods_order) // 2 - 1) else [], fontsize=fontsize)\n",
        "        ax.set_ylabel('average profitability of submitted ranking' if i % 2 == 0 else '', fontsize=18)\n",
        "        ax.set_xlabel('')\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "        \n",
        "        if i == 0:\n",
        "            # Main legend\n",
        "            legend1 = ax.legend(loc='lower right', bbox_to_anchor=(1.0, 0), title=\"Voters, Manipulator Info, Generation\", title_fontsize = 18, fontsize=fontsize)\n",
        "            ax.add_artist(legend1)\n",
        "            \n",
        "\n",
        "    plt.subplots_adjust(hspace=0.0, wspace=0.0, top=.83, bottom=0.15)\n",
        "    plt.savefig(filename)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_average_dfs(\n",
        "        dfs, \n",
        "        agent_infos_gen_list, \n",
        "        num_cands_list, \n",
        "        manip_weight_list,\n",
        "        probmodel,\n",
        "        labeling):\n",
        "    # function to generate the dataframes for the graphs \n",
        "    # in the supplemental material\n",
        "    for num_cands in num_cands_list:\n",
        "        for agent_infos, gen in agent_infos_gen_list:\n",
        "            print(agent_infos)\n",
        "            print(gen)\n",
        "            for manip_weight in manip_weight_list:\n",
        "                    \n",
        "                df = dfs[(tuple(agent_infos), gen)]\n",
        "                df_avg_10 = generate_avg_df(df[(df[\"num_cands\"] == num_cands) & (df[\"num_voters\"]==10)], num_cands, 10,  tuple(agent_infos), manip_weight, labeling, probmodel)\n",
        "                    \n",
        "                df_avg_11 = generate_avg_df(df[(df[\"num_cands\"] == num_cands) & (df[\"num_voters\"]==11)], num_cands, 11,  tuple(agent_infos), manip_weight, labeling, probmodel)\n",
        "                    \n",
        "                df_avg = pd.concat([df_avg_10, df_avg_11])\n",
        "                print(\"generated \", f\"evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/df_avg_{num_cands}_{agent_infos}_{manip_weight}_{gen}.csv\")\n",
        "                    \n",
        "                df_avg.to_csv(f\"evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/df_avg_{num_cands}_{agent_infos}_{manip_weight}_{gen}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_combined_df(num_cands, num_voters, color_dict, legend_label_mapping, agent_infos,  manip_weight, labeling, probmodel, filename):\n",
        "    # combined the dataframes produced by create_average_dfs\n",
        "    \n",
        "    gen=1\n",
        "    _df1 = pd.read_csv(f\"evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/df_avg_{num_cands}_{list(agent_infos)}_{manip_weight}_{gen}.csv\")\n",
        "    df1 = _df1[_df1[\"num_voters\"] == num_voters]\n",
        "    df1['agent_infos'] = f'{agent_infos[0]}, 1'\n",
        "\n",
        "    gen=2\n",
        "    _df2 = pd.read_csv(f\"evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/df_avg_{num_cands}_{list(agent_infos)}_{manip_weight}_{gen}.csv\")\n",
        "    df2 = _df2[_df2[\"num_voters\"] == num_voters]\n",
        "    df2['agent_infos'] = f'{agent_infos[0]}, 2'\n",
        "\n",
        "    gen=3\n",
        "    agent_infos2=['plurality_scores']\n",
        "    _df3 = pd.read_csv(f\"evaluation/evaluation_{tuple(agent_infos)}_{probmodel}_{labeling}_{manip_weight}/df_avg_{num_cands}_{list(agent_infos)}_{manip_weight}_{gen}.csv\")\n",
        "    df3 = _df3[_df3[\"num_voters\"] ==  num_voters]\n",
        "    df3['agent_infos'] = f'{agent_infos[0]}, 3'\n",
        "\n",
        "    combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "\n",
        "    # Create the 'voters_info' column which combines the number of voters and agent_infos\n",
        "\n",
        "    combined_df['voters_info'] = combined_df['num_voters'].astype(str) + ', ' + combined_df['agent_infos']\n",
        "\n",
        "    return combined_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS:\n",
        "    agent_infos_gen_list = [\n",
        "        [[f'{agent_info}'], 1], \n",
        "        [[f'{agent_info}'], 2],\n",
        "        [[f'{agent_info}'], 3],\n",
        "        ]\n",
        "    manip_weight_list = [manip_weight]\n",
        "    num_cands_list = [num_cands]\n",
        "\n",
        "    create_average_dfs(dfs, agent_infos_gen_list, num_cands_list, manip_weight_list, probmodel, labeling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VISUALIZE_RESULTS:\n",
        "\n",
        "    agent_infos = [agent_info]\n",
        "    color_dict = {\n",
        "        f\"{num_voters}, {agent_info}, 1\": colors_for_bars[barcolor],\n",
        "        f\"{num_voters}, {agent_info}, 2\": dark[barcolor],\n",
        "        f\"{num_voters}, {agent_info}, 3\": pastel[barcolor],\n",
        "    }\n",
        "    \n",
        "    legend_label_mapping = {\n",
        "            f\"{num_voters}, {agent_info}, 1\": f\"{num_voters}, {agent_info_str}, 1\",\n",
        "            f\"{num_voters}, {agent_info}, 2\": f\"{num_voters}, {agent_info_str}, 2\",\n",
        "            f\"{num_voters}, {agent_info}, 3\": f\"{num_voters}, {agent_info_str}, 3\",\n",
        "        }\n",
        "\n",
        "    filename = f\"{graphs_dir}/graphs_{num_cands}_{num_voters}_{agent_info}_3generations.pdf\"\n",
        "\n",
        "    combined_df = generate_combined_df(num_cands, num_voters, color_dict, legend_label_mapping, agent_infos, manip_weight, labeling, probmodel, filename)\n",
        "\n",
        "    generate_graphs_with_errorbars_for_different_generations(combined_df, num_cands, manip_weight, legend_label_mapping, color_dict, filename, labeling=labeling, probmodel=probmodel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.5. Statistics about the number of models evaluated (Footnote 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_eval_files = 0\n",
        "total_eval_dict = {}\n",
        "eval_directories = [\n",
        "    \"evaluation_('majority',)_uniform_optimize_1\",\n",
        "    \"evaluation_('margin',)_uniform_optimize_1\",\n",
        "    \"evaluation_('plurality_ranking',)_uniform_optimize_1\",\n",
        "    \"evaluation_('plurality_scores',)_uniform_optimize_1\",\n",
        "    \"evaluation_('qual_margin',)_uniform_optimize_1\",\n",
        "    \"evaluation_('sincere_winners',)_uniform_optimize_1\",\n",
        "]    \n",
        "for eval_dir in eval_directories:\n",
        "\n",
        "    for filename in glob.glob(f\"evaluation/{eval_dir}/*.pickle\"):\n",
        "        num_eval_files += 1\n",
        "        #print(f\"loading {filename.split('/')[-1]}\")\n",
        "        _eval_dict  = pickle.load(open(filename, 'rb'))\n",
        "        total_eval_dict.update(_eval_dict)\n",
        "print(f\"Number of evaluation files: {num_eval_files}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_num_trials = []\n",
        "for key in total_eval_dict.keys():\n",
        "    all_num_trials.append(total_eval_dict[key]['num_trials'])\n",
        "        \n",
        "print(\"MIN \", min(all_num_trials))\n",
        "print(\"MAX \", max(all_num_trials))\n",
        "print(\"MEAN \", np.mean(all_num_trials))\n",
        "print(\"STD \", np.std(all_num_trials))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.6. The total number of model evaluated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note that the models are not saved in this repo because they are too large, so this code will not run unless you have the models saved locally. \n",
        "\n",
        "num_model_files = 0\n",
        "num_models = 0\n",
        "num_training_iters = []\n",
        "models_directories = [\n",
        "    \"models_('majority',)_uniform_optimize_1\",\n",
        "    \"models_('margin',)_uniform_optimize_1\",\n",
        "    \"models_('plurality_ranking',)_uniform_optimize_1\",\n",
        "    \"models_('plurality_scores',)_uniform_optimize_1\",\n",
        "    \"models_('qual_margin',)_uniform_optimize_1\",\n",
        "    \"models_('sincere_winners',)_uniform_optimize_1\",\n",
        "]    \n",
        "for model_dir in tqdm(models_directories, leave=False):\n",
        "    for filename in glob.glob(f\"models/{model_dir}/*.pickle\"):\n",
        "        num_model_files += 1\n",
        "        #print(f\"loading {filename.split('/')[-1]}\")\n",
        "        _model_dict  = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "        for key in _model_dict.keys(): \n",
        "            num_training_iters.append(len(_model_dict[key][1]))\n",
        "        num_models += len(_model_dict.keys())\n",
        "\n",
        "print(f\"Number of models files: {num_model_files}\")\n",
        "print(f\"The total number of models: {num_models}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"max number of iterations during training: {max(num_training_iters)}\")\n",
        "print(f\"min number of iterations during training: {min(num_training_iters)}\")\n",
        "print(f\"average number of iterations during training: {np.mean(num_training_iters)}\")\n",
        "print(f\"standard deviation of the number of iterations during training: {np.std(num_training_iters)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
